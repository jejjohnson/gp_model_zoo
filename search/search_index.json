{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Gaussian Process Model Zoo \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.github.io/gp_model_zoo/ Personal Website: jejjohnson.netlify.app Motivation \u00b6 I recently ran into someone at a conference who said, A lot of research dies in Graduate students laptops . (it was actually this scientist right here ). So I decided to go through all of my stuff, organize it a little bit and make it public. I have included some of the algorithms that I use or have worked with during my PhD. My lab works with kernel methods and we frequently use Gaussian Processes (GPs) for different Earth science applications, e.g. emulation, ocean applications and parameter retrievals. We typically use GPs for the following reasons with number 1 being the most important (as it with research groups whether they admit it or not). It's what we've been doing... We are a kernel lab, use mainly kernel methods and GPs are essentially a Bayesian treatment of kernel methods for regression and classification applications. Properly handling uncertainty is essential when dealing with physical data. With GPs, can often use sensible priors and a fairly consistent way of tuning the hyperparameters. Somewhat robust to overfitting via built-in regularization. I created this repo because I didn't want my code to go to waste in case there were people who are new to GPs and want to see a few key algorithms implemented. Also, it allows me to centralize my code for all my projects. Hopefully it will be of some use to others. What you'll find here \u00b6 Beginners \u00b6 These are the resources I consider to be the best when it comes to learning about GPs. The field has grown a lot but we still have newcomers. Start here! Literature \u00b6 There are many resources on the internet and I try to compile as much as I can. I do try and keep track of the SOTA and peruse arxiv from time to time. Software \u00b6 I like to keep track of what's going on. So I've listed the libraries that I am aware of as well as some things I've noticed about them.","title":"Home"},{"location":"#gaussian-process-model-zoo","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.github.io/gp_model_zoo/ Personal Website: jejjohnson.netlify.app","title":"Gaussian Process Model Zoo"},{"location":"#motivation","text":"I recently ran into someone at a conference who said, A lot of research dies in Graduate students laptops . (it was actually this scientist right here ). So I decided to go through all of my stuff, organize it a little bit and make it public. I have included some of the algorithms that I use or have worked with during my PhD. My lab works with kernel methods and we frequently use Gaussian Processes (GPs) for different Earth science applications, e.g. emulation, ocean applications and parameter retrievals. We typically use GPs for the following reasons with number 1 being the most important (as it with research groups whether they admit it or not). It's what we've been doing... We are a kernel lab, use mainly kernel methods and GPs are essentially a Bayesian treatment of kernel methods for regression and classification applications. Properly handling uncertainty is essential when dealing with physical data. With GPs, can often use sensible priors and a fairly consistent way of tuning the hyperparameters. Somewhat robust to overfitting via built-in regularization. I created this repo because I didn't want my code to go to waste in case there were people who are new to GPs and want to see a few key algorithms implemented. Also, it allows me to centralize my code for all my projects. Hopefully it will be of some use to others.","title":"Motivation"},{"location":"#what-youll-find-here","text":"","title":"What you'll find here"},{"location":"#beginners","text":"These are the resources I consider to be the best when it comes to learning about GPs. The field has grown a lot but we still have newcomers. Start here!","title":"Beginners"},{"location":"#literature","text":"There are many resources on the internet and I try to compile as much as I can. I do try and keep track of the SOTA and peruse arxiv from time to time.","title":"Literature"},{"location":"#software","text":"I like to keep track of what's going on. So I've listed the libraries that I am aware of as well as some things I've noticed about them.","title":"Software"},{"location":"literature/","text":"Gaussian Process Literature \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Docsify Website: jejjohnson.github.io/gp_model_zoo/ Personal Website: jejjohnson.netlify.com Introduction \u00b6 This folder contains lists of papers and code bases that deal with Gaussian processes and applications. I also try to keep up with the literature and show some state-of-the-art (SOTA) algorithms and code bases. I'm quite fond of GPs because they are mathematically beautiful and they have many nice properties. They also work with one of the nicest distributions to work with: the Guassian distribution. They deal with uncertainty and because they are function based, they work well with small (and large datasets) with sophisticated interpolation schemes. The biggest problem of GPs back in the day was that they did not scale but nowadays that isn't the case as there are dozens of sparse methods that allow us to scale GPs to 100K+ up to even 1 million data points. My Perspective \u00b6 I think that the GP literature is fairly spread over many fields but there is not really much of any compilation of GP resources (not until recently at least). I have been studying GPs for a while now to try to understand the field as well to see how they have been used. My feeling is that there are more methods than actual applications and many of the algorithms seem to have never been applied to many things outside of their \"corner case\" application. If you look at a standard GP algorithm, it seems to go within one of the following modifications: Algorithm original algorithm is invented Special Kernel a special kernel is invented for a specific use case Fourier-ify they do some random fourier features approximation scheme (or some other approximate kernel methods) Heteroscedastic-ify they modify the likelihood such that the noise function varies w.r.t. the inputs X as well. Sparse-ify they scale it using subsampling, model approximations or posterior approximations Latent Variable Representation they consider the case where the variables are latent and not deterministic MultiOutput-ify they apply it to multioutput (or multifidelity cases) Deep-ify they start stacking the GPs on top of each other In reality, almost all of the methods you'll find in the literature come within these subfields or a combination of a 2 or more more or less in the order I've listed. That's not to say that what people do isn't important or impressive, but it would be nice if we had some better structure to how we classify GP algorithms and improvements we make. My Special Interests There are a few questions and issues which I think have not been answered but are steadily improving with time. They are individual components of the GP algorithms that I consider a bit weak and I think they can be improved by taking knowledge from other fields. A good example would be how the treatment of Matrix-Vector-Multiplication (MVM) was used for scaling GP algorithms. Back in the day, the dominant community was various ways to subsample the data. The MVM community was fairly small within the main GP community but now it has risen as the most scalable and customizable method to date with a dominant python package. Just goes to show how other fields can really come in and improve some of the little intricate aspects of the algorithms and make huge improvements. Below is a list of methods which are not as prevalent in the literature that you won't see at major machine learning conferences but that I believe are super important and could possibly greatly improve the methods that we already have. I will try to pay special attention to #1 in particular because it is apart of my research. Input Uncertainty Uncertainty Calibration Kernel Parameters (initializations and priors) Missing Data / Semi-Supervised Learning Training procedures Expressive Kernels Topics \u00b6 Gaussian Processes \u00b6 This file some recommended resources as well as some SOTA algorithms and key improvements over the years. It also includes sparse GPs and the treatment of random fourier features. At some point I will do a timeline of some of the most import GP papers within the literature. Sparse Gaussian Processes \u00b6 This contains some of the main GP algorithms that you need to know in the way of scaling. If your dataset is larger than 2K then you should probably start using one of these sparse methods. Latent Variable Models \u00b6 These algorithms are when we assume that the input X X is not determinant but instead an unknown. The applications of this range from dimensionality reduction to uncertain inputs and even applications in missing data and semi-supervised learning. Sparse Spectrum \u00b6 These algorithms make use of the Bochner theorem which states that we can represent kernel functions as an infinite series with weights that stem from a Gaussian distribution. These (what are essentially Fourier transformations) are typically known as Sparse Spectrum GPs in the community. Uncertain Inputs \u00b6 This is directly related to my research so I'll pay special attention to this. I look at the literature spanned from the beginning up until now. This will mostly be about moment-matching algorithms and algorithms that use variational inference. Deep GPs \u00b6 I have made this a special section because I think it's quite an interesting topic. It's still a fairly young aspect of GPs (last 7 years or so) so it won't have such a depth of literature like the other topics. I'll also include stuff related to how Deep GPs are related to neural networks. Neural Networks and GPs \u00b6 This section is made up of the papers that talk about the connections between neural networks and GPs as well as some specific cases where people have used neural networks to extract features as inputs for GPs (i.e. Deep Kernel Learning). Components \u00b6 These are key elements of the GP algorithm that have been studied in the 'special interest' listed above, e.g. input uncertainty, training procedures, parameter estimation. Kernels \u00b6 I have a section where you can find stuff on different kernel methods that have specific use cases. Software \u00b6 The fun part. Here is where I look at all the latest software that one can use to run some of the SOTA algorithms. It will python libraries only because it's the language I personally use to code. Applications \u00b6 This consists of papers which have applied GPs to their respective fields. I'll focus more on Earth observation applications but I'll put up any others if I find them of interest. Special \u00b6 This will include all of the GP algorithms that I consider 'corner cases', i.e. GPs modified that apply to a specific application where some modification was necessary to make the GP work their use case.","title":"Literature"},{"location":"literature/#gaussian-process-literature","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Docsify Website: jejjohnson.github.io/gp_model_zoo/ Personal Website: jejjohnson.netlify.com","title":"Gaussian Process Literature"},{"location":"literature/#introduction","text":"This folder contains lists of papers and code bases that deal with Gaussian processes and applications. I also try to keep up with the literature and show some state-of-the-art (SOTA) algorithms and code bases. I'm quite fond of GPs because they are mathematically beautiful and they have many nice properties. They also work with one of the nicest distributions to work with: the Guassian distribution. They deal with uncertainty and because they are function based, they work well with small (and large datasets) with sophisticated interpolation schemes. The biggest problem of GPs back in the day was that they did not scale but nowadays that isn't the case as there are dozens of sparse methods that allow us to scale GPs to 100K+ up to even 1 million data points.","title":"Introduction"},{"location":"literature/#my-perspective","text":"I think that the GP literature is fairly spread over many fields but there is not really much of any compilation of GP resources (not until recently at least). I have been studying GPs for a while now to try to understand the field as well to see how they have been used. My feeling is that there are more methods than actual applications and many of the algorithms seem to have never been applied to many things outside of their \"corner case\" application. If you look at a standard GP algorithm, it seems to go within one of the following modifications: Algorithm original algorithm is invented Special Kernel a special kernel is invented for a specific use case Fourier-ify they do some random fourier features approximation scheme (or some other approximate kernel methods) Heteroscedastic-ify they modify the likelihood such that the noise function varies w.r.t. the inputs X as well. Sparse-ify they scale it using subsampling, model approximations or posterior approximations Latent Variable Representation they consider the case where the variables are latent and not deterministic MultiOutput-ify they apply it to multioutput (or multifidelity cases) Deep-ify they start stacking the GPs on top of each other In reality, almost all of the methods you'll find in the literature come within these subfields or a combination of a 2 or more more or less in the order I've listed. That's not to say that what people do isn't important or impressive, but it would be nice if we had some better structure to how we classify GP algorithms and improvements we make. My Special Interests There are a few questions and issues which I think have not been answered but are steadily improving with time. They are individual components of the GP algorithms that I consider a bit weak and I think they can be improved by taking knowledge from other fields. A good example would be how the treatment of Matrix-Vector-Multiplication (MVM) was used for scaling GP algorithms. Back in the day, the dominant community was various ways to subsample the data. The MVM community was fairly small within the main GP community but now it has risen as the most scalable and customizable method to date with a dominant python package. Just goes to show how other fields can really come in and improve some of the little intricate aspects of the algorithms and make huge improvements. Below is a list of methods which are not as prevalent in the literature that you won't see at major machine learning conferences but that I believe are super important and could possibly greatly improve the methods that we already have. I will try to pay special attention to #1 in particular because it is apart of my research. Input Uncertainty Uncertainty Calibration Kernel Parameters (initializations and priors) Missing Data / Semi-Supervised Learning Training procedures Expressive Kernels","title":"My Perspective"},{"location":"literature/#topics","text":"","title":"Topics"},{"location":"literature/#gaussian-processes","text":"This file some recommended resources as well as some SOTA algorithms and key improvements over the years. It also includes sparse GPs and the treatment of random fourier features. At some point I will do a timeline of some of the most import GP papers within the literature.","title":"Gaussian Processes"},{"location":"literature/#sparse-gaussian-processes","text":"This contains some of the main GP algorithms that you need to know in the way of scaling. If your dataset is larger than 2K then you should probably start using one of these sparse methods.","title":"Sparse Gaussian Processes"},{"location":"literature/#latent-variable-models","text":"These algorithms are when we assume that the input X X is not determinant but instead an unknown. The applications of this range from dimensionality reduction to uncertain inputs and even applications in missing data and semi-supervised learning.","title":"Latent Variable Models"},{"location":"literature/#sparse-spectrum","text":"These algorithms make use of the Bochner theorem which states that we can represent kernel functions as an infinite series with weights that stem from a Gaussian distribution. These (what are essentially Fourier transformations) are typically known as Sparse Spectrum GPs in the community.","title":"Sparse Spectrum"},{"location":"literature/#uncertain-inputs","text":"This is directly related to my research so I'll pay special attention to this. I look at the literature spanned from the beginning up until now. This will mostly be about moment-matching algorithms and algorithms that use variational inference.","title":"Uncertain Inputs"},{"location":"literature/#deep-gps","text":"I have made this a special section because I think it's quite an interesting topic. It's still a fairly young aspect of GPs (last 7 years or so) so it won't have such a depth of literature like the other topics. I'll also include stuff related to how Deep GPs are related to neural networks.","title":"Deep GPs"},{"location":"literature/#neural-networks-and-gps","text":"This section is made up of the papers that talk about the connections between neural networks and GPs as well as some specific cases where people have used neural networks to extract features as inputs for GPs (i.e. Deep Kernel Learning).","title":"Neural Networks and GPs"},{"location":"literature/#components","text":"These are key elements of the GP algorithm that have been studied in the 'special interest' listed above, e.g. input uncertainty, training procedures, parameter estimation.","title":"Components"},{"location":"literature/#kernels","text":"I have a section where you can find stuff on different kernel methods that have specific use cases.","title":"Kernels"},{"location":"literature/#software","text":"The fun part. Here is where I look at all the latest software that one can use to run some of the SOTA algorithms. It will python libraries only because it's the language I personally use to code.","title":"Software"},{"location":"literature/#applications","text":"This consists of papers which have applied GPs to their respective fields. I'll focus more on Earth observation applications but I'll put up any others if I find them of interest.","title":"Applications"},{"location":"literature/#special","text":"This will include all of the GP algorithms that I consider 'corner cases', i.e. GPs modified that apply to a specific application where some modification was necessary to make the GP work their use case.","title":"Special"},{"location":"model_zoo/","text":"Model Zoo \u00b6 Implementations \u00b6 GPy \u00b6 I wrap a few baseline algorithms that I tend to use quite often in my research. Wrapped Exact GP Sparse GP (VFE, FITC, PEP) Stochastic Variational GP Pyro \u00b6 I wrap a few baseline algorithms that I have been investigating in my research. Wrapped Variational GP Sparse GP (VFE) Bayesian GPLVM TODO \u00b6 I plan to release some of the algorithms I have wrapped for the following libraries: GPyTorch GPFlow","title":"Model Zoo"},{"location":"model_zoo/#model-zoo","text":"","title":"Model Zoo"},{"location":"model_zoo/#implementations","text":"","title":"Implementations"},{"location":"model_zoo/#gpy","text":"I wrap a few baseline algorithms that I tend to use quite often in my research. Wrapped Exact GP Sparse GP (VFE, FITC, PEP) Stochastic Variational GP","title":"GPy"},{"location":"model_zoo/#pyro","text":"I wrap a few baseline algorithms that I have been investigating in my research. Wrapped Variational GP Sparse GP (VFE) Bayesian GPLVM","title":"Pyro"},{"location":"model_zoo/#todo","text":"I plan to release some of the algorithms I have wrapped for the following libraries: GPyTorch GPFlow","title":"TODO"},{"location":"software/","text":"Software \u00b6 Software Library Classification Quick Overview Sklearn GPy GPyTorch Pyro GPFlow TensorFlow Probability Edward2 Other Libraries Library Classification GPU Support Algorithms Implemented Software for Gaussian processes (GPs) have really been improving for quite a while now. It is now a lot easier to not only to actually use the GP models, but also to modify them improve them. Library Classification \u00b6 Photo Credit : Francois Chollet Tweet So how to classify a library's worth is impossible because it's completely subjective. But I like this chart by Francois Chollet who put the different depths a package can go to in order to create a package that caters to different users. But libraries typically can be classified on this spectrum. The same breakdown of Deep Learning algorithms into Models and Training can be done for GPs as well. Since GPs aren't super mainstream yet, most modern large scale GP libraries will fall in the fully flexible category. But recently, with the edition of TensorFlow probability and Edward2, we have more modern GPs that will fall into the Easy to use category (but not necessarily easy to train...). Quick Overview \u00b6 Sklearn \u00b6 The GP implementation in the scikit-learn library are already sufficient to get people started with GPs in scikit-learn. Often times when I'm data wrangling and I'm exploring possible algorithms, I'll already have the sklearn library installed in my conda environment so I typically start there myself especially for datasets less than 2,000 points. Sample Code Snippet \u00b6 The sklearn implementation is as basic as it gets. If you are familiar with the scikit-learn API then you will have no problems using the GPR module. It's a three step process with very little things to change. ** Model ** \u00b6 # define kernel function kernel = \\ RBF ( length_scale = 1.0 ) \\ + WhiteKernel ( noise_level = 0.1 ) # initialize GP model gpr_model = GaussianProcessRegressor ( kernel = kernel_gpml , alpha = 0 , optimizer = None , normalize_y = True ) ** Training ** \u00b6 # train GP model gpr_model . fit ( Xtrain , ytrain ) ** Predictions ** \u00b6 # get predictions y_pred , y_std = gpr_model . predict ( Xtest , return_std = True ) Again, this is the simplest API you will find and for small data problems, you'll find that this works fine out-of-the-box. I highly recommend this when starting especially if you're not a GP connoisseur. What I showed above is as complicated as it gets. Any more customization outside of this is a bit difficult as the scikit-learn API for GPs isn't very modular and wasn't designed as such. But as a first pass, it's good enough. GPy \u00b6 GPy is the most comprehensive research library I have found to date. It has the most number of different special GP \"corner case\" algorithms of any package available. The GPy examples and tutorials are very comprehensive. The major caveat is that the documentation is very difficult to navigate. I also found the code base to be a bit difficult to really understand what's going on because there is no automatic differentiation to reduce the computations so there can be a bit of redundancy. I typically wrap some typical GP algorithms with some common parameters that I use within the sklearn .fit() , .predict() , .score() framework and call it a day. The standard algorithms will include the Exact GP, the Sparse GP, and Bayesian GPLVM. A warning though: this library does not get updated very often so you will likely run into very silly bugs if you don't use strict package versions that are recommended. There are rumors of a GPy2 library that's based on MXFusion but I have failed to see anything concrete yet. Idea : Some of the main algorithms such as the sparse GP implementations are mature enough to be dumped into the sklearn library. For small-medium data problems, I think this would be extremely beneficial to the community. Some of the key papers like the (e.g. the FITC-SGP , VFE-SGP , Heteroscedastic GP , GP-LVM ) certainly pass some of the strict sklearn criteria . But I suspect that it wouldn't be a joy to code because you would need to do some of the gradients from scratch. I do feel like it might make GPs a bit more popular if some of the mainstream methods were included in the scikit-learn library. Sample Code Snippet \u00b6 The GPy implementation is also very basic. If you are familiar with the scikit-learn API then you will have no problems using the GPR module. It's a three step process with very little things to change. ** Model ** \u00b6 # define kernel function kernel = GPy . kern . RBF ( input_dim = 1 , variance = 1. , lengthscale = 1. ) # initialize GP model gpr_model = GPy . models . GPRegression ( Xtrain , ytrain , kern = kernel ) ** Training ** \u00b6 # train GP model m . optimize ( messages = True ) ** Predictions ** \u00b6 # get predictions y_pred , y_std = gpr_model . predict ( Xtest ) So as you can see, the API is very similar to the scikit-learn API with some small differences; the main one being that you have to initiate the GP model with the data. The rest is fairly similar. You should definitely take a look at the GPy docs if you are interested in some more advanced examples. GPyTorch \u00b6 This is my defacto library for applying GPs to large scale data. Anything above 10,000 points, and I will resort to this library. It has GPU acceleration and a large suite of different GP algorithms depending upon your problem. I think this is currently the dominant GP library for actually using GPs and I highly recommend it for utility. They have many options available ranging from latent variables to multi-outputs. Recently they've just revamped their entire library and documentation with some I still find it a bit difficult to really customize anything under the hood. But if you can figure out how to mix and match each of the modular parts, then it should work for you. Sample Code Snippet \u00b6 In GPyTorch, the library follows the pythonic way of coding that became super popular from deep learning frameworks such as Chainer and subsequently PyTorch. It consists of a 4 step process which is seen in the snippet below. ** Model ** \u00b6 class MyGP ( gpytorch . models . ExactGP ): def __init__ ( self , train_x , train_y , likelihood ): super () . __init__ ( train_x , train_y , likelihood ) # Mean Function self . mean_module = gpytorch . means . ZeroMean () # Kernel Function self . covar_module = gpytorch . kernels . ScaleKernel ( gpytorch . kernels . RBFKernel ()) def forward ( self , x ): mean = self . mean_module ( x ) covar = self . covar_module ( x ) return gpytorch . distributions . MultivariateNormal ( mean , covar ) # train_x = ...; train_y = ... likelihood = gpytorch . likelihoods . GaussianLikelihood () model = MyGP ( train_x , train_y , likelihood ) model = MyGP ( train_x , train_y , likelihood ) Source - GPyTorch Docs I am only scratching the surface with this quick snippet. But I wanted to highlight how this fits into Pyro \u00b6 This is my personal defacto library for doing research with GPs with PyTorch. In particular for GPs, I find the library to be super easy to mix and match priors and parameters for my GP models. I'm more comfortable with PyTorch so it was easy for me to Also pyro has a great forum which is very active and the devs are always willing to help. It is backed by Uber and built off of PyTorch so it has a strong dev community. I also talked to the devs at the ICML conference in 2019 and found that they were super open and passionate about the project. ** Model ** \u00b6 kernel2 = gp . kernels . RBF ( input_dim = 1 , variance = torch . tensor ( 0.1 ), lengthscale = torch . tensor ( 10. ) ) gpr_model = gp . models . GPRegression ( X , y , kernel2 , noise = torch . tensor ( 0.1 )) ** Training ** \u00b6 # define optimizer optimizer = torch . optim . Adam ( gpr . parameters (), lr = 0.005 ) # define loss function loss_fn = pyro . infer . Trace_ELBO () . differentiable_loss losses = [] num_steps = 1_000 # typical PyTorch boilerplate code for i in range ( num_steps ): optimizer . zero_grad () loss = loss_fn ( gpr . model , gpr . guide ) loss . backward () optimizer . step () Source : Pyro Docs GPFlow \u00b6 What Pyro is to PyTorch, GPFlow is to TensorFlow. This library is the successor to the GPy library. It is very comprehensive with a lot of SOTA algorithms. I definitely think ifA few of the devs from GPy went to GPFlow so it has a very similar style as GPy. But it is a lot cleaner due to the use of autograd which eliminates all of the code used to track the gradients. Many researchers use this library as a backend for their own research code so I would say it is the second most used library in the research domain. I didn't find it particularly easy to customize in tensorflow =<1.1 because of the session tracking which wasn't clear to me from the beginning. But now with the addition of tensorflow 2.0 and GPFlow adopting that new framework, I am eager to try it out again. They have a new public slack group so their network is going to grow hopefully. ** Model ** \u00b6 # define kernel function k = gpflow . kernels . Matern52 () # define GP model gpr_model = gpflow . models . GPR ( data = ( X , Y ), kernel = k , mean_function = None ) ** Training ** \u00b6 # define optimizer optimizer = gpflow . optimizers . Scipy () # define loss function def objective_closure (): return - m . log_marginal_likelihood () num_steps = 1_000 # optimize function opt_logs = opt . minimize ( objective_closure , m . trainable_variables , options = dict ( maxiter = 100 )) Source : GPFlow Docs TensorFlow Probability \u00b6 This library is built into Tensorflow already and they have a few GP modules that allow you to train GP algorithms. In edition, they have a keras-like GP layer which is very useful for using a GP as a final layer in probabilistic neural networks. The GP community is quite small for TFP so I haven't seen too many examples for this. ** Model ** \u00b6 # define kernel function kernel = tfp . math . psd_kernels . ExponentiatedQuadratic () # Define the model. model = tfp . layers . VariationalGaussianProcess ( num_inducing_points = 512 , kernel_provider = kernel ) ** Keras Training ** \u00b6 # Custom Loss Function loss = lambda y , rv_y : rv_y . variational_loss ( y , kl_weight = np . array ( batch_size , x . dtype ) / x . shape [ 0 ] ) # TF2.0 Keras Training Loop model . compile ( optimizer = tf . optimizers . Adam ( learning_rate = 0.01 ), loss = loss ) model . fit ( x , y , batch_size = batch_size , epochs = 1000 , verbose = False ) Edward2 \u00b6 This is the most exciting one in my opinion because this library will allow GPs (and Deep GPs) to be used for the most novice users and engineers. It features the GP and sparse GP as bayesian keras-like layers. So you can stack as many of them as you want and then call the keras model.fit() . With this API, we will be able to prototype very quickly and really start applying GPs out-of-the-box. I think this is a really great feature and will put GPs on the map because it doesn't get any easier than this. ** Model ** \u00b6 # define kernel function kernel = ExponentiatedQuadratic () # Define the model. model = ed . layers . SparseGaussianProcess ( 3 , num_inducing = 512 , covariance_fn = kernel ) predictions = model ( features ) ** Custom Training ** \u00b6 # Custom Loss Function def loss_fn ( features , labels ): preds = model ( features ) nll = - tf . reduce_mean ( predictions . distribution . log_prob ( labels )) kl = sum ( model . losses ) / total_dataset_size return nll + kl # TF2.0 Custom Training loop# num_steps = 1000 for _ in range ( num_steps ): with tf . GradientTape () as tape : loss = loss_fn ( features , labels ) gradients = tape . gradient ( loss , model . variables ) # use any optimizer here Other Libraries \u00b6 PyMC3 | PyMC4 MXFusion Stan GPU Support \u00b6 Package Backend GPU Support GPy Numpy \u2713 Scikit-Learn Numpy \u2717 PyMC3 Theano \u2713 TensorFlow (Probability) TensorFlow \u2713 Edward TensorFlow \u2713 GPFlow TensorFlow \u2713 Pyro.contrib PyTorch \u2713 GPyTorch PyTorch \u2713 PyMC4 TensorFlow \u2713 Algorithms Implemented \u00b6 Package GPy Scikit-Learn PyMC3 TensorFlow (Probability) GPFlow Pyro GPyTorch Exact \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Moment Matching GP \u2713 \u2717 \u2713 \u2717 S S \u2713 SparseGP - FITC \u2713 \u2717 \u2713 \u2717 \u2713 \u2713 \u2713 SparseGP - PEP \u2713 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 SparseSP - VFE \u2713 \u2717 \u2717 \u2717 \u2713 \u2713 \u2713 Variational GP \u2713 \u2717 \u2717 \u2713 \u2713 \u2713 \u2717 Stochastic Variational GP \u2713 \u2717 \u2717 S \u2713 \u2713 \u2713 Deep GP \u2717 \u2717 \u2717 S S \u2713 D Deep Kernel Learning \u2717 \u2717 \u2717 S S S \u2713 GPLVM \u2713 \u2717 \u2717 \u2717 \u2717 \u2713 \u2713 Bayesian GPLVM \u2713 \u2717 \u2717 \u2717 \u2713 \u2713 \u2713 SKI/KISS \u2717 \u2717 \u2717 \u2717 \u2717 \u2713 LOVE \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2713 Key Symbol Status \u2713 Implemented \u2717 Not Implemented D Development S Supported S(?) Maybe Supported","title":"Software"},{"location":"software/#software","text":"Software Library Classification Quick Overview Sklearn GPy GPyTorch Pyro GPFlow TensorFlow Probability Edward2 Other Libraries Library Classification GPU Support Algorithms Implemented Software for Gaussian processes (GPs) have really been improving for quite a while now. It is now a lot easier to not only to actually use the GP models, but also to modify them improve them.","title":"Software"},{"location":"software/#library-classification","text":"Photo Credit : Francois Chollet Tweet So how to classify a library's worth is impossible because it's completely subjective. But I like this chart by Francois Chollet who put the different depths a package can go to in order to create a package that caters to different users. But libraries typically can be classified on this spectrum. The same breakdown of Deep Learning algorithms into Models and Training can be done for GPs as well. Since GPs aren't super mainstream yet, most modern large scale GP libraries will fall in the fully flexible category. But recently, with the edition of TensorFlow probability and Edward2, we have more modern GPs that will fall into the Easy to use category (but not necessarily easy to train...).","title":"Library Classification"},{"location":"software/#quick-overview","text":"","title":"Quick Overview"},{"location":"software/#sklearn","text":"The GP implementation in the scikit-learn library are already sufficient to get people started with GPs in scikit-learn. Often times when I'm data wrangling and I'm exploring possible algorithms, I'll already have the sklearn library installed in my conda environment so I typically start there myself especially for datasets less than 2,000 points.","title":"Sklearn"},{"location":"software/#sample-code-snippet","text":"The sklearn implementation is as basic as it gets. If you are familiar with the scikit-learn API then you will have no problems using the GPR module. It's a three step process with very little things to change.","title":"Sample Code Snippet"},{"location":"software/#model","text":"# define kernel function kernel = \\ RBF ( length_scale = 1.0 ) \\ + WhiteKernel ( noise_level = 0.1 ) # initialize GP model gpr_model = GaussianProcessRegressor ( kernel = kernel_gpml , alpha = 0 , optimizer = None , normalize_y = True )","title":"** Model **"},{"location":"software/#training","text":"# train GP model gpr_model . fit ( Xtrain , ytrain )","title":"** Training **"},{"location":"software/#predictions","text":"# get predictions y_pred , y_std = gpr_model . predict ( Xtest , return_std = True ) Again, this is the simplest API you will find and for small data problems, you'll find that this works fine out-of-the-box. I highly recommend this when starting especially if you're not a GP connoisseur. What I showed above is as complicated as it gets. Any more customization outside of this is a bit difficult as the scikit-learn API for GPs isn't very modular and wasn't designed as such. But as a first pass, it's good enough.","title":"** Predictions **"},{"location":"software/#gpy","text":"GPy is the most comprehensive research library I have found to date. It has the most number of different special GP \"corner case\" algorithms of any package available. The GPy examples and tutorials are very comprehensive. The major caveat is that the documentation is very difficult to navigate. I also found the code base to be a bit difficult to really understand what's going on because there is no automatic differentiation to reduce the computations so there can be a bit of redundancy. I typically wrap some typical GP algorithms with some common parameters that I use within the sklearn .fit() , .predict() , .score() framework and call it a day. The standard algorithms will include the Exact GP, the Sparse GP, and Bayesian GPLVM. A warning though: this library does not get updated very often so you will likely run into very silly bugs if you don't use strict package versions that are recommended. There are rumors of a GPy2 library that's based on MXFusion but I have failed to see anything concrete yet. Idea : Some of the main algorithms such as the sparse GP implementations are mature enough to be dumped into the sklearn library. For small-medium data problems, I think this would be extremely beneficial to the community. Some of the key papers like the (e.g. the FITC-SGP , VFE-SGP , Heteroscedastic GP , GP-LVM ) certainly pass some of the strict sklearn criteria . But I suspect that it wouldn't be a joy to code because you would need to do some of the gradients from scratch. I do feel like it might make GPs a bit more popular if some of the mainstream methods were included in the scikit-learn library.","title":"GPy"},{"location":"software/#sample-code-snippet_1","text":"The GPy implementation is also very basic. If you are familiar with the scikit-learn API then you will have no problems using the GPR module. It's a three step process with very little things to change.","title":"Sample Code Snippet"},{"location":"software/#model_1","text":"# define kernel function kernel = GPy . kern . RBF ( input_dim = 1 , variance = 1. , lengthscale = 1. ) # initialize GP model gpr_model = GPy . models . GPRegression ( Xtrain , ytrain , kern = kernel )","title":"** Model **"},{"location":"software/#training_1","text":"# train GP model m . optimize ( messages = True )","title":"** Training **"},{"location":"software/#predictions_1","text":"# get predictions y_pred , y_std = gpr_model . predict ( Xtest ) So as you can see, the API is very similar to the scikit-learn API with some small differences; the main one being that you have to initiate the GP model with the data. The rest is fairly similar. You should definitely take a look at the GPy docs if you are interested in some more advanced examples.","title":"** Predictions **"},{"location":"software/#gpytorch","text":"This is my defacto library for applying GPs to large scale data. Anything above 10,000 points, and I will resort to this library. It has GPU acceleration and a large suite of different GP algorithms depending upon your problem. I think this is currently the dominant GP library for actually using GPs and I highly recommend it for utility. They have many options available ranging from latent variables to multi-outputs. Recently they've just revamped their entire library and documentation with some I still find it a bit difficult to really customize anything under the hood. But if you can figure out how to mix and match each of the modular parts, then it should work for you.","title":"GPyTorch"},{"location":"software/#sample-code-snippet_2","text":"In GPyTorch, the library follows the pythonic way of coding that became super popular from deep learning frameworks such as Chainer and subsequently PyTorch. It consists of a 4 step process which is seen in the snippet below.","title":"Sample Code Snippet"},{"location":"software/#model_2","text":"class MyGP ( gpytorch . models . ExactGP ): def __init__ ( self , train_x , train_y , likelihood ): super () . __init__ ( train_x , train_y , likelihood ) # Mean Function self . mean_module = gpytorch . means . ZeroMean () # Kernel Function self . covar_module = gpytorch . kernels . ScaleKernel ( gpytorch . kernels . RBFKernel ()) def forward ( self , x ): mean = self . mean_module ( x ) covar = self . covar_module ( x ) return gpytorch . distributions . MultivariateNormal ( mean , covar ) # train_x = ...; train_y = ... likelihood = gpytorch . likelihoods . GaussianLikelihood () model = MyGP ( train_x , train_y , likelihood ) model = MyGP ( train_x , train_y , likelihood ) Source - GPyTorch Docs I am only scratching the surface with this quick snippet. But I wanted to highlight how this fits into","title":"** Model **"},{"location":"software/#pyro","text":"This is my personal defacto library for doing research with GPs with PyTorch. In particular for GPs, I find the library to be super easy to mix and match priors and parameters for my GP models. I'm more comfortable with PyTorch so it was easy for me to Also pyro has a great forum which is very active and the devs are always willing to help. It is backed by Uber and built off of PyTorch so it has a strong dev community. I also talked to the devs at the ICML conference in 2019 and found that they were super open and passionate about the project.","title":"Pyro"},{"location":"software/#model_3","text":"kernel2 = gp . kernels . RBF ( input_dim = 1 , variance = torch . tensor ( 0.1 ), lengthscale = torch . tensor ( 10. ) ) gpr_model = gp . models . GPRegression ( X , y , kernel2 , noise = torch . tensor ( 0.1 ))","title":"** Model **"},{"location":"software/#training_2","text":"# define optimizer optimizer = torch . optim . Adam ( gpr . parameters (), lr = 0.005 ) # define loss function loss_fn = pyro . infer . Trace_ELBO () . differentiable_loss losses = [] num_steps = 1_000 # typical PyTorch boilerplate code for i in range ( num_steps ): optimizer . zero_grad () loss = loss_fn ( gpr . model , gpr . guide ) loss . backward () optimizer . step () Source : Pyro Docs","title":"** Training **"},{"location":"software/#gpflow","text":"What Pyro is to PyTorch, GPFlow is to TensorFlow. This library is the successor to the GPy library. It is very comprehensive with a lot of SOTA algorithms. I definitely think ifA few of the devs from GPy went to GPFlow so it has a very similar style as GPy. But it is a lot cleaner due to the use of autograd which eliminates all of the code used to track the gradients. Many researchers use this library as a backend for their own research code so I would say it is the second most used library in the research domain. I didn't find it particularly easy to customize in tensorflow =<1.1 because of the session tracking which wasn't clear to me from the beginning. But now with the addition of tensorflow 2.0 and GPFlow adopting that new framework, I am eager to try it out again. They have a new public slack group so their network is going to grow hopefully.","title":"GPFlow"},{"location":"software/#model_4","text":"# define kernel function k = gpflow . kernels . Matern52 () # define GP model gpr_model = gpflow . models . GPR ( data = ( X , Y ), kernel = k , mean_function = None )","title":"** Model **"},{"location":"software/#training_3","text":"# define optimizer optimizer = gpflow . optimizers . Scipy () # define loss function def objective_closure (): return - m . log_marginal_likelihood () num_steps = 1_000 # optimize function opt_logs = opt . minimize ( objective_closure , m . trainable_variables , options = dict ( maxiter = 100 )) Source : GPFlow Docs","title":"** Training **"},{"location":"software/#tensorflow-probability","text":"This library is built into Tensorflow already and they have a few GP modules that allow you to train GP algorithms. In edition, they have a keras-like GP layer which is very useful for using a GP as a final layer in probabilistic neural networks. The GP community is quite small for TFP so I haven't seen too many examples for this.","title":"TensorFlow Probability"},{"location":"software/#model_5","text":"# define kernel function kernel = tfp . math . psd_kernels . ExponentiatedQuadratic () # Define the model. model = tfp . layers . VariationalGaussianProcess ( num_inducing_points = 512 , kernel_provider = kernel )","title":"** Model **"},{"location":"software/#keras-training","text":"# Custom Loss Function loss = lambda y , rv_y : rv_y . variational_loss ( y , kl_weight = np . array ( batch_size , x . dtype ) / x . shape [ 0 ] ) # TF2.0 Keras Training Loop model . compile ( optimizer = tf . optimizers . Adam ( learning_rate = 0.01 ), loss = loss ) model . fit ( x , y , batch_size = batch_size , epochs = 1000 , verbose = False )","title":"** Keras Training **"},{"location":"software/#edward2","text":"This is the most exciting one in my opinion because this library will allow GPs (and Deep GPs) to be used for the most novice users and engineers. It features the GP and sparse GP as bayesian keras-like layers. So you can stack as many of them as you want and then call the keras model.fit() . With this API, we will be able to prototype very quickly and really start applying GPs out-of-the-box. I think this is a really great feature and will put GPs on the map because it doesn't get any easier than this.","title":"Edward2"},{"location":"software/#model_6","text":"# define kernel function kernel = ExponentiatedQuadratic () # Define the model. model = ed . layers . SparseGaussianProcess ( 3 , num_inducing = 512 , covariance_fn = kernel ) predictions = model ( features )","title":"** Model **"},{"location":"software/#custom-training","text":"# Custom Loss Function def loss_fn ( features , labels ): preds = model ( features ) nll = - tf . reduce_mean ( predictions . distribution . log_prob ( labels )) kl = sum ( model . losses ) / total_dataset_size return nll + kl # TF2.0 Custom Training loop# num_steps = 1000 for _ in range ( num_steps ): with tf . GradientTape () as tape : loss = loss_fn ( features , labels ) gradients = tape . gradient ( loss , model . variables ) # use any optimizer here","title":"** Custom Training **"},{"location":"software/#other-libraries","text":"PyMC3 | PyMC4 MXFusion Stan","title":"Other Libraries"},{"location":"software/#gpu-support","text":"Package Backend GPU Support GPy Numpy \u2713 Scikit-Learn Numpy \u2717 PyMC3 Theano \u2713 TensorFlow (Probability) TensorFlow \u2713 Edward TensorFlow \u2713 GPFlow TensorFlow \u2713 Pyro.contrib PyTorch \u2713 GPyTorch PyTorch \u2713 PyMC4 TensorFlow \u2713","title":"GPU Support"},{"location":"software/#algorithms-implemented","text":"Package GPy Scikit-Learn PyMC3 TensorFlow (Probability) GPFlow Pyro GPyTorch Exact \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Moment Matching GP \u2713 \u2717 \u2713 \u2717 S S \u2713 SparseGP - FITC \u2713 \u2717 \u2713 \u2717 \u2713 \u2713 \u2713 SparseGP - PEP \u2713 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 SparseSP - VFE \u2713 \u2717 \u2717 \u2717 \u2713 \u2713 \u2713 Variational GP \u2713 \u2717 \u2717 \u2713 \u2713 \u2713 \u2717 Stochastic Variational GP \u2713 \u2717 \u2717 S \u2713 \u2713 \u2713 Deep GP \u2717 \u2717 \u2717 S S \u2713 D Deep Kernel Learning \u2717 \u2717 \u2717 S S S \u2713 GPLVM \u2713 \u2717 \u2717 \u2717 \u2717 \u2713 \u2713 Bayesian GPLVM \u2713 \u2717 \u2717 \u2717 \u2713 \u2713 \u2713 SKI/KISS \u2717 \u2717 \u2717 \u2717 \u2717 \u2713 LOVE \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2713 Key Symbol Status \u2713 Implemented \u2717 Not Implemented D Development S Supported S(?) Maybe Supported","title":"Algorithms Implemented"},{"location":"literature/applications/","text":"Applications \u00b6 Earth Observation \u00b6 All papers in this section look at applications in Earth observation e.g. parameter retrieval, time-scale modeling, remote sensing, and any data dealing with multi/hyperspectral imagery. A Survey on Gaussian Processes for Earth-Observation Data Analysis - Camps-Valls et. al. (2016) | Paper This survey introduces GPs in a fairly simple way and lists a few basic GP algorithms. It also goes over some applications such as parameter retrieval, emulation and feature ranking. It's a great overview of GPs in the field from standard GPs. The software recommendations are outdated. I suggest you check out the GPy library for all GP algorithms mentioned in the paper. Also the treatment of sparse GPs (especially Matrix-Vector-Multiplications (MVNs)) is very brief and underrated. See this for more information about large scale GPs. Gaussian Processes for Vegetation Parameter Estimation from Hyperspectral Data with Limited Ground Truth - Gewali et. al. (2019) | Paper Physics-Aware \u00b6 Physics-aware Gaussian processes in remote sensing - Camps-Valls et. al. (2018) | Paper Other Arguments \u00b6 GPs for Little Data","title":"Applications"},{"location":"literature/applications/#applications","text":"","title":"Applications"},{"location":"literature/applications/#earth-observation","text":"All papers in this section look at applications in Earth observation e.g. parameter retrieval, time-scale modeling, remote sensing, and any data dealing with multi/hyperspectral imagery. A Survey on Gaussian Processes for Earth-Observation Data Analysis - Camps-Valls et. al. (2016) | Paper This survey introduces GPs in a fairly simple way and lists a few basic GP algorithms. It also goes over some applications such as parameter retrieval, emulation and feature ranking. It's a great overview of GPs in the field from standard GPs. The software recommendations are outdated. I suggest you check out the GPy library for all GP algorithms mentioned in the paper. Also the treatment of sparse GPs (especially Matrix-Vector-Multiplications (MVNs)) is very brief and underrated. See this for more information about large scale GPs. Gaussian Processes for Vegetation Parameter Estimation from Hyperspectral Data with Limited Ground Truth - Gewali et. al. (2019) | Paper","title":"Earth Observation"},{"location":"literature/applications/#physics-aware","text":"Physics-aware Gaussian processes in remote sensing - Camps-Valls et. al. (2018) | Paper","title":"Physics-Aware"},{"location":"literature/applications/#other-arguments","text":"GPs for Little Data","title":"Other Arguments"},{"location":"literature/components/","text":"Components \u00b6 This is a poor name but I want to put all of the papers here that seek to improve specific components within the GP algorithms, e.g. the inference scheme. Table of Contents Components Variational Inference Variational Evidence Lower Bound (ELBO) Natural Gradients (NGs) Importance Weighted Variational Inference (IWVI) Predictive Log Likelihood (PLL) Generalized Variational Inference (GVI) Gradient Descent Regimes Regularization Variational Inference \u00b6 This section outlines a few interesting papers I found where they are trying to improve how we do variational inference. I try to stick to methods where people have tried and succeeded at applying them to GPs. Below are a few key SOTA objective functions that you may come across in the GP literature. The most common is definitely the Variational ELBO but there are a few unknown objective functions that came out recently and I think they might be useful in the future. We just need to get them implemented and tested. Along the way there have been other modifications. Variational Evidence Lower Bound (ELBO) \u00b6 This is the standard objective function that you will find the literature. $$ \\mathcal{L}_{ELBO} = \\sum_{i=1}^{N} \\mathbb{E}_{q(\\mathbf{u})} \\left[ \\mathbb{E}_{f(f|\\mathbf{u})} \\left[ \\log p(y_i | f_i) \\right] \\right] - \\beta D_{KL}\\left[ q(\\mathbf{u} || p(\\mathbf{u})) \\right] $$ where: * $N$ - number of data points * $p(\\mathbf{u})$ - prior distribution for the inducing function values * $q(\\mathbf{u})$ - variational distribution for the inducing function values * $\\beta$ - free parameter for the $D_{KL}$ regularization penalization Scalable Variational Gaussian Process Classification - Hensman et. al. (2015) Natural Gradients (NGs) \u00b6 Natural Gradients in Practice: Non-Conjugate Variational Inference in Gaussian Process Models - Salimbeni et. al. (2018) | Code This paper argues that training sparse GP algorithms with gradient descent can be quite slow due to the need to optimize the variational parameters q_\\phi(u) q_\\phi(u) as well as the model parameters. So they propose to use the natural gradient for the variational parameters and then the standard gradient methods for the remaining parameters. They show that the SVGP and the DGP methods all converge much faster with this training regime. I imagine this would also be super useful for the BayesianGPLVM where we also have variational parameters for our inputs as well. * Noisy Natural Gradient as Variational Inference - Zhang (2018) - Code * PyTorch Importance Weighted Variational Inference (IWVI) \u00b6 They propose a way to do importance sampling coupled with variational inference to improve single layer and multi-layer GPs and have shown that they can get equivalent or better results than just standard variational inference. Deep Gaussian Processes with Importance-Weighted Variational Inference - Salimbeni et. al. (2019) * Paper | Code | Video | Poster | ICML 2019 Slides | Workshop Slides * Importance Weighting and Variational Inference - Domke & Sheldon (2018) Predictive Log Likelihood (PLL) \u00b6 $$ \\begin{aligned} \\mathcal{L}_{PLL} &= \\mathbb{E}_{p_{data}(\\mathbf{y}, \\mathbf{x})} \\left[ \\log p(\\mathbf{y|x})\\right] - \\beta D_{KL}\\left[ q(\\mathbf{u}|| p(\\mathbf{u})\\right] \\\\ &\\approx \\sum_{i=1}^{N} \\log \\mathbb{E}_{q(\\mathbf{u})} \\left[ \\int p(y_i |f_i) p(f_i | \\mathbf{u})df_i \\right] - \\beta D_{KL}\\left[ q(\\mathbf{u}) || p(\\mathbf{u}) \\right] \\end{aligned} $$ where: * $N$ - number of data points * $p(\\mathbf{u})$ - prior distribution for the inducing function values * $q(\\mathbf{u})$ - variational distribution for the inducing function values Sparse Gaussian Process Regression Beyond Variational Inference - Jankowiak et. al. (2019) Generalized Variational Inference (GVI) \u00b6 Generalized Variational Inference - Knoblauch et. al. (2019) A generalized Bayesian inference framework. It goes into a different variational family related to Renyi's family of Information theoretic methods; which isn't very typical because normally we look at the Shannon perspective. They had success applying it to Bayesian Neural Networks and Deep Gaussian Processes. * Deep GP paper Gradient Descent Regimes \u00b6 Parallel training of DNNs with Natural Gradient and Parameter Averaging - Povey et. al. (2014) | Code | Blog A seamingly drop-in replacement for stochastic gradient descent with some added benefits of being shown to improve generalization tasks, stability of the training, and can help obtain high quality uncertainty estimates. Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm - Lui & Wang (2016) A tractable approach for learning high dimensional prob dist using Functional Gradient Descent in RKHS. It's from a connection with the derivative of the KL divergence and the Stein's identity. * Stein's Method Webpage * Pyro Implementation Regularization \u00b6 Regularized Sparse Gaussian Processes - Meng & Lee (2019) [ arxiv ] Impose a regularization coefficient on the KL term in the Sparse GP implementation. Addresses issue where the distribution of the inducing inputs fail to capture the distribution of the training inputs.","title":"Components"},{"location":"literature/components/#components","text":"This is a poor name but I want to put all of the papers here that seek to improve specific components within the GP algorithms, e.g. the inference scheme. Table of Contents Components Variational Inference Variational Evidence Lower Bound (ELBO) Natural Gradients (NGs) Importance Weighted Variational Inference (IWVI) Predictive Log Likelihood (PLL) Generalized Variational Inference (GVI) Gradient Descent Regimes Regularization","title":"Components"},{"location":"literature/components/#variational-inference","text":"This section outlines a few interesting papers I found where they are trying to improve how we do variational inference. I try to stick to methods where people have tried and succeeded at applying them to GPs. Below are a few key SOTA objective functions that you may come across in the GP literature. The most common is definitely the Variational ELBO but there are a few unknown objective functions that came out recently and I think they might be useful in the future. We just need to get them implemented and tested. Along the way there have been other modifications.","title":"Variational Inference"},{"location":"literature/components/#variational-evidence-lower-bound-elbo","text":"This is the standard objective function that you will find the literature. $$ \\mathcal{L}_{ELBO} = \\sum_{i=1}^{N} \\mathbb{E}_{q(\\mathbf{u})} \\left[ \\mathbb{E}_{f(f|\\mathbf{u})} \\left[ \\log p(y_i | f_i) \\right] \\right] - \\beta D_{KL}\\left[ q(\\mathbf{u} || p(\\mathbf{u})) \\right] $$ where: * $N$ - number of data points * $p(\\mathbf{u})$ - prior distribution for the inducing function values * $q(\\mathbf{u})$ - variational distribution for the inducing function values * $\\beta$ - free parameter for the $D_{KL}$ regularization penalization Scalable Variational Gaussian Process Classification - Hensman et. al. (2015)","title":"Variational Evidence Lower Bound (ELBO)"},{"location":"literature/components/#natural-gradients-ngs","text":"Natural Gradients in Practice: Non-Conjugate Variational Inference in Gaussian Process Models - Salimbeni et. al. (2018) | Code This paper argues that training sparse GP algorithms with gradient descent can be quite slow due to the need to optimize the variational parameters q_\\phi(u) q_\\phi(u) as well as the model parameters. So they propose to use the natural gradient for the variational parameters and then the standard gradient methods for the remaining parameters. They show that the SVGP and the DGP methods all converge much faster with this training regime. I imagine this would also be super useful for the BayesianGPLVM where we also have variational parameters for our inputs as well. * Noisy Natural Gradient as Variational Inference - Zhang (2018) - Code * PyTorch","title":"Natural Gradients (NGs)"},{"location":"literature/components/#importance-weighted-variational-inference-iwvi","text":"They propose a way to do importance sampling coupled with variational inference to improve single layer and multi-layer GPs and have shown that they can get equivalent or better results than just standard variational inference. Deep Gaussian Processes with Importance-Weighted Variational Inference - Salimbeni et. al. (2019) * Paper | Code | Video | Poster | ICML 2019 Slides | Workshop Slides * Importance Weighting and Variational Inference - Domke & Sheldon (2018)","title":"Importance Weighted Variational Inference (IWVI)"},{"location":"literature/components/#predictive-log-likelihood-pll","text":"$$ \\begin{aligned} \\mathcal{L}_{PLL} &= \\mathbb{E}_{p_{data}(\\mathbf{y}, \\mathbf{x})} \\left[ \\log p(\\mathbf{y|x})\\right] - \\beta D_{KL}\\left[ q(\\mathbf{u}|| p(\\mathbf{u})\\right] \\\\ &\\approx \\sum_{i=1}^{N} \\log \\mathbb{E}_{q(\\mathbf{u})} \\left[ \\int p(y_i |f_i) p(f_i | \\mathbf{u})df_i \\right] - \\beta D_{KL}\\left[ q(\\mathbf{u}) || p(\\mathbf{u}) \\right] \\end{aligned} $$ where: * $N$ - number of data points * $p(\\mathbf{u})$ - prior distribution for the inducing function values * $q(\\mathbf{u})$ - variational distribution for the inducing function values Sparse Gaussian Process Regression Beyond Variational Inference - Jankowiak et. al. (2019)","title":"Predictive Log Likelihood (PLL)"},{"location":"literature/components/#generalized-variational-inference-gvi","text":"Generalized Variational Inference - Knoblauch et. al. (2019) A generalized Bayesian inference framework. It goes into a different variational family related to Renyi's family of Information theoretic methods; which isn't very typical because normally we look at the Shannon perspective. They had success applying it to Bayesian Neural Networks and Deep Gaussian Processes. * Deep GP paper","title":"Generalized Variational Inference (GVI)"},{"location":"literature/components/#gradient-descent-regimes","text":"Parallel training of DNNs with Natural Gradient and Parameter Averaging - Povey et. al. (2014) | Code | Blog A seamingly drop-in replacement for stochastic gradient descent with some added benefits of being shown to improve generalization tasks, stability of the training, and can help obtain high quality uncertainty estimates. Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm - Lui & Wang (2016) A tractable approach for learning high dimensional prob dist using Functional Gradient Descent in RKHS. It's from a connection with the derivative of the KL divergence and the Stein's identity. * Stein's Method Webpage * Pyro Implementation","title":"Gradient Descent Regimes"},{"location":"literature/components/#regularization","text":"Regularized Sparse Gaussian Processes - Meng & Lee (2019) [ arxiv ] Impose a regularization coefficient on the KL term in the Sparse GP implementation. Addresses issue where the distribution of the inducing inputs fail to capture the distribution of the training inputs.","title":"Regularization"},{"location":"literature/deep_gps/","text":"Deep Gaussian Processes \u00b6 These are GP models that stack GPs one after the other. As far as understanding, the best would be lectures as I have highlighted below. Resources \u00b6 Neil Lawrence @ MLSS 2019 > I would say this is the best lecture to understand the nature of GPs and why we would might want to use them. Blog | Lecture | Slides Neil Lawrence @ GPSS 2019 Notes | Lecture Maurizio Filippone @ DeepBayes.ru 2018 I would say this is the second best lecture because Maurizio gives a nice overview of the GP methods there already are (at the time). Lecture | Slides | New Slides Algorithms \u00b6 The literature isn\u2019t so big but there a number of different implementations depending on the lab: Variational Inference This is the most popular method and has been pursued the most. It's also the implementation that you will find standard in libraries like GPyTorch, GPFlow and Pyro. Expectation Propagation This group used expectation propagation to train the GP. They haven't really done so much since then and I'm not entirely sure why this line of the DGP has gone a bit dry. It would be nice if they resumed. I suspect it may be because of the software. I haven't seen too much software that focuses on clever expectation propagation schemes; they mainly focus on variational inference and MC sampling schemes. MC sampling One lab has tackled this where you can use some variants of MC sampling to train a Deep GP. You'll find this standard in many GP libraries because it's fairly easy to integrate in almost any scheme. MC sampling is famous for being slow but the community is working on it. I imagine a break-through is bound to happen. Random Feature Expansions This uses RFF to approximate a GP and then stacks these on top. I find this a big elegent and probably the simplest. But I didn't see too much research on the tiny bits of the algorithm like the training or the initialization procedures. I don\u2019t think there is any best one because I\u2019m almost certain noone has done any complete comparison. I can say that the VI one is the most studied because that lab is still working on it. In the meantime, personally I would try to use implementations in standard libraries where the devs have ironed out the bugs and allowed for easy customization and configuration; so basically the doubly stochastic. Variational Inference \u00b6 Deep Gaussian Processes - Damianou & Lawrence (2013) This paper is the original method of Deep GPs. It might not be useful for production but there are still many insights to be had from the originators. Code Nested Variational Compression in Deep Gaussian Processes - Hensman & Lawrence (2014) Doubly Stochastic Variational Inference for Deep Gaussian Processes - Salimbeni & Deisenroth (2017) This paper uses stochastic gradient descent for training the Deep GP. I think this achieves the state-of-the-art results thus far. It also has the most implementations in the standard literature. Code | Pyro | GPyTorch Random Fourier Features \u00b6 Random Feature Expansions for Deep Gaussian Processes - Cutjar et. al. (2017) This implementation uses ideas from random fourier features in conjunction with Deep GPs. Paper II | Video | Code Lecture I | Slides | Lecture (Maurizio) | Slides | Code MC Sampling \u00b6 Learning deep latent Gaussian models with Markov chain Monte Carlo - Hoffman (2017) Inference in Deep Gaussian Processes Using Stochastic Gradient Hamiltonian Monte Carlo - Havasi et. al. (2018) Expectation Propagation \u00b6 Deep Gaussian Processes for Regression using Approximate Expectation Propagation - Bui et. al. (2016) This paper uses an approximate expectation method for the inference in Deep GPs. Paper | Code Hybrids \u00b6 Deep Gaussian Processes with Importance-Weighted Variational Inference - Salimbeni et. al. (2019) This paper uses the idea that our noisy inputs are instead 'latent covariates' instead of additive noise or that our input itself is a latent covariate. They also propose a way to do importance sampling coupled with variational inference to improve single layer and multiple layer GPs and have shown that they can get equivalent or better results than just standard variational inference. The latent variables alone will improve performance for both the IWVI and the VI training procedures. * Paper | Code | Video | Poster | ICML 2019 Slides | Workshop Slides Insights \u00b6 Problems with Deep GPs \u00b6 Deep Gaussian Process Pathologies - Paper This paper shows how some of the kernel compositions give very bad estimates of the functions between layers; similar to how residual NN do much better.","title":"Deep Gaussian Processes"},{"location":"literature/deep_gps/#deep-gaussian-processes","text":"These are GP models that stack GPs one after the other. As far as understanding, the best would be lectures as I have highlighted below.","title":"Deep Gaussian Processes"},{"location":"literature/deep_gps/#resources","text":"Neil Lawrence @ MLSS 2019 > I would say this is the best lecture to understand the nature of GPs and why we would might want to use them. Blog | Lecture | Slides Neil Lawrence @ GPSS 2019 Notes | Lecture Maurizio Filippone @ DeepBayes.ru 2018 I would say this is the second best lecture because Maurizio gives a nice overview of the GP methods there already are (at the time). Lecture | Slides | New Slides","title":"Resources"},{"location":"literature/deep_gps/#algorithms","text":"The literature isn\u2019t so big but there a number of different implementations depending on the lab: Variational Inference This is the most popular method and has been pursued the most. It's also the implementation that you will find standard in libraries like GPyTorch, GPFlow and Pyro. Expectation Propagation This group used expectation propagation to train the GP. They haven't really done so much since then and I'm not entirely sure why this line of the DGP has gone a bit dry. It would be nice if they resumed. I suspect it may be because of the software. I haven't seen too much software that focuses on clever expectation propagation schemes; they mainly focus on variational inference and MC sampling schemes. MC sampling One lab has tackled this where you can use some variants of MC sampling to train a Deep GP. You'll find this standard in many GP libraries because it's fairly easy to integrate in almost any scheme. MC sampling is famous for being slow but the community is working on it. I imagine a break-through is bound to happen. Random Feature Expansions This uses RFF to approximate a GP and then stacks these on top. I find this a big elegent and probably the simplest. But I didn't see too much research on the tiny bits of the algorithm like the training or the initialization procedures. I don\u2019t think there is any best one because I\u2019m almost certain noone has done any complete comparison. I can say that the VI one is the most studied because that lab is still working on it. In the meantime, personally I would try to use implementations in standard libraries where the devs have ironed out the bugs and allowed for easy customization and configuration; so basically the doubly stochastic.","title":"Algorithms"},{"location":"literature/deep_gps/#variational-inference","text":"Deep Gaussian Processes - Damianou & Lawrence (2013) This paper is the original method of Deep GPs. It might not be useful for production but there are still many insights to be had from the originators. Code Nested Variational Compression in Deep Gaussian Processes - Hensman & Lawrence (2014) Doubly Stochastic Variational Inference for Deep Gaussian Processes - Salimbeni & Deisenroth (2017) This paper uses stochastic gradient descent for training the Deep GP. I think this achieves the state-of-the-art results thus far. It also has the most implementations in the standard literature. Code | Pyro | GPyTorch","title":"Variational Inference"},{"location":"literature/deep_gps/#random-fourier-features","text":"Random Feature Expansions for Deep Gaussian Processes - Cutjar et. al. (2017) This implementation uses ideas from random fourier features in conjunction with Deep GPs. Paper II | Video | Code Lecture I | Slides | Lecture (Maurizio) | Slides | Code","title":"Random Fourier Features"},{"location":"literature/deep_gps/#mc-sampling","text":"Learning deep latent Gaussian models with Markov chain Monte Carlo - Hoffman (2017) Inference in Deep Gaussian Processes Using Stochastic Gradient Hamiltonian Monte Carlo - Havasi et. al. (2018)","title":"MC Sampling"},{"location":"literature/deep_gps/#expectation-propagation","text":"Deep Gaussian Processes for Regression using Approximate Expectation Propagation - Bui et. al. (2016) This paper uses an approximate expectation method for the inference in Deep GPs. Paper | Code","title":"Expectation Propagation"},{"location":"literature/deep_gps/#hybrids","text":"Deep Gaussian Processes with Importance-Weighted Variational Inference - Salimbeni et. al. (2019) This paper uses the idea that our noisy inputs are instead 'latent covariates' instead of additive noise or that our input itself is a latent covariate. They also propose a way to do importance sampling coupled with variational inference to improve single layer and multiple layer GPs and have shown that they can get equivalent or better results than just standard variational inference. The latent variables alone will improve performance for both the IWVI and the VI training procedures. * Paper | Code | Video | Poster | ICML 2019 Slides | Workshop Slides","title":"Hybrids"},{"location":"literature/deep_gps/#insights","text":"","title":"Insights"},{"location":"literature/deep_gps/#problems-with-deep-gps","text":"Deep Gaussian Process Pathologies - Paper This paper shows how some of the kernel compositions give very bad estimates of the functions between layers; similar to how residual NN do much better.","title":"Problems with Deep GPs"},{"location":"literature/fourier/","text":"Sparse Spectrum Gaussian Processes \u00b6 These are essentially the analogue to the random fourier features for Gaussian processes. SSGP \u00b6 Sparse Spectrum Gaussian Process Regression - L\u00e1zaro-Gredilla et. al. (2010) - PDF The original algorithm for SSGP. Prediction under Uncertainty in Sparse Spectrum Gaussian Processes with Applications to Filtering and Control - Pan et. al. (2017) - PDF > This is a moment matching extension to deal with the uncertainty in the inputs at prediction time. Python Implementation Numpy GPFlow Variational SSGPs \u00b6 So according to this paper the SSGP algorithm had a tendency to overfit. So they added some additional parameters to account for the noise in the inputs making the marginal likelihood term intractable. They added variational methods to deal with the 1. Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs - Gal et. al. (2015) \"...proposed variational inference in a sparse spectrum model that is derived from a GP model.\" - Hensman et. al. (2018) 2. Variational Fourier Features for Gaussian Processes - Hensman et al (2018) \"...our work aims to directly approximate the posterior of the true models using a variational representation.\" - Hensman et. al. (2018) Yarin Gal's Stuff - website Code Numpy Theano Uncertain Inputs \u00b6 I've only seen one paper that attempts to extend this method to account for uncertain inputs. Prediction under Uncertainty in Sparse Spectrum Gaussian Processes with Applications to Filtering and Control - Pan et. al. (2017) This is the only paper I've seen that tries to extend this method Latest \u00b6 Know Your Boundaries: Constraining Gaussian Processes by Variational Harmonic Features - Solin & Kok (2019)","title":"Fourier"},{"location":"literature/fourier/#sparse-spectrum-gaussian-processes","text":"These are essentially the analogue to the random fourier features for Gaussian processes.","title":"Sparse Spectrum Gaussian Processes"},{"location":"literature/fourier/#ssgp","text":"Sparse Spectrum Gaussian Process Regression - L\u00e1zaro-Gredilla et. al. (2010) - PDF The original algorithm for SSGP. Prediction under Uncertainty in Sparse Spectrum Gaussian Processes with Applications to Filtering and Control - Pan et. al. (2017) - PDF > This is a moment matching extension to deal with the uncertainty in the inputs at prediction time. Python Implementation Numpy GPFlow","title":"SSGP"},{"location":"literature/fourier/#variational-ssgps","text":"So according to this paper the SSGP algorithm had a tendency to overfit. So they added some additional parameters to account for the noise in the inputs making the marginal likelihood term intractable. They added variational methods to deal with the 1. Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs - Gal et. al. (2015) \"...proposed variational inference in a sparse spectrum model that is derived from a GP model.\" - Hensman et. al. (2018) 2. Variational Fourier Features for Gaussian Processes - Hensman et al (2018) \"...our work aims to directly approximate the posterior of the true models using a variational representation.\" - Hensman et. al. (2018) Yarin Gal's Stuff - website Code Numpy Theano","title":"Variational SSGPs"},{"location":"literature/fourier/#uncertain-inputs","text":"I've only seen one paper that attempts to extend this method to account for uncertain inputs. Prediction under Uncertainty in Sparse Spectrum Gaussian Processes with Applications to Filtering and Control - Pan et. al. (2017) This is the only paper I've seen that tries to extend this method","title":"Uncertain Inputs"},{"location":"literature/fourier/#latest","text":"Know Your Boundaries: Constraining Gaussian Processes by Variational Harmonic Features - Solin & Kok (2019)","title":"Latest"},{"location":"literature/gps/","text":"Gaussian Processes \u00b6 Best Lectures Best Visualize Explanations Best Books 1. Standard Book 2. Better Book 3. Brief Overview Best Thesis Explanation Code Introductions From Scratch Using Libraries Other Resources Previously Compiled Stuff This is fairly subjective but I have included the best tutorials that I could find for GPs on the internet. I focused on clarity and detail and I'm partial to resources with lots of plots. The resources range from lectures to blogs so hopefully by the end of this page you will find something that will ease you into understanding Gaussian processes. Best Lectures \u00b6 By far the best beginners lecture on GPs (that I have found) is by Neil Lawrence ; a prominent figure that you should know once you enter this field. The lecture video that I have included below are the most recent of his lectures and I personally think it gives the most intuition behind GPs without being too mathy. The blog that's listed are his entire lecture notes in notebook format so you can read more or less verbatim what was said in the lecture; although you might miss out on the nuggets of wisdom he tends to drop during his lectures. Neil Lawrence Lecture @ MLSS 2019 - Blog | Lecture | Slides I would say that the best slides that I have found are by Marc Deisenroth . Unfortunately, I cannot find the video lectures online. I think these lecture slides are by far the best I've seen. It's fairly math intensive but there arent't too many proofs. These don't have proofs so you'll have to look somewhere else for that. Bishop's book (below) will be able to fill in any gaps. He also has a distill.pub page which has a nice introduction with a few practical tips and tricks to use. Foundations of Machine Learning: GPs - Deisenroth (2018-2019) - Slides | Practical Guide to GPs Best Visualize Explanations \u00b6 If you are a visual person (like me) then you will appreciate resources where they go through step-by-step how a Gaussian process is formulated as well as the importance of the kernel and how one can train them. A Visual Exploration of Gaussian Processes - G\u00f6rtler et al. (2019) Best Books \u00b6 1. Standard Book \u00b6 Gaussian Processes for Machine Learning - Rasmussen (2006) This is the standard book that everyone recommends. It gives a fantastic overview with a few different approaches to explaining. However, for details about the more mathy bits, it may not be the best. 2. Better Book \u00b6 Pattern Recognition and Machine Learning - Bishop (2006) I find this a much better book which highlights a lot of the mathy bits (e.g. being able to fully manipulate joint Gaussian distributions to arrive at the GP). 3. Brief Overview \u00b6 Machine Learning: A Probabilistic Perspective - Murphy (2012) If you are already familiar with probability and just want a quick overview of GPs, then I would recommend you take a look at Murphy's book. Actually, if you're into probabilistic machine learning in general then I suggest you go through Murphy's book extensively. Best Thesis Explanation \u00b6 Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a thesis that I found extremely helpful when going step-by-step. It definitely trumps every other thesis that I've read which all assume the reader has some knowledge. The notation is a bit weird at first but once you get used to it, it becomes clearer and clearer. Gaussian Process Regression Techniques with Applications to Wind Turbines - Bijl (2016) - Thesis Code Introductions \u00b6 These resources are the best resources that explain GPs while also walking you through the code. I like going through it step-by-step using code because for some reason, when I have to code things, all of the sudden details start to click. When you can make a program do it from scratch then I think that is where the understanding kicks in. I have also included tutorials where they use python packages if you're interesting in just jumping in. From Scratch \u00b6 Gaussian Processes - Martin Krasser (2018) Implements GPs from scratch using numpy. I also like the use of functions which breaks things down quite nicely. Quickly glosses over sklearn and GPy. 4-Part Gaussian Process Tutorial Multivariate Normal Distribution Primer Understanding Gaussian Processes Fitting a GP GP Kernels A nice 4 part tutorial on GPs from scratch. It uses numpy to start and then switches to tensorflow on the 3rth tutorial (not sure why). But it's another explanation of the first tutorial. Gaussian Processes Not Quite for Dummies - Yuge Shi (2019) A more detailed introduction which focuses a bit more on the derivations and the properties of the Gaussian distribution in the context of GPs. Definitely more mathy than the previous posts. It also borrows heavily from a lecture by Richard Turner (another prominent figure in the GP community). Gaussian Processes - Jonathan Landy (2017) I like this blog post because it goes over everything in a simple way and also includes some nice nuggets such as acquisition functions for Bayesian Optimization and the derivation of the posterior function which you can find in Bishop. I like the format. Using Libraries \u00b6 Fitting GP models with Python - Chris Fonnesbeck (2017) This post skips over the 'from-scratch' and goes straight to practical implementations from well-known python libraries sklearn, GPy and PyMC3. Does a nice little comparison of each of the libraries. Intro to GPs Demo by Damianou - Jupyter Notebook Yet another prominent figure in the GP community. He put out a tweet where he expressed how this demo is what he will use when he teaches GPs to another community. So class/demo notes in a nutshell. A valuable tutorial. It almost exclusively uses GPy. Other Resources \u00b6 Previously Compiled Stuff \u00b6 So I found a few resources that give papers as well as some codes and lectures that you can look at. Into to GPs - Super Compilation Deep GPs Nonparametric - Papers GPs for Dynamical System Modeling - Papers","title":"Gaussian Processes"},{"location":"literature/gps/#gaussian-processes","text":"Best Lectures Best Visualize Explanations Best Books 1. Standard Book 2. Better Book 3. Brief Overview Best Thesis Explanation Code Introductions From Scratch Using Libraries Other Resources Previously Compiled Stuff This is fairly subjective but I have included the best tutorials that I could find for GPs on the internet. I focused on clarity and detail and I'm partial to resources with lots of plots. The resources range from lectures to blogs so hopefully by the end of this page you will find something that will ease you into understanding Gaussian processes.","title":"Gaussian Processes"},{"location":"literature/gps/#best-lectures","text":"By far the best beginners lecture on GPs (that I have found) is by Neil Lawrence ; a prominent figure that you should know once you enter this field. The lecture video that I have included below are the most recent of his lectures and I personally think it gives the most intuition behind GPs without being too mathy. The blog that's listed are his entire lecture notes in notebook format so you can read more or less verbatim what was said in the lecture; although you might miss out on the nuggets of wisdom he tends to drop during his lectures. Neil Lawrence Lecture @ MLSS 2019 - Blog | Lecture | Slides I would say that the best slides that I have found are by Marc Deisenroth . Unfortunately, I cannot find the video lectures online. I think these lecture slides are by far the best I've seen. It's fairly math intensive but there arent't too many proofs. These don't have proofs so you'll have to look somewhere else for that. Bishop's book (below) will be able to fill in any gaps. He also has a distill.pub page which has a nice introduction with a few practical tips and tricks to use. Foundations of Machine Learning: GPs - Deisenroth (2018-2019) - Slides | Practical Guide to GPs","title":"Best Lectures"},{"location":"literature/gps/#best-visualize-explanations","text":"If you are a visual person (like me) then you will appreciate resources where they go through step-by-step how a Gaussian process is formulated as well as the importance of the kernel and how one can train them. A Visual Exploration of Gaussian Processes - G\u00f6rtler et al. (2019)","title":"Best Visualize Explanations"},{"location":"literature/gps/#best-books","text":"","title":"Best Books"},{"location":"literature/gps/#1-standard-book","text":"Gaussian Processes for Machine Learning - Rasmussen (2006) This is the standard book that everyone recommends. It gives a fantastic overview with a few different approaches to explaining. However, for details about the more mathy bits, it may not be the best.","title":"1. Standard Book"},{"location":"literature/gps/#2-better-book","text":"Pattern Recognition and Machine Learning - Bishop (2006) I find this a much better book which highlights a lot of the mathy bits (e.g. being able to fully manipulate joint Gaussian distributions to arrive at the GP).","title":"2. Better Book"},{"location":"literature/gps/#3-brief-overview","text":"Machine Learning: A Probabilistic Perspective - Murphy (2012) If you are already familiar with probability and just want a quick overview of GPs, then I would recommend you take a look at Murphy's book. Actually, if you're into probabilistic machine learning in general then I suggest you go through Murphy's book extensively.","title":"3. Brief Overview"},{"location":"literature/gps/#best-thesis-explanation","text":"Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a thesis that I found extremely helpful when going step-by-step. It definitely trumps every other thesis that I've read which all assume the reader has some knowledge. The notation is a bit weird at first but once you get used to it, it becomes clearer and clearer. Gaussian Process Regression Techniques with Applications to Wind Turbines - Bijl (2016) - Thesis","title":"Best Thesis Explanation"},{"location":"literature/gps/#code-introductions","text":"These resources are the best resources that explain GPs while also walking you through the code. I like going through it step-by-step using code because for some reason, when I have to code things, all of the sudden details start to click. When you can make a program do it from scratch then I think that is where the understanding kicks in. I have also included tutorials where they use python packages if you're interesting in just jumping in.","title":"Code Introductions"},{"location":"literature/gps/#from-scratch","text":"Gaussian Processes - Martin Krasser (2018) Implements GPs from scratch using numpy. I also like the use of functions which breaks things down quite nicely. Quickly glosses over sklearn and GPy. 4-Part Gaussian Process Tutorial Multivariate Normal Distribution Primer Understanding Gaussian Processes Fitting a GP GP Kernels A nice 4 part tutorial on GPs from scratch. It uses numpy to start and then switches to tensorflow on the 3rth tutorial (not sure why). But it's another explanation of the first tutorial. Gaussian Processes Not Quite for Dummies - Yuge Shi (2019) A more detailed introduction which focuses a bit more on the derivations and the properties of the Gaussian distribution in the context of GPs. Definitely more mathy than the previous posts. It also borrows heavily from a lecture by Richard Turner (another prominent figure in the GP community). Gaussian Processes - Jonathan Landy (2017) I like this blog post because it goes over everything in a simple way and also includes some nice nuggets such as acquisition functions for Bayesian Optimization and the derivation of the posterior function which you can find in Bishop. I like the format.","title":"From Scratch"},{"location":"literature/gps/#using-libraries","text":"Fitting GP models with Python - Chris Fonnesbeck (2017) This post skips over the 'from-scratch' and goes straight to practical implementations from well-known python libraries sklearn, GPy and PyMC3. Does a nice little comparison of each of the libraries. Intro to GPs Demo by Damianou - Jupyter Notebook Yet another prominent figure in the GP community. He put out a tweet where he expressed how this demo is what he will use when he teaches GPs to another community. So class/demo notes in a nutshell. A valuable tutorial. It almost exclusively uses GPy.","title":"Using Libraries"},{"location":"literature/gps/#other-resources","text":"","title":"Other Resources"},{"location":"literature/gps/#previously-compiled-stuff","text":"So I found a few resources that give papers as well as some codes and lectures that you can look at. Into to GPs - Super Compilation Deep GPs Nonparametric - Papers GPs for Dynamical System Modeling - Papers","title":"Previously Compiled Stuff"},{"location":"literature/kernels/","text":"Kernel Functions \u00b6 Software \u00b6 Multiple Kernel Learning - MKLpy Kernel Methods - kernelmethods pykernels > A huge suite of different python kernels. kernpy Library focused on statistical tests keops Use kernel methods on the GPU with autograd and without memory overflows. Backend of numpy and pytorch. pyGPs This is a GP library but I saw quite a few graph kernels implemented with different Laplacian matrices implemented. megaman A library for large scale manifold learning. I saw quite a few different Laplacian matrices implemented.","title":"Kernel Functions"},{"location":"literature/kernels/#kernel-functions","text":"","title":"Kernel Functions"},{"location":"literature/kernels/#software","text":"Multiple Kernel Learning - MKLpy Kernel Methods - kernelmethods pykernels > A huge suite of different python kernels. kernpy Library focused on statistical tests keops Use kernel methods on the GPU with autograd and without memory overflows. Backend of numpy and pytorch. pyGPs This is a GP library but I saw quite a few graph kernels implemented with different Laplacian matrices implemented. megaman A library for large scale manifold learning. I saw quite a few different Laplacian matrices implemented.","title":"Software"},{"location":"literature/large_scale/","text":"State-of-the-Art \u00b6 Exact GP on a Million Data Points - Wang et. al. (2019) | Code The authors manage to train a GP with multiple GPUs on a million or so data points. They use the matrix-vector-multiplication strategy. Quite amazing actually... Constant-Time Predictive Distributions for GPs - Pleiss et. al. (2018) | Code Using MVM techniquees, they are able to make constant time predictive mean and variance estimates; addressing the computational bottleneck of predictive distributions for GPs.","title":"Large scale"},{"location":"literature/large_scale/#state-of-the-art","text":"Exact GP on a Million Data Points - Wang et. al. (2019) | Code The authors manage to train a GP with multiple GPUs on a million or so data points. They use the matrix-vector-multiplication strategy. Quite amazing actually... Constant-Time Predictive Distributions for GPs - Pleiss et. al. (2018) | Code Using MVM techniquees, they are able to make constant time predictive mean and variance estimates; addressing the computational bottleneck of predictive distributions for GPs.","title":"State-of-the-Art"},{"location":"literature/latent_variable/","text":"Algorithms \u00b6 Latent Variable Models \u00b6 Figure : (Gal et. al., 2015) Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data - Gal et. al. (2015) - Resources","title":"Latent variable"},{"location":"literature/latent_variable/#algorithms","text":"","title":"Algorithms"},{"location":"literature/latent_variable/#latent-variable-models","text":"Figure : (Gal et. al., 2015) Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data - Gal et. al. (2015) - Resources","title":"Latent Variable Models"},{"location":"literature/multioutput/","text":"","title":"Multioutput"},{"location":"literature/neural_networks/","text":"Neural Networks and Gaussian Processes \u00b6 Neural Networks & Deep Gaussian Processes \u00b6 Building Bayesian Neural Networks with Blocks: On Structure, Interpretability and Uncertainty - Zhou et. al. (2018) Deep Kernel Learning \u00b6 This is a Probabilistic Neural Network (PNN). It's when we try to learn features through a Neural Network and then on the last layer, we fit a Gaussian Process. It's a great idea and I think that this has a lot of potential. One of the criticisms of people in the GP community ( Bonilla et. al., 2016 ) is that we don't typically use very expressive kernels. That's where the power of GPs come from. So if we can have kernels from Neural Networks (one of the most expressive ML methods available to date), then we can get a potentially great ML algorithm. Even in practice, a developer have stated that we can get state-of-the-art results with some minimum tweaking of the architecture. Comments : * I've also heard this called \"Deep Feature Extraction\". * This is NOT a Deep GP. I've seen one paper that incorrectly called it that. A deep GP is where we stack GPs on top of each other. See the deep GP guide for more details. Literature Deep Kernel Learning - Wilson et. al. (2015) Stochastic Variational Deep Kernel learning - Wilson et. al. (2016) A Representer Theorem for Deep Kernel Learning - Bohn et. al. (2019) Code TensorFlow Implementation | GP Dist Example Pyro Implementation GPFlow Implementation GPyTorch Implementation Latest \u00b6 Deep Probabilistic Kernels for Sample-Efficient Learning - Mallick et. al. (2019) [ arxiv ] Propose a deep probabilistic kernel to address 1) traditional GP kernels aren't good at capturing similarities between high dimensional data points and 2) deep neural network kernels are not sample efficient. Has aspects such as Random Fourier Features, semi-supervised learning and utilizes the Stein Variational Gradient Descent algorithm. On the expected behaviour of noise regulariseddeep neural networks as Gaussian processes - Pretorius et al (2019) [ arxiv ] They study the impact of noise regularization via droput on Deep Kernel learning. SOTA \u00b6","title":"Neural Networks and Gaussian Processes"},{"location":"literature/neural_networks/#neural-networks-and-gaussian-processes","text":"","title":"Neural Networks and Gaussian Processes"},{"location":"literature/neural_networks/#neural-networks-deep-gaussian-processes","text":"Building Bayesian Neural Networks with Blocks: On Structure, Interpretability and Uncertainty - Zhou et. al. (2018)","title":"Neural Networks &amp; Deep Gaussian Processes"},{"location":"literature/neural_networks/#deep-kernel-learning","text":"This is a Probabilistic Neural Network (PNN). It's when we try to learn features through a Neural Network and then on the last layer, we fit a Gaussian Process. It's a great idea and I think that this has a lot of potential. One of the criticisms of people in the GP community ( Bonilla et. al., 2016 ) is that we don't typically use very expressive kernels. That's where the power of GPs come from. So if we can have kernels from Neural Networks (one of the most expressive ML methods available to date), then we can get a potentially great ML algorithm. Even in practice, a developer have stated that we can get state-of-the-art results with some minimum tweaking of the architecture. Comments : * I've also heard this called \"Deep Feature Extraction\". * This is NOT a Deep GP. I've seen one paper that incorrectly called it that. A deep GP is where we stack GPs on top of each other. See the deep GP guide for more details. Literature Deep Kernel Learning - Wilson et. al. (2015) Stochastic Variational Deep Kernel learning - Wilson et. al. (2016) A Representer Theorem for Deep Kernel Learning - Bohn et. al. (2019) Code TensorFlow Implementation | GP Dist Example Pyro Implementation GPFlow Implementation GPyTorch Implementation","title":"Deep Kernel Learning"},{"location":"literature/neural_networks/#latest","text":"Deep Probabilistic Kernels for Sample-Efficient Learning - Mallick et. al. (2019) [ arxiv ] Propose a deep probabilistic kernel to address 1) traditional GP kernels aren't good at capturing similarities between high dimensional data points and 2) deep neural network kernels are not sample efficient. Has aspects such as Random Fourier Features, semi-supervised learning and utilizes the Stein Variational Gradient Descent algorithm. On the expected behaviour of noise regulariseddeep neural networks as Gaussian processes - Pretorius et al (2019) [ arxiv ] They study the impact of noise regularization via droput on Deep Kernel learning.","title":"Latest"},{"location":"literature/neural_networks/#sota","text":"","title":"SOTA"},{"location":"literature/sparse_gps/","text":"Resources \u00b6 Papers \u00b6 Nystrom Approximation Using Nystrom to Speed Up Kernel Machines - Williams & Seeger (2001) Fully Independent Training Conditional (FITC) Sparse Gaussian Processes Using Pseudo-Inputs - Snelson and Ghahramani (2006) Flexible and Efficient GP Models for Machine Learning - Snelson (2007) Variational Free Energy (VFE) Variational Learning of Inducing Variables in Sparse GPs - Titsias (2009) On Sparse Variational meethods and the KL Divergence between Stochastic Processes - Matthews et. al. (2015) Stochastic Variational Inference Gaussian Processes for Big Data - Hensman et al. (2013) Sparse Spectrum GPR - Lazaro-Gredilla et al. (2010) SGD, SVI Improving the GP SS Approximation by Representing Uncertainty in Frequency Inputs - Gal et al. (2015) Prediction under Uncertainty in SSGPs w/ Applications to Filtering and Control - Pan et. al. (2017) Variational Fourier Features for GPs - Hensman (2018) Understanding Probabilistic Sparse GP Approx - Bauer et. al. (2016) A good paper which highlights some import differences between the FITC, DTC and VFE. It provides a clear notational differences and also mentions how VFE is a special case of DTC. A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation - Bui (2017) A good summary of all of the methods under one unified framework called the Power Expectation Propagation formula. Latest \u00b6 Deep Structured Mixtures of Gaussian Processes - Trapp et. al. (2019) Going back to the old days of improving the local-expert technique. Sparse Gaussian Process Regression Beyond Variational Inference - Jankowiak et. al. (2019) Thesis Explain \u00b6 Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a few thesis that help to explain some of the finer details. I've arranged them in order starting from the easiest to the most difficult. GPR Techniques - Bijl (2016) Chapter V - Noisy Input GPR Non-Stationary Surrogate Modeling with Deep Gaussian Processes - Dutordoir (2016) Chapter IV - Finding Uncertain Patterns in GPs Nonlinear Modeling and Control using GPs - McHutchon (2014) Chapter II - GP w/ Input Noise (NIGP) Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM Presentations \u00b6 Variational Inference for Gaussian and Determinantal Point Processes - Titsias (2014) Notes \u00b6 On the paper: Variational Learning of Inducing Variables in Sparse Gaussian Processees - Bui and Turner (2014) Blogs \u00b6 Variational Free Energy for Sparse GPs - Gonzalo","title":"Sparse gps"},{"location":"literature/sparse_gps/#resources","text":"","title":"Resources"},{"location":"literature/sparse_gps/#papers","text":"Nystrom Approximation Using Nystrom to Speed Up Kernel Machines - Williams & Seeger (2001) Fully Independent Training Conditional (FITC) Sparse Gaussian Processes Using Pseudo-Inputs - Snelson and Ghahramani (2006) Flexible and Efficient GP Models for Machine Learning - Snelson (2007) Variational Free Energy (VFE) Variational Learning of Inducing Variables in Sparse GPs - Titsias (2009) On Sparse Variational meethods and the KL Divergence between Stochastic Processes - Matthews et. al. (2015) Stochastic Variational Inference Gaussian Processes for Big Data - Hensman et al. (2013) Sparse Spectrum GPR - Lazaro-Gredilla et al. (2010) SGD, SVI Improving the GP SS Approximation by Representing Uncertainty in Frequency Inputs - Gal et al. (2015) Prediction under Uncertainty in SSGPs w/ Applications to Filtering and Control - Pan et. al. (2017) Variational Fourier Features for GPs - Hensman (2018) Understanding Probabilistic Sparse GP Approx - Bauer et. al. (2016) A good paper which highlights some import differences between the FITC, DTC and VFE. It provides a clear notational differences and also mentions how VFE is a special case of DTC. A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation - Bui (2017) A good summary of all of the methods under one unified framework called the Power Expectation Propagation formula.","title":"Papers"},{"location":"literature/sparse_gps/#latest","text":"Deep Structured Mixtures of Gaussian Processes - Trapp et. al. (2019) Going back to the old days of improving the local-expert technique. Sparse Gaussian Process Regression Beyond Variational Inference - Jankowiak et. al. (2019)","title":"Latest"},{"location":"literature/sparse_gps/#thesis-explain","text":"Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a few thesis that help to explain some of the finer details. I've arranged them in order starting from the easiest to the most difficult. GPR Techniques - Bijl (2016) Chapter V - Noisy Input GPR Non-Stationary Surrogate Modeling with Deep Gaussian Processes - Dutordoir (2016) Chapter IV - Finding Uncertain Patterns in GPs Nonlinear Modeling and Control using GPs - McHutchon (2014) Chapter II - GP w/ Input Noise (NIGP) Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM","title":"Thesis Explain"},{"location":"literature/sparse_gps/#presentations","text":"Variational Inference for Gaussian and Determinantal Point Processes - Titsias (2014)","title":"Presentations"},{"location":"literature/sparse_gps/#notes","text":"On the paper: Variational Learning of Inducing Variables in Sparse Gaussian Processees - Bui and Turner (2014)","title":"Notes"},{"location":"literature/sparse_gps/#blogs","text":"Variational Free Energy for Sparse GPs - Gonzalo","title":"Blogs"},{"location":"literature/special/","text":"Special Models \u00b6 These are models that I deem 'special' in the sense that they're not exactly what GPs were originally tended for but they have usefulness in other fields. I consider these algorithms in the category of \"jacking around\" (see Neal Lawrences lecture for the origin of the phrase). In my mind, these are algorithms that tackle a special case problem in the application realm. It's a bit difficult to verify how it does outside of the corner case due to no real comprehensive code-bases or benchmarks of GPs in real-world applications. Derivative Constraints \u00b6 Scaling GPR with Derivatives - Eriksson et. al. (2018) | Code They use MVM methods to enable one to solve for functions and their derivatives at the same time at scale. Multi-Output Models \u00b6 In regression, this is when we try to learn a function f f or multiple functions to predict multiple output y y . Concretly, we are trying to predict $y\\in \\mathbb{R}^{N\\times P}$ where $N$ is the number of samples and $P$ is the number of outputs. Terminology \u00b6 Firstly, there is some confusion about the terminology. I've heard the following names: Multi-Task Multi-Output Multi-Fidelity I cannot for the life of me figure out what is the difference between all of them. I have yet to see a paper that is consistent with how these are done. I have broken up each section based off of their name but I won't claim that there is no overlap between terms. Lectures \u00b6 GPSS 2017 Summer School - Alvarez - Video | Slides Literature \u00b6 Multi-Task \u00b6 Multi-task Gaussian Process prediction - Bonilla et al. 2007 - paper | GPSS 2008 Slides Multi-Output \u00b6 Efficient multioutput Gaussian processes through variational inducing kernels - Alvarez et al. (2011) - paper Remarks on multi-output Gaussian process regression - Liu et. al. (2018) - pdf Heterogeneous Multi-output Gaussian Process Prediction - Moreno-Mu\u00f1ez et. al. (2018) - paper | code Multi-Fidelity \u00b6 Deep Multi-fidelity Gaussian Processes - Raissi & Karniadakis (2016) - paper | blog | Deep Gaussian Processes for Multi-fidelity Modeling - Cutjar et. al. (2019) - paper | notebook | poster","title":"Special Models"},{"location":"literature/special/#special-models","text":"These are models that I deem 'special' in the sense that they're not exactly what GPs were originally tended for but they have usefulness in other fields. I consider these algorithms in the category of \"jacking around\" (see Neal Lawrences lecture for the origin of the phrase). In my mind, these are algorithms that tackle a special case problem in the application realm. It's a bit difficult to verify how it does outside of the corner case due to no real comprehensive code-bases or benchmarks of GPs in real-world applications.","title":"Special Models"},{"location":"literature/special/#derivative-constraints","text":"Scaling GPR with Derivatives - Eriksson et. al. (2018) | Code They use MVM methods to enable one to solve for functions and their derivatives at the same time at scale.","title":"Derivative Constraints"},{"location":"literature/special/#multi-output-models","text":"In regression, this is when we try to learn a function f f or multiple functions to predict multiple output y y . Concretly, we are trying to predict $y\\in \\mathbb{R}^{N\\times P}$ where $N$ is the number of samples and $P$ is the number of outputs.","title":"Multi-Output Models"},{"location":"literature/special/#terminology","text":"Firstly, there is some confusion about the terminology. I've heard the following names: Multi-Task Multi-Output Multi-Fidelity I cannot for the life of me figure out what is the difference between all of them. I have yet to see a paper that is consistent with how these are done. I have broken up each section based off of their name but I won't claim that there is no overlap between terms.","title":"Terminology"},{"location":"literature/special/#lectures","text":"GPSS 2017 Summer School - Alvarez - Video | Slides","title":"Lectures"},{"location":"literature/special/#literature","text":"","title":"Literature"},{"location":"literature/special/#multi-task","text":"Multi-task Gaussian Process prediction - Bonilla et al. 2007 - paper | GPSS 2008 Slides","title":"Multi-Task"},{"location":"literature/special/#multi-output","text":"Efficient multioutput Gaussian processes through variational inducing kernels - Alvarez et al. (2011) - paper Remarks on multi-output Gaussian process regression - Liu et. al. (2018) - pdf Heterogeneous Multi-output Gaussian Process Prediction - Moreno-Mu\u00f1ez et. al. (2018) - paper | code","title":"Multi-Output"},{"location":"literature/special/#multi-fidelity","text":"Deep Multi-fidelity Gaussian Processes - Raissi & Karniadakis (2016) - paper | blog | Deep Gaussian Processes for Multi-fidelity Modeling - Cutjar et. al. (2019) - paper | notebook | poster","title":"Multi-Fidelity"},{"location":"literature/uncertain_inputs/","text":"Uncertain Inputs in Gaussian Processes \u00b6 Please go to my dedicated repository where I explore all things to do with uncertain Gaussian processes. It can be found here: jejjohnson.github.io/uncertain_gps/","title":"Uncertain Inputs in Gaussian Processes"},{"location":"literature/uncertain_inputs/#uncertain-inputs-in-gaussian-processes","text":"Please go to my dedicated repository where I explore all things to do with uncertain Gaussian processes. It can be found here: jejjohnson.github.io/uncertain_gps/","title":"Uncertain Inputs in Gaussian Processes"},{"location":"prezi/tf2/slides/","text":"TF2.X and PyTorch \u00b6 For not so Dummies J. Emmanuel Johnson Second slide \u00b6 Best quote ever. Note: speaker notes FTW!","title":"TF2.X and PyTorch"},{"location":"prezi/tf2/slides/#tf2x-and-pytorch","text":"For not so Dummies J. Emmanuel Johnson","title":"TF2.X and PyTorch"},{"location":"prezi/tf2/slides/#second-slide","text":"Best quote ever. Note: speaker notes FTW!","title":"Second slide"},{"location":"prezi/tf2/docs/css/theme/","text":"Dependencies \u00b6 Themes are written using Sass to keep things modular and reduce the need for repeated selectors across files. Make sure that you have the reveal.js development environment including the Grunt dependencies installed before proceeding: https://github.com/hakimel/reveal.js#full-setup Creating a Theme \u00b6 To create your own theme, start by duplicating a .scss file in /css/theme/source . It will be automatically compiled by Grunt from Sass to CSS (see the Gruntfile ) when you run npm run build -- css-themes . Each theme file does four things in the following order: Include /css/theme/template/mixins.scss Shared utility functions. Include /css/theme/template/settings.scss Declares a set of custom variables that the template file (step 4) expects. Can be overridden in step 3. Override This is where you override the default theme. Either by specifying variables (see settings.scss for reference) or by adding any selectors and styles you please. Include /css/theme/template/theme.scss The template theme file which will generate final CSS output based on the currently defined variables.","title":"Index"},{"location":"prezi/tf2/docs/css/theme/#dependencies","text":"Themes are written using Sass to keep things modular and reduce the need for repeated selectors across files. Make sure that you have the reveal.js development environment including the Grunt dependencies installed before proceeding: https://github.com/hakimel/reveal.js#full-setup","title":"Dependencies"},{"location":"prezi/tf2/docs/css/theme/#creating-a-theme","text":"To create your own theme, start by duplicating a .scss file in /css/theme/source . It will be automatically compiled by Grunt from Sass to CSS (see the Gruntfile ) when you run npm run build -- css-themes . Each theme file does four things in the following order: Include /css/theme/template/mixins.scss Shared utility functions. Include /css/theme/template/settings.scss Declares a set of custom variables that the template file (step 4) expects. Can be overridden in step 3. Override This is where you override the default theme. Either by specifying variables (see settings.scss for reference) or by adding any selectors and styles you please. Include /css/theme/template/theme.scss The template theme file which will generate final CSS output based on the currently defined variables.","title":"Creating a Theme"},{"location":"prezi/tf2/docs/plugin/markdown/example/","text":"Markdown Demo \u00b6 External 1.1 \u00b6 Content 1.1 Note: This will only appear in the speaker notes window. External 1.2 \u00b6 Content 1.2 External 2 \u00b6 Content 2.1 External 3.1 \u00b6 Content 3.1 External 3.2 \u00b6 Content 3.2 External 3.3 \u00b6","title":"Markdown Demo"},{"location":"prezi/tf2/docs/plugin/markdown/example/#markdown-demo","text":"","title":"Markdown Demo"},{"location":"prezi/tf2/docs/plugin/markdown/example/#external-11","text":"Content 1.1 Note: This will only appear in the speaker notes window.","title":"External 1.1"},{"location":"prezi/tf2/docs/plugin/markdown/example/#external-12","text":"Content 1.2","title":"External 1.2"},{"location":"prezi/tf2/docs/plugin/markdown/example/#external-2","text":"Content 2.1","title":"External 2"},{"location":"prezi/tf2/docs/plugin/markdown/example/#external-31","text":"Content 3.1","title":"External 3.1"},{"location":"prezi/tf2/docs/plugin/markdown/example/#external-32","text":"Content 3.2","title":"External 3.2"},{"location":"prezi/tf2/docs/plugin/markdown/example/#external-33","text":"","title":"External 3.3"}]}