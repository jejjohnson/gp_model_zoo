{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Gaussian Process Model Zoo \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.github.io/gp_model_zoo/ Personal Website: jejjohnson.netlify.app Motivation \u00b6 I recently ran into someone at a conference who said, A lot of research dies in Graduate students laptops . (it was actually this scientist right here ). So I decided to go through all of my stuff, organize it a little bit and make it public. I have included some of the algorithms that I use or have worked with during my PhD. My lab works with kernel methods and we frequently use Gaussian Processes (GPs) for different Earth science applications, e.g. emulation, ocean applications and parameter retrievals. We typically use GPs for the following reasons with number 1 being the most important (as it with research groups whether they admit it or not). It's what we've been doing... We are a kernel lab, use mainly kernel methods and GPs are essentially a Bayesian treatment of kernel methods for regression and classification applications. Properly handling uncertainty is essential when dealing with physical data. With GPs, can often use sensible priors and a fairly consistent way of tuning the hyperparameters. Somewhat robust to overfitting via built-in regularization. I created this repo because I didn't want my code to go to waste in case there were people who are new to GPs and want to see a few key algorithms implemented. Also, it allows me to centralize my code for all my projects. Hopefully it will be of some use to others. What you'll find here \u00b6 Beginners \u00b6 These are the resources I consider to be the best when it comes to learning about GPs. The field has grown a lot but we still have newcomers. Start here! Literature \u00b6 There are many resources on the internet and I try to compile as much as I can. I do try and keep track of the SOTA and peruse arxiv from time to time. Software \u00b6 I like to keep track of what's going on. So I've listed the libraries that I am aware of as well as some things I've noticed about them. Demos \u00b6 I have a few demos where I will go through, mostly for learning and teaching purposes. It might be useful for some people.","title":"Home"},{"location":"#gaussian-process-model-zoo","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.github.io/gp_model_zoo/ Personal Website: jejjohnson.netlify.app","title":"Gaussian Process Model Zoo"},{"location":"#motivation","text":"I recently ran into someone at a conference who said, A lot of research dies in Graduate students laptops . (it was actually this scientist right here ). So I decided to go through all of my stuff, organize it a little bit and make it public. I have included some of the algorithms that I use or have worked with during my PhD. My lab works with kernel methods and we frequently use Gaussian Processes (GPs) for different Earth science applications, e.g. emulation, ocean applications and parameter retrievals. We typically use GPs for the following reasons with number 1 being the most important (as it with research groups whether they admit it or not). It's what we've been doing... We are a kernel lab, use mainly kernel methods and GPs are essentially a Bayesian treatment of kernel methods for regression and classification applications. Properly handling uncertainty is essential when dealing with physical data. With GPs, can often use sensible priors and a fairly consistent way of tuning the hyperparameters. Somewhat robust to overfitting via built-in regularization. I created this repo because I didn't want my code to go to waste in case there were people who are new to GPs and want to see a few key algorithms implemented. Also, it allows me to centralize my code for all my projects. Hopefully it will be of some use to others.","title":"Motivation"},{"location":"#what-youll-find-here","text":"","title":"What you'll find here"},{"location":"#beginners","text":"These are the resources I consider to be the best when it comes to learning about GPs. The field has grown a lot but we still have newcomers. Start here!","title":"Beginners"},{"location":"#literature","text":"There are many resources on the internet and I try to compile as much as I can. I do try and keep track of the SOTA and peruse arxiv from time to time.","title":"Literature"},{"location":"#software","text":"I like to keep track of what's going on. So I've listed the libraries that I am aware of as well as some things I've noticed about them.","title":"Software"},{"location":"#demos","text":"I have a few demos where I will go through, mostly for learning and teaching purposes. It might be useful for some people.","title":"Demos"},{"location":"intro/","text":"Gaussian Processes \u00b6 About This is fairly subjective but I have included the best tutorials that I could find for GPs on the internet. I focused on clarity and detail and I'm partial to resources with lots of plots. The resources range from lectures to blogs so hopefully by the end of this page you will find something that will ease you into understanding Gaussian processes. Best Lectures \u00b6 By far the best beginners lecture on GPs (that I have found) is by Neil Lawrence ; a prominent figure that you should know once you enter this field. The lecture video that I have included below are the most recent of his lectures and I personally think it gives the most intuition behind GPs without being too mathy. The blog that's listed are his entire lecture notes in notebook format so you can read more or less verbatim what was said in the lecture; although you might miss out on the nuggets of wisdom he tends to drop during his lectures. Neil Lawrence Lecture @ MLSS 2019 - Blog | Lecture | Slides I would say that the best slides that I have found are by Marc Deisenroth . Unfortunately, I cannot find the video lectures online. I think these lecture slides are by far the best I've seen. It's fairly math intensive but there arent't too many proofs. These don't have proofs so you'll have to look somewhere else for that. Bishop's book (below) will be able to fill in any gaps. He also has a distill.pub page which has a nice introduction with a few practical tips and tricks to use. Foundations of Machine Learning: GPs - Deisenroth (2018-2019) - Slides | Practical Guide to GPs Best Visualize Explanations \u00b6 If you are a visual person (like me) then you will appreciate resources where they go through step-by-step how a Gaussian process is formulated as well as the importance of the kernel and how one can train them. A Visual Exploration of Gaussian Processes - G\u00f6rtler et al. (2019) Best Books \u00b6 1. Standard Book \u00b6 Gaussian Processes for Machine Learning - Rasmussen (2006) This is the standard book that everyone recommends. It gives a fantastic overview with a few different approaches to explaining. However, for details about the more mathy bits, it may not be the best. 2. Better Book \u00b6 Pattern Recognition and Machine Learning - Bishop (2006) I find this a much better book which highlights a lot of the mathy bits (e.g. being able to fully manipulate joint Gaussian distributions to arrive at the GP). 3. Brief Overview \u00b6 Machine Learning: A Probabilistic Perspective - Murphy (2012) If you are already familiar with probability and just want a quick overview of GPs, then I would recommend you take a look at Murphy's book. Actually, if you're into probabilistic machine learning in general then I suggest you go through Murphy's book extensively. Best Thesis Explanation \u00b6 Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a thesis that I found extremely helpful when going step-by-step. It definitely trumps every other thesis that I've read which all assume the reader has some knowledge. The notation is a bit weird at first but once you get used to it, it becomes clearer and clearer. Gaussian Process Regression Techniques with Applications to Wind Turbines - Bijl (2016) - Thesis Code Introductions \u00b6 These resources are the best resources that explain GPs while also walking you through the code. I like going through it step-by-step using code because for some reason, when I have to code things, all of the sudden details start to click. When you can make a program do it from scratch then I think that is where the understanding kicks in. I have also included tutorials where they use python packages if you're interesting in just jumping in. From Scratch \u00b6 Gaussian Processes - Martin Krasser (2018) Implements GPs from scratch using numpy. I also like the use of functions which breaks things down quite nicely. Quickly glosses over sklearn and GPy. 4-Part Gaussian Process Tutorial Multivariate Normal Distribution Primer Understanding Gaussian Processes Fitting a GP GP Kernels A nice 4 part tutorial on GPs from scratch. It uses numpy to start and then switches to tensorflow on the 3rth tutorial (not sure why). But it's another explanation of the first tutorial. Gaussian Processes Not Quite for Dummies - Yuge Shi (2019) A more detailed introduction which focuses a bit more on the derivations and the properties of the Gaussian distribution in the context of GPs. Definitely more mathy than the previous posts. It also borrows heavily from a lecture by Richard Turner (another prominent figure in the GP community). Gaussian Processes - Jonathan Landy (2017) I like this blog post because it goes over everything in a simple way and also includes some nice nuggets such as acquisition functions for Bayesian Optimization and the derivation of the posterior function which you can find in Bishop. I like the format. Using Libraries \u00b6 Fitting GP models with Python - Chris Fonnesbeck (2017) This post skips over the 'from-scratch' and goes straight to practical implementations from well-known python libraries sklearn, GPy and PyMC3. Does a nice little comparison of each of the libraries. Intro to GPs Demo by Damianou - Jupyter Notebook Yet another prominent figure in the GP community. He put out a tweet where he expressed how this demo is what he will use when he teaches GPs to another community. So class/demo notes in a nutshell. A valuable tutorial. It almost exclusively uses GPy. Other Resources \u00b6 Previously Compiled Stuff \u00b6 So I found a few resources that give papers as well as some codes and lectures that you can look at. Into to GPs - Super Compilation Deep GPs Nonparametric - Papers GPs for Dynamical System Modeling - Papers","title":"Intro to GPs"},{"location":"intro/#gaussian-processes","text":"About This is fairly subjective but I have included the best tutorials that I could find for GPs on the internet. I focused on clarity and detail and I'm partial to resources with lots of plots. The resources range from lectures to blogs so hopefully by the end of this page you will find something that will ease you into understanding Gaussian processes.","title":"Gaussian Processes"},{"location":"intro/#best-lectures","text":"By far the best beginners lecture on GPs (that I have found) is by Neil Lawrence ; a prominent figure that you should know once you enter this field. The lecture video that I have included below are the most recent of his lectures and I personally think it gives the most intuition behind GPs without being too mathy. The blog that's listed are his entire lecture notes in notebook format so you can read more or less verbatim what was said in the lecture; although you might miss out on the nuggets of wisdom he tends to drop during his lectures. Neil Lawrence Lecture @ MLSS 2019 - Blog | Lecture | Slides I would say that the best slides that I have found are by Marc Deisenroth . Unfortunately, I cannot find the video lectures online. I think these lecture slides are by far the best I've seen. It's fairly math intensive but there arent't too many proofs. These don't have proofs so you'll have to look somewhere else for that. Bishop's book (below) will be able to fill in any gaps. He also has a distill.pub page which has a nice introduction with a few practical tips and tricks to use. Foundations of Machine Learning: GPs - Deisenroth (2018-2019) - Slides | Practical Guide to GPs","title":"Best Lectures"},{"location":"intro/#best-visualize-explanations","text":"If you are a visual person (like me) then you will appreciate resources where they go through step-by-step how a Gaussian process is formulated as well as the importance of the kernel and how one can train them. A Visual Exploration of Gaussian Processes - G\u00f6rtler et al. (2019)","title":"Best Visualize Explanations"},{"location":"intro/#best-books","text":"","title":"Best Books"},{"location":"intro/#1-standard-book","text":"Gaussian Processes for Machine Learning - Rasmussen (2006) This is the standard book that everyone recommends. It gives a fantastic overview with a few different approaches to explaining. However, for details about the more mathy bits, it may not be the best.","title":"1. Standard Book"},{"location":"intro/#2-better-book","text":"Pattern Recognition and Machine Learning - Bishop (2006) I find this a much better book which highlights a lot of the mathy bits (e.g. being able to fully manipulate joint Gaussian distributions to arrive at the GP).","title":"2. Better Book"},{"location":"intro/#3-brief-overview","text":"Machine Learning: A Probabilistic Perspective - Murphy (2012) If you are already familiar with probability and just want a quick overview of GPs, then I would recommend you take a look at Murphy's book. Actually, if you're into probabilistic machine learning in general then I suggest you go through Murphy's book extensively.","title":"3. Brief Overview"},{"location":"intro/#best-thesis-explanation","text":"Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a thesis that I found extremely helpful when going step-by-step. It definitely trumps every other thesis that I've read which all assume the reader has some knowledge. The notation is a bit weird at first but once you get used to it, it becomes clearer and clearer. Gaussian Process Regression Techniques with Applications to Wind Turbines - Bijl (2016) - Thesis","title":"Best Thesis Explanation"},{"location":"intro/#code-introductions","text":"These resources are the best resources that explain GPs while also walking you through the code. I like going through it step-by-step using code because for some reason, when I have to code things, all of the sudden details start to click. When you can make a program do it from scratch then I think that is where the understanding kicks in. I have also included tutorials where they use python packages if you're interesting in just jumping in.","title":"Code Introductions"},{"location":"intro/#from-scratch","text":"Gaussian Processes - Martin Krasser (2018) Implements GPs from scratch using numpy. I also like the use of functions which breaks things down quite nicely. Quickly glosses over sklearn and GPy. 4-Part Gaussian Process Tutorial Multivariate Normal Distribution Primer Understanding Gaussian Processes Fitting a GP GP Kernels A nice 4 part tutorial on GPs from scratch. It uses numpy to start and then switches to tensorflow on the 3rth tutorial (not sure why). But it's another explanation of the first tutorial. Gaussian Processes Not Quite for Dummies - Yuge Shi (2019) A more detailed introduction which focuses a bit more on the derivations and the properties of the Gaussian distribution in the context of GPs. Definitely more mathy than the previous posts. It also borrows heavily from a lecture by Richard Turner (another prominent figure in the GP community). Gaussian Processes - Jonathan Landy (2017) I like this blog post because it goes over everything in a simple way and also includes some nice nuggets such as acquisition functions for Bayesian Optimization and the derivation of the posterior function which you can find in Bishop. I like the format.","title":"From Scratch"},{"location":"intro/#using-libraries","text":"Fitting GP models with Python - Chris Fonnesbeck (2017) This post skips over the 'from-scratch' and goes straight to practical implementations from well-known python libraries sklearn, GPy and PyMC3. Does a nice little comparison of each of the libraries. Intro to GPs Demo by Damianou - Jupyter Notebook Yet another prominent figure in the GP community. He put out a tweet where he expressed how this demo is what he will use when he teaches GPs to another community. So class/demo notes in a nutshell. A valuable tutorial. It almost exclusively uses GPy.","title":"Using Libraries"},{"location":"intro/#other-resources","text":"","title":"Other Resources"},{"location":"intro/#previously-compiled-stuff","text":"So I found a few resources that give papers as well as some codes and lectures that you can look at. Into to GPs - Super Compilation Deep GPs Nonparametric - Papers GPs for Dynamical System Modeling - Papers","title":"Previously Compiled Stuff"},{"location":"other_software/","text":"Pyro \u00b6 This is my personal defacto library for doing research with GPs with PyTorch. In particular for GPs, I find the library to be super easy to mix and match priors and parameters for my GP models. I'm more comfortable with PyTorch so it was easy for me to Also pyro has a great forum which is very active and the devs are always willing to help. It is backed by Uber and built off of PyTorch so it has a strong dev community. I also talked to the devs at the ICML conference in 2019 and found that they were super open and passionate about the project. ** Model ** \u00b6 kernel2 = gp . kernels . RBF ( input_dim = 1 , variance = torch . tensor ( 0.1 ), lengthscale = torch . tensor ( 10. ) ) gpr_model = gp . models . GPRegression ( X , y , kernel2 , noise = torch . tensor ( 0.1 )) ** Training ** \u00b6 # define optimizer optimizer = torch . optim . Adam ( gpr . parameters (), lr = 0.005 ) # define loss function loss_fn = pyro . infer . Trace_ELBO () . differentiable_loss losses = [] num_steps = 1_000 # typical PyTorch boilerplate code for i in range ( num_steps ): optimizer . zero_grad () loss = loss_fn ( gpr . model , gpr . guide ) loss . backward () optimizer . step () Source : Pyro Docs TensorFlow Probability \u00b6 This library is built into Tensorflow already and they have a few GP modules that allow you to train GP algorithms. In edition, they have a keras-like GP layer which is very useful for using a GP as a final layer in probabilistic neural networks. The GP community is quite small for TFP so I haven't seen too many examples for this. ** Model ** \u00b6 # define kernel function kernel = tfp . math . psd_kernels . ExponentiatedQuadratic () # Define the model. model = tfp . layers . VariationalGaussianProcess ( num_inducing_points = 512 , kernel_provider = kernel ) ** Keras Training ** \u00b6 # Custom Loss Function loss = lambda y , rv_y : rv_y . variational_loss ( y , kl_weight = np . array ( batch_size , x . dtype ) / x . shape [ 0 ] ) # TF2.0 Keras Training Loop model . compile ( optimizer = tf . optimizers . Adam ( learning_rate = 0.01 ), loss = loss ) model . fit ( x , y , batch_size = batch_size , epochs = 1000 , verbose = False ) Edward2 \u00b6 This is the most exciting one in my opinion because this library will allow GPs (and Deep GPs) to be used for the most novice users and engineers. It features the GP and sparse GP as bayesian keras-like layers. So you can stack as many of them as you want and then call the keras model.fit() . With this API, we will be able to prototype very quickly and really start applying GPs out-of-the-box. I think this is a really great feature and will put GPs on the map because it doesn't get any easier than this. ** Model ** \u00b6 # define kernel function kernel = ExponentiatedQuadratic () # Define the model. model = ed . layers . SparseGaussianProcess ( 3 , num_inducing = 512 , covariance_fn = kernel ) predictions = model ( features ) ** Custom Training ** \u00b6 # Custom Loss Function def loss_fn ( features , labels ): preds = model ( features ) nll = - tf . reduce_mean ( predictions . distribution . log_prob ( labels )) kl = sum ( model . losses ) / total_dataset_size return nll + kl # TF2.0 Custom Training loop# num_steps = 1000 for _ in range ( num_steps ): with tf . GradientTape () as tape : loss = loss_fn ( features , labels ) gradients = tape . gradient ( loss , model . variables ) # use any optimizer here","title":"Other software"},{"location":"other_software/#pyro","text":"This is my personal defacto library for doing research with GPs with PyTorch. In particular for GPs, I find the library to be super easy to mix and match priors and parameters for my GP models. I'm more comfortable with PyTorch so it was easy for me to Also pyro has a great forum which is very active and the devs are always willing to help. It is backed by Uber and built off of PyTorch so it has a strong dev community. I also talked to the devs at the ICML conference in 2019 and found that they were super open and passionate about the project.","title":"Pyro"},{"location":"other_software/#model","text":"kernel2 = gp . kernels . RBF ( input_dim = 1 , variance = torch . tensor ( 0.1 ), lengthscale = torch . tensor ( 10. ) ) gpr_model = gp . models . GPRegression ( X , y , kernel2 , noise = torch . tensor ( 0.1 ))","title":"** Model **"},{"location":"other_software/#training","text":"# define optimizer optimizer = torch . optim . Adam ( gpr . parameters (), lr = 0.005 ) # define loss function loss_fn = pyro . infer . Trace_ELBO () . differentiable_loss losses = [] num_steps = 1_000 # typical PyTorch boilerplate code for i in range ( num_steps ): optimizer . zero_grad () loss = loss_fn ( gpr . model , gpr . guide ) loss . backward () optimizer . step () Source : Pyro Docs","title":"** Training **"},{"location":"other_software/#tensorflow-probability","text":"This library is built into Tensorflow already and they have a few GP modules that allow you to train GP algorithms. In edition, they have a keras-like GP layer which is very useful for using a GP as a final layer in probabilistic neural networks. The GP community is quite small for TFP so I haven't seen too many examples for this.","title":"TensorFlow Probability"},{"location":"other_software/#model_1","text":"# define kernel function kernel = tfp . math . psd_kernels . ExponentiatedQuadratic () # Define the model. model = tfp . layers . VariationalGaussianProcess ( num_inducing_points = 512 , kernel_provider = kernel )","title":"** Model **"},{"location":"other_software/#keras-training","text":"# Custom Loss Function loss = lambda y , rv_y : rv_y . variational_loss ( y , kl_weight = np . array ( batch_size , x . dtype ) / x . shape [ 0 ] ) # TF2.0 Keras Training Loop model . compile ( optimizer = tf . optimizers . Adam ( learning_rate = 0.01 ), loss = loss ) model . fit ( x , y , batch_size = batch_size , epochs = 1000 , verbose = False )","title":"** Keras Training **"},{"location":"other_software/#edward2","text":"This is the most exciting one in my opinion because this library will allow GPs (and Deep GPs) to be used for the most novice users and engineers. It features the GP and sparse GP as bayesian keras-like layers. So you can stack as many of them as you want and then call the keras model.fit() . With this API, we will be able to prototype very quickly and really start applying GPs out-of-the-box. I think this is a really great feature and will put GPs on the map because it doesn't get any easier than this.","title":"Edward2"},{"location":"other_software/#model_2","text":"# define kernel function kernel = ExponentiatedQuadratic () # Define the model. model = ed . layers . SparseGaussianProcess ( 3 , num_inducing = 512 , covariance_fn = kernel ) predictions = model ( features )","title":"** Model **"},{"location":"other_software/#custom-training","text":"# Custom Loss Function def loss_fn ( features , labels ): preds = model ( features ) nll = - tf . reduce_mean ( predictions . distribution . log_prob ( labels )) kl = sum ( model . losses ) / total_dataset_size return nll + kl # TF2.0 Custom Training loop# num_steps = 1000 for _ in range ( num_steps ): with tf . GradientTape () as tape : loss = loss_fn ( features , labels ) gradients = tape . gradient ( loss , model . variables ) # use any optimizer here","title":"** Custom Training **"},{"location":"software/","text":"Software \u00b6 Software for Gaussian processes (GPs) have really been improving for quite a while now. It is now a lot easier to not only to actually use the GP models, but also to modify them improve them. What is Deep Learning? \u00b6 Before we get into the software, I just wanted to quickly define deep learning. A recent debate on twitter got me thinking about an appropriate definition and it helped me think about how this definition relates to the software. It gave me perspective. Definition 1 by Yann LeCun - tweet (paraphrased) Deep Learning is methodology: building a model by assembling parameterized modules into (possibly dynamic) graphs and optimizing it with gradient-based methods. Definition II by Danilo Rezende - tweet (paraphrased) Deep Learning is a collection of tools to build complex modular differentiable functions. These definitions are more or less the same: deep learning is a tool to facilitate gradient-based optimization scheme for models. The data we use, the exact way we construct it, and how we train it aren't really in the definition. Most people might think a DL tool is the ensemble of different neural networks like these . But from henceforth, I refer to DL in the terms of facilitating the development of those neural networks, not the network library itself. So in terms of DL software, we need only a few components: Tensor structures Automatic differentiation (AutoGrad) Model Framework (Layers, etc) Optimizers Loss Functions Anything built on top of that can be special cases where we need special structures to create models for special cases. The simple example is a Multi-Layer Perceptron (MLP) model where we need some weight parameter, a bias parameter and an activation function. A library that allows you to train this model using an optimizer and a loss function, I would consider this autograd software (e.g. JAX). A library that has this functionality built-in (a.k.a. a layer ), I would consider this deep learning software (e.g. TensorFlow, PyTorch). While the only difference is the level of encapsulation, the latter makes it much easier to build ' complex modular ' neural networks whereas the former, not so much. You could still do it with the autograd library but you would have to design your entire model structure from scratch as well. So, there are still a LOT of things we can do with parameters and autograd alone but I wouldn't classify it as DL software. This isn't super important in the grand scheme of things but I think it's important to think about when creating a programming language and/or package and thinking about the target user. Anatomy of good DL software \u00b6 Francios Chollet (the creator of keras ) has been very vocal about the benefits of how TensorFlow caters to a broad audience ranging from applied users and algorithm developers. Both sides of the audience have different needs so building software for both audiences can very, very challenging. Below I have included a really interesting figure which highlights the axis of operations. Photo Credit : Francois Chollet Tweet As shown, there are two axis which define one way to split the DL software styles: the x-axis covers the model construction process and the y-axis covers the training process. I am sure that this is just one way to break apart DL software but I find it a good abstract way to look at it because I find that we can classify most use cases somewhere along this graph. I'll briefly outline a few below: Case 1 : All I care about is using a prebuilt model on some new data that my company has given me. I would probably fall somewhere on the upper right corner of the graph with the Sequential model and the built-in training scheme. Case II : I need a slightly more complex training scheme because I want to learn two models that share hidden nodes but they're not the same size. I also want to do some sort of cycle training, i.e. train one model first and then train the other. Then I would probably fall somewhere near the middle, and slightly to the right with the Functional model and a custom training scheme. Case III : I am a DL researcher and I need to control every single aspect of my model. I belong to the left and on the bottom with the full subclass model and completely custom training scheme. So there are many more special cases but by now you can imagine that most general cases can be found on the graph. I would like to stress that designing software to do all of these cases is not easy as these cases require careful design individually. It needs to be flexible. Maybe I'm old school, but I like the modular way of design. So in essence, I think we should design libraries that focus on one aspect, one audience and do it well. I also like a standard practice and integration so that everything can fit together in the end and we can transfer information or products from one part to another. This is similar to how the Japanese revolutionized building cars by having one machine do one thing at a time and it all fit together via a standard assembly line. So in the end, I want people to be able to mix and match as they see fit. To try to please everyone with \" one DL library that rules them all \" seems a bit silly in my opinion because you're spreading out your resources. But then again, I've never built software from scratch and I'm not a mega coorperation like Google or Facebook, so what do I know? I'm just one user...in a sea of many. With great power, comes great responsibility - Uncle Ben On a side note, when you build popular libraries, you shape how a massive amount of people think about the problem. Just like expressiveness is only as good as your vocabulary and limited by your language, the software you create actively morphs how your users think about framing and solving their problems. Just something to think about. Convergence of the Libraries \u00b6 Originally, there was a lot of differences between the deep learning libraries, e.g. static v.s. dynamic , Sequential v.s. Subclass . But now they are all starting to converge or at least have similar ways of constructing models and training. Below is a quick example of 4 deep learning libraries. If you know your python DL libraries trivia, try and guess which library do you think it is. Click on the details below to find out the answer. Photo Credit : Francois Chollet Tweet Answer Gluon TensorFlow PyTorch Chainer It does begs the question: if all of the libraries are basically the same, why are their multiple libraries? That's a great question and I do not know the answer to that. I think options are good as competition generally stimulates innovation. But at some point, there should be a limit no? But then again, the companies backing each of these languages are quite huge (Google, Microsoft, Uber, Facebook, etc). So I'm sure they have more than enough employees to justify the existence of their own library. But then again, imagine if they all put their efforts into making one great library. It could be an epic success! Or an epic disaster. I guess we will never know. So how to classify a library's worth is impossible because it's completely subjective. But I like this chart by Francois Chollet who put the different depths a package can go to in order to create a package that caters to different users. But libraries typically can be classified on this spectrum. The same breakdown of Deep Learning algorithms into Models and Training can be done for GPs as well. Since GPs aren't super mainstream yet, most modern large scale GP libraries will fall in the fully flexible category. But recently, with the edition of TensorFlow probability and Edward2, we have more modern GPs that will fall into the Easy to use category (but not necessarily easy to train...). Rant One thing I don't like about the GP community is that it is quite split in terms of SOTA. This is reflected in the software. You'll have Andrew Gordon Wilsons spin-off that works a lot with Black-Box Matrix Multiplication (BBMMs) and then you'll have James Hensen's spin-off group that works a lot with methods of inducing points. The GPyTorch library scales amazingly but the GPFlow library has the best multi-output configuration I've ever seen. Python Packages \u00b6 Below I list all of the GP packages available in Python. After this section, there will be more information on packages outside of the python ecosystem including some super intersting and well like GaussianProcess.jl for Julia and Stan as the universal programming language with many bindings. Tip If you're new to python, then I highly recommend you check out my other resource gatherings. It can be found here TLDR - My Recommendations \u00b6 Already Installed - scikit-learn If you're already installed python through anaconda of some sort then scikit-learn will be there close to being default. It should be in everyone's toolbox so it's really easy to whip out a GP method with this library. If you don't have a lot of data points (10-1_000) then just use this. It will do the job. Python Standard - PyMC3 This is the standard probabilistic programming language for doing Bayesian modeling in (more or less) standard Python. I personally think this library should also be in everyone's simple toolnox. The only thing that I don't like is that it uses Theano ; it's not impossible but it's another API that you need to understand the moment you start trying to customize. However, the devs did a great job at making most of that API no necessary and it's very scalable on CPUs. So out of the box, you should be good! From Scratch - JAX If you like to do things from scratch in a very numpy-like way but also want all of the benefits of autograd on CPU/GPU/TPUs, then this is for you. If you want access to some distributions, you can always use numpyro or tensorflow-probability which both use JAX and have a JAX-backend respectively. Standard / Researcher - GPFlow I think the GPFlow library has the best balance of ease of use and customizability. It has a lot of nice little features that make it really nice to use out of the box while also allowing for customization. The only thing is that you're not going to get the most scalable nor does it inherit many SOTA methods in the GP community. Researcher / Production - PyTorch If you're doing GP research and you really know how to program, then I suggest you use GPyTorch. It is currently the most popular library for doing GP research and it hosts an entire suite of SOTA ready to go. In addition, it is the most scalable library to date. While the developers made it super easy to play with on the surface, you need to dig deep and put on your coder hat in order to get to things under the hood. So maybe contributing stuff might have a barrier. Warning The machine learning community changes rapidly so any trends you observe are extremely volatile. Just like the machine learning literature, what's popular today can change within 6 months. So don't ever lock yourself in and stay flexible to cope with the changes. But also don't jump on bandwagons either as you'll be jumping every weekend. Keep a good balance and maintain your mental health. scikit-learn \u00b6 Image caption So we can start with the one that everyone has installed on their machine. The GP implementation in the scikit-learn library are already sufficient to get people started with GPs in scikit-learn. Often times when I'm data wrangling and I'm exploring possible algorithms, I'll already have the sklearn library installed in my conda environment so I typically start there myself especially for datasets with 100 points. Sample Code The sklearn implementation is as basic as it gets. If you are familiar with the scikit-learn API then you will have no problems using the GPR module. It's a three step process with very little things to change. Model # initialize kernel function kernel1 = RBF ( length_scale = 1.0 ) \\ kernel2 = WhiteKernel ( noise_level = 0.1 ) kernel = kernel1 + kernel2 # initialize noise parameter alpha = 1e-5 # initialize optimizer optimizer = \"fmin_l_bfgs_b\" # initialize GP model gpr_model = GaussianProcessRegressor ( kernel = kernel_gpml , alpha = alpha , optimizer = optimizer , ) Training # train GP model gpr_model . fit ( Xtrain , ytrain ) Predictions # get predictions y_pred , y_std = gpr_model . predict ( Xtest , return_std = True ) Again, this is the simplest API you will find and for small data problems, you'll find that this works fine out-of-the-box. I highly recommend this when starting especially if you're not a GP connoisseur. What I showed above is as complicated as it gets. Any more customization outside of this is a bit difficult as the scikit-learn API for GPs isn't very modular and wasn't designed as such. But as a first pass, it's good enough. Best Resource By far the best you'll see is the scikit-learn documentation. If you're feeling adventurous, you can check out how some people have extended this with additional kernels . Verdict \u2714\ufe0f Simple \u2714\ufe0f Standard \u274c Simple. No SOTA. No Sparse models. No tricks. \u274c Hard to modify individual parts. \u274c No Autograd GPy \u00b6 GPy is the most comprehensive research library I have found to date. It has the most number of different special GP \"corner case\" algorithms of any package available. The GPy examples and tutorials are very comprehensive. The major caveat is that the documentation is very difficult to navigate. I also found the code base to be a bit difficult to really understand what's going on because there is no automatic differentiation to reduce the computations so there can be a bit of redundancy. I typically wrap some typical GP algorithms with some common parameters that I use within the sklearn .fit() , .predict() , .score() framework and call it a day. The standard algorithms will include the Exact GP, the Sparse GP, and Bayesian GPLVM. Warning This library does not get updated very often so you will likely run into very silly bugs if you don't use strict package versions that are recommended. There are rumors of a GPy2 library that's based on MXFusion but I have failed to see anything concrete yet. ??? note \"Idea: Idea : Some of the main algorithms such as the sparse GP implementations are mature enough to be dumped into the sklearn library. For small-medium data problems, I think this would be extremely beneficial to the community. Some of the key papers like the (e.g. the FITC-SGP , VFE-SGP , Heteroscedastic GP , GP-LVM ) certainly pass some of the strict sklearn criteria . But I suspect that it wouldn't be a joy to code because you would need to do some of the gradients from scratch. I do feel like it might make GPs a bit more popular if some of the mainstream methods were included in the scikit-learn library. Sample Code The GPy implementation is also very basic. If you are familiar with the scikit-learn API then you will have no problems using the GPR module. It's a three step process with very little things to change. Model # define kernel function kernel = GPy . kern . RBF ( input_dim = 1 , variance = 1. , lengthscale = 1. ) # initialize GP model gpr_model = GPy . models . GPRegression ( Xtrain , ytrain , kern = kernel ) Training # train GP model gpr_model . optimize ( messages = True ) Predictions # get predictions y_pred , y_std = gpr_model . predict ( Xtest ) So as you can see, the API is very similar to the scikit-learn API with some small differences; the main one being that you have to initiate the GP model with the data. The rest is fairly similar. You should definitely take a look at the GPy docs if you are interested in some more advanced examples. Best Resource By far the best you'll find are the GP Summer school labs that happen every year . ( This year is virtual! ) They have a lot of good examples on the website. The documentation is very extensive but very difficult to get to the nitty gritty details. Verdict \u2714\ufe0f Simple \u2714\ufe0f Legacy \u274c Not industry battle-tested. \u274c No Autograd GPyTorch \u00b6 This is my defacto library for applying GPs to large scale data. Anything above 10,000 points, and I will resort to this library. It has GPU acceleration and a large suite of different GP algorithms depending upon your problem. I think this is currently the dominant GP library for actually using GPs and I highly recommend it for utility. They have many options available ranging from latent variables to multi-outputs. Recently they've just revamped their entire library and documentation with some I still find it a bit difficult to really customize anything under the hood. But if you can figure out how to mix and match each of the modular parts, then it should work for you. Sample Code In GPyTorch, the library follows the pythonic way of coding that became super popular from deep learning frameworks such as Chainer and subsequently PyTorch. It consists of a 4 step process which is seen in the snippet below. Source - GPyTorch Docs Model class MyGP ( gpytorch . models . ExactGP ): def __init__ ( self , train_x , train_y , likelihood ): super () . __init__ ( train_x , train_y , likelihood ) # Mean Function self . mean_module = gpytorch . means . ZeroMean () # Kernel Function self . covar_module = gpytorch . kernels . ScaleKernel ( gpytorch . kernels . RBFKernel ()) def forward ( self , x ): mean = self . mean_module ( x ) covar = self . covar_module ( x ) return gpytorch . distributions . MultivariateNormal ( mean , covar ) # train_x = ...; train_y = ... likelihood = gpytorch . likelihoods . GaussianLikelihood () model = MyGP ( train_x , train_y , likelihood ) Training # Put model in train mode model . train () likelihood . train () # Define optimizer optimizer = torch . optim . Adam ( model . parameters (), lr = 0.1 ) # Define loss function the marginal log likelihood mll = gpytorch . mlls . ExactMarginalLogLikelihood ( likelihood , model ) # training step for i in range ( training_iter ): # Zero gradients from previous iteration optimizer . zero_grad () # Output from model output = model ( train_x ) # Calc loss loss = - mll ( output , train_y ) # backprop gradients loss . backward () print ( 'Iter %d / %d - Loss: %.3f lengthscale: %.3f noise: %.3f ' % ( i + 1 , training_iter , loss . item (), model . covar_module . base_kernel . lengthscale . item (), model . likelihood . noise . item () )) optimizer . step () Predictions # get the predictive mean class f_preds = model ( test_x ) # can do the same with we want the noise model y_preds = likelihood ( model ( test_x )) # predictive mean f_mean = f_preds . mean # predictive variance f_var = f_preds . variance # predictive covariance f_covar = f_preds . covariance_matrix # sample from posterior distribution f_samples = f_preds . sample ( sample_shape = torch . Size ( 1000 ,)) I am only scratching the surface with this quick snippet. But I wanted to highlight how this fits into Best Resource By far the best you'll see are form the GPyTorch documentation : The Tutorial The Examples . There are a lot. Gaussian Processes (GP) with GPyTorch by DeepBayes.ru A good tutorial from an outside perspective. Similar to the GPy tutorial. Verdict \u2714\ufe0f All working pieces to GP Model customizable \u2714\ufe0f Lots of SOTA models \u2714\ufe0f Super responsive devs \u2714\ufe0f Integration with PyTorch and Pyro \u274c Difficult for absolute beginners \u274c Very difficult to contribute actual code \u274c Boilerplate Code GPFlow \u00b6 GPFlow Logo What Pyro is to PyTorch, GPFlow is to TensorFlow. This library is the successor to the GPy library. It is very comprehensive with a lot of SOTA algorithms. I definitely think ifA few of the devs from GPy went to GPFlow so it has a very similar style as GPy. But it is a lot cleaner due to the use of autograd which eliminates all of the code used to track the gradients. Many researchers use this library as a backend for their own research code so I would say it is the second most used library in the research domain. I didn't find it particularly easy to customize in tensorflow =<1.1 because of the session tracking which wasn't clear to me from the beginning. But now with the addition of tensorflow 2.0 and GPFlow adopting that new framework, I am eager to try it out again. They have a new public slack group so their network is going to grow hopefully. Sample Code Model Source : GPFlow Docs # kernel function kernel = gpflow . kernels . Matern52 () # mean function meanf = gpflow . mean_functions . Linear () # define GP model gpr_model = gpflow . models . GPR ( data = ( X , Y ), kernel = kernel , mean_function = meanf ) Training # define optimizer optimizer = gpflow . optimizers . Scipy () # optimize function num_steps = 1_000 opt_logs = opt . minimize ( m . training_loss , m . trainable_variables , options = dict ( maxiter = num_steps ) ) Best Resource By far the best you'll see are form the GPFlow documentation : Tutorials Integration with TensorFlow . There are a lot. Slack Channel StackOverFlow Verdict \u2714\ufe0f Customizable BUT GPy Familiar \u2714\ufe0f Lots of SOTA models \u2714\ufe0f Super responsive devs \u2714\ufe0f Integration with TensorFlow and TensorFlow-probability \u274c Difficult for absolute beginners \u274c Very difficult to contribute actual code \u274c Missing some SOTA Other Libraries \u00b6 Name Language Comments PyMC3 Python (Theano) Probabilistic programming with exact and sparse implementations and HMC/NUTS inference. Edward2 Python (TensorFlow) Implements drop-in GPs and Sparse GPs as keras layers. MATLAB MATLAB They have their own native implementations (straight from Rasmussen) gpml MATLAB Examples and code used in Rasmussen & Williams GPstuff MATLAB A library with a wide range of inference methods. Including HMC. GaussianProcess.jl Julia GP library utilising Julia's fast JIT compilation Stan R, Python, shell, MATLAB, Julia, Stata Probabilistic programming using MCMC that can be easily be used to model GPs GPU Support \u00b6 Package Backend GPU Support GPy Numpy \u2713 Scikit-Learn Numpy \u2717 PyMC3 Theano \u2713 TensorFlow (Probability) TensorFlow \u2713 Edward TensorFlow \u2713 GPFlow TensorFlow \u2713 Pyro.contrib PyTorch \u2713 GPyTorch PyTorch \u2713 PyMC4 TensorFlow \u2713 Algorithms Implemented \u00b6 Package GPy Scikit-Learn PyMC3 TensorFlow (Probability) GPFlow Pyro GPyTorch Exact \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Moment Matching GP \u2713 \u2717 \u2713 \u2717 S S \u2713 SparseGP - FITC \u2713 \u2717 \u2713 \u2717 \u2713 \u2713 \u2713 SparseGP - PEP \u2713 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 SparseSP - VFE \u2713 \u2717 \u2717 \u2717 \u2713 \u2713 \u2713 Variational GP \u2713 \u2717 \u2717 \u2713 \u2713 \u2713 \u2717 Stochastic Variational GP \u2713 \u2717 \u2717 S \u2713 \u2713 \u2713 Deep GP \u2717 \u2717 \u2717 S S \u2713 D Deep Kernel Learning \u2717 \u2717 \u2717 S S S \u2713 GPLVM \u2713 \u2717 \u2717 \u2717 \u2717 \u2713 \u2713 Bayesian GPLVM \u2713 \u2717 \u2717 \u2717 \u2713 \u2713 \u2713 SKI/KISS \u2717 \u2717 \u2717 \u2717 \u2717 \u2713 LOVE \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2713 Key Symbol Status \u2713 Implemented \u2717 Not Implemented D Development S Supported S(?) Maybe Supported","title":"Software"},{"location":"software/#software","text":"Software for Gaussian processes (GPs) have really been improving for quite a while now. It is now a lot easier to not only to actually use the GP models, but also to modify them improve them.","title":"Software"},{"location":"software/#what-is-deep-learning","text":"Before we get into the software, I just wanted to quickly define deep learning. A recent debate on twitter got me thinking about an appropriate definition and it helped me think about how this definition relates to the software. It gave me perspective. Definition 1 by Yann LeCun - tweet (paraphrased) Deep Learning is methodology: building a model by assembling parameterized modules into (possibly dynamic) graphs and optimizing it with gradient-based methods. Definition II by Danilo Rezende - tweet (paraphrased) Deep Learning is a collection of tools to build complex modular differentiable functions. These definitions are more or less the same: deep learning is a tool to facilitate gradient-based optimization scheme for models. The data we use, the exact way we construct it, and how we train it aren't really in the definition. Most people might think a DL tool is the ensemble of different neural networks like these . But from henceforth, I refer to DL in the terms of facilitating the development of those neural networks, not the network library itself. So in terms of DL software, we need only a few components: Tensor structures Automatic differentiation (AutoGrad) Model Framework (Layers, etc) Optimizers Loss Functions Anything built on top of that can be special cases where we need special structures to create models for special cases. The simple example is a Multi-Layer Perceptron (MLP) model where we need some weight parameter, a bias parameter and an activation function. A library that allows you to train this model using an optimizer and a loss function, I would consider this autograd software (e.g. JAX). A library that has this functionality built-in (a.k.a. a layer ), I would consider this deep learning software (e.g. TensorFlow, PyTorch). While the only difference is the level of encapsulation, the latter makes it much easier to build ' complex modular ' neural networks whereas the former, not so much. You could still do it with the autograd library but you would have to design your entire model structure from scratch as well. So, there are still a LOT of things we can do with parameters and autograd alone but I wouldn't classify it as DL software. This isn't super important in the grand scheme of things but I think it's important to think about when creating a programming language and/or package and thinking about the target user.","title":"What is Deep Learning?"},{"location":"software/#anatomy-of-good-dl-software","text":"Francios Chollet (the creator of keras ) has been very vocal about the benefits of how TensorFlow caters to a broad audience ranging from applied users and algorithm developers. Both sides of the audience have different needs so building software for both audiences can very, very challenging. Below I have included a really interesting figure which highlights the axis of operations. Photo Credit : Francois Chollet Tweet As shown, there are two axis which define one way to split the DL software styles: the x-axis covers the model construction process and the y-axis covers the training process. I am sure that this is just one way to break apart DL software but I find it a good abstract way to look at it because I find that we can classify most use cases somewhere along this graph. I'll briefly outline a few below: Case 1 : All I care about is using a prebuilt model on some new data that my company has given me. I would probably fall somewhere on the upper right corner of the graph with the Sequential model and the built-in training scheme. Case II : I need a slightly more complex training scheme because I want to learn two models that share hidden nodes but they're not the same size. I also want to do some sort of cycle training, i.e. train one model first and then train the other. Then I would probably fall somewhere near the middle, and slightly to the right with the Functional model and a custom training scheme. Case III : I am a DL researcher and I need to control every single aspect of my model. I belong to the left and on the bottom with the full subclass model and completely custom training scheme. So there are many more special cases but by now you can imagine that most general cases can be found on the graph. I would like to stress that designing software to do all of these cases is not easy as these cases require careful design individually. It needs to be flexible. Maybe I'm old school, but I like the modular way of design. So in essence, I think we should design libraries that focus on one aspect, one audience and do it well. I also like a standard practice and integration so that everything can fit together in the end and we can transfer information or products from one part to another. This is similar to how the Japanese revolutionized building cars by having one machine do one thing at a time and it all fit together via a standard assembly line. So in the end, I want people to be able to mix and match as they see fit. To try to please everyone with \" one DL library that rules them all \" seems a bit silly in my opinion because you're spreading out your resources. But then again, I've never built software from scratch and I'm not a mega coorperation like Google or Facebook, so what do I know? I'm just one user...in a sea of many. With great power, comes great responsibility - Uncle Ben On a side note, when you build popular libraries, you shape how a massive amount of people think about the problem. Just like expressiveness is only as good as your vocabulary and limited by your language, the software you create actively morphs how your users think about framing and solving their problems. Just something to think about.","title":"Anatomy of good DL software"},{"location":"software/#convergence-of-the-libraries","text":"Originally, there was a lot of differences between the deep learning libraries, e.g. static v.s. dynamic , Sequential v.s. Subclass . But now they are all starting to converge or at least have similar ways of constructing models and training. Below is a quick example of 4 deep learning libraries. If you know your python DL libraries trivia, try and guess which library do you think it is. Click on the details below to find out the answer. Photo Credit : Francois Chollet Tweet Answer Gluon TensorFlow PyTorch Chainer It does begs the question: if all of the libraries are basically the same, why are their multiple libraries? That's a great question and I do not know the answer to that. I think options are good as competition generally stimulates innovation. But at some point, there should be a limit no? But then again, the companies backing each of these languages are quite huge (Google, Microsoft, Uber, Facebook, etc). So I'm sure they have more than enough employees to justify the existence of their own library. But then again, imagine if they all put their efforts into making one great library. It could be an epic success! Or an epic disaster. I guess we will never know. So how to classify a library's worth is impossible because it's completely subjective. But I like this chart by Francois Chollet who put the different depths a package can go to in order to create a package that caters to different users. But libraries typically can be classified on this spectrum. The same breakdown of Deep Learning algorithms into Models and Training can be done for GPs as well. Since GPs aren't super mainstream yet, most modern large scale GP libraries will fall in the fully flexible category. But recently, with the edition of TensorFlow probability and Edward2, we have more modern GPs that will fall into the Easy to use category (but not necessarily easy to train...). Rant One thing I don't like about the GP community is that it is quite split in terms of SOTA. This is reflected in the software. You'll have Andrew Gordon Wilsons spin-off that works a lot with Black-Box Matrix Multiplication (BBMMs) and then you'll have James Hensen's spin-off group that works a lot with methods of inducing points. The GPyTorch library scales amazingly but the GPFlow library has the best multi-output configuration I've ever seen.","title":"Convergence of the Libraries"},{"location":"software/#python-packages","text":"Below I list all of the GP packages available in Python. After this section, there will be more information on packages outside of the python ecosystem including some super intersting and well like GaussianProcess.jl for Julia and Stan as the universal programming language with many bindings. Tip If you're new to python, then I highly recommend you check out my other resource gatherings. It can be found here","title":"Python Packages"},{"location":"software/#tldr-my-recommendations","text":"Already Installed - scikit-learn If you're already installed python through anaconda of some sort then scikit-learn will be there close to being default. It should be in everyone's toolbox so it's really easy to whip out a GP method with this library. If you don't have a lot of data points (10-1_000) then just use this. It will do the job. Python Standard - PyMC3 This is the standard probabilistic programming language for doing Bayesian modeling in (more or less) standard Python. I personally think this library should also be in everyone's simple toolnox. The only thing that I don't like is that it uses Theano ; it's not impossible but it's another API that you need to understand the moment you start trying to customize. However, the devs did a great job at making most of that API no necessary and it's very scalable on CPUs. So out of the box, you should be good! From Scratch - JAX If you like to do things from scratch in a very numpy-like way but also want all of the benefits of autograd on CPU/GPU/TPUs, then this is for you. If you want access to some distributions, you can always use numpyro or tensorflow-probability which both use JAX and have a JAX-backend respectively. Standard / Researcher - GPFlow I think the GPFlow library has the best balance of ease of use and customizability. It has a lot of nice little features that make it really nice to use out of the box while also allowing for customization. The only thing is that you're not going to get the most scalable nor does it inherit many SOTA methods in the GP community. Researcher / Production - PyTorch If you're doing GP research and you really know how to program, then I suggest you use GPyTorch. It is currently the most popular library for doing GP research and it hosts an entire suite of SOTA ready to go. In addition, it is the most scalable library to date. While the developers made it super easy to play with on the surface, you need to dig deep and put on your coder hat in order to get to things under the hood. So maybe contributing stuff might have a barrier. Warning The machine learning community changes rapidly so any trends you observe are extremely volatile. Just like the machine learning literature, what's popular today can change within 6 months. So don't ever lock yourself in and stay flexible to cope with the changes. But also don't jump on bandwagons either as you'll be jumping every weekend. Keep a good balance and maintain your mental health.","title":"TLDR - My Recommendations"},{"location":"software/#scikit-learn","text":"Image caption So we can start with the one that everyone has installed on their machine. The GP implementation in the scikit-learn library are already sufficient to get people started with GPs in scikit-learn. Often times when I'm data wrangling and I'm exploring possible algorithms, I'll already have the sklearn library installed in my conda environment so I typically start there myself especially for datasets with 100 points. Sample Code The sklearn implementation is as basic as it gets. If you are familiar with the scikit-learn API then you will have no problems using the GPR module. It's a three step process with very little things to change. Model # initialize kernel function kernel1 = RBF ( length_scale = 1.0 ) \\ kernel2 = WhiteKernel ( noise_level = 0.1 ) kernel = kernel1 + kernel2 # initialize noise parameter alpha = 1e-5 # initialize optimizer optimizer = \"fmin_l_bfgs_b\" # initialize GP model gpr_model = GaussianProcessRegressor ( kernel = kernel_gpml , alpha = alpha , optimizer = optimizer , ) Training # train GP model gpr_model . fit ( Xtrain , ytrain ) Predictions # get predictions y_pred , y_std = gpr_model . predict ( Xtest , return_std = True ) Again, this is the simplest API you will find and for small data problems, you'll find that this works fine out-of-the-box. I highly recommend this when starting especially if you're not a GP connoisseur. What I showed above is as complicated as it gets. Any more customization outside of this is a bit difficult as the scikit-learn API for GPs isn't very modular and wasn't designed as such. But as a first pass, it's good enough. Best Resource By far the best you'll see is the scikit-learn documentation. If you're feeling adventurous, you can check out how some people have extended this with additional kernels . Verdict \u2714\ufe0f Simple \u2714\ufe0f Standard \u274c Simple. No SOTA. No Sparse models. No tricks. \u274c Hard to modify individual parts. \u274c No Autograd","title":"scikit-learn"},{"location":"software/#gpy","text":"GPy is the most comprehensive research library I have found to date. It has the most number of different special GP \"corner case\" algorithms of any package available. The GPy examples and tutorials are very comprehensive. The major caveat is that the documentation is very difficult to navigate. I also found the code base to be a bit difficult to really understand what's going on because there is no automatic differentiation to reduce the computations so there can be a bit of redundancy. I typically wrap some typical GP algorithms with some common parameters that I use within the sklearn .fit() , .predict() , .score() framework and call it a day. The standard algorithms will include the Exact GP, the Sparse GP, and Bayesian GPLVM. Warning This library does not get updated very often so you will likely run into very silly bugs if you don't use strict package versions that are recommended. There are rumors of a GPy2 library that's based on MXFusion but I have failed to see anything concrete yet. ??? note \"Idea: Idea : Some of the main algorithms such as the sparse GP implementations are mature enough to be dumped into the sklearn library. For small-medium data problems, I think this would be extremely beneficial to the community. Some of the key papers like the (e.g. the FITC-SGP , VFE-SGP , Heteroscedastic GP , GP-LVM ) certainly pass some of the strict sklearn criteria . But I suspect that it wouldn't be a joy to code because you would need to do some of the gradients from scratch. I do feel like it might make GPs a bit more popular if some of the mainstream methods were included in the scikit-learn library. Sample Code The GPy implementation is also very basic. If you are familiar with the scikit-learn API then you will have no problems using the GPR module. It's a three step process with very little things to change. Model # define kernel function kernel = GPy . kern . RBF ( input_dim = 1 , variance = 1. , lengthscale = 1. ) # initialize GP model gpr_model = GPy . models . GPRegression ( Xtrain , ytrain , kern = kernel ) Training # train GP model gpr_model . optimize ( messages = True ) Predictions # get predictions y_pred , y_std = gpr_model . predict ( Xtest ) So as you can see, the API is very similar to the scikit-learn API with some small differences; the main one being that you have to initiate the GP model with the data. The rest is fairly similar. You should definitely take a look at the GPy docs if you are interested in some more advanced examples. Best Resource By far the best you'll find are the GP Summer school labs that happen every year . ( This year is virtual! ) They have a lot of good examples on the website. The documentation is very extensive but very difficult to get to the nitty gritty details. Verdict \u2714\ufe0f Simple \u2714\ufe0f Legacy \u274c Not industry battle-tested. \u274c No Autograd","title":"GPy"},{"location":"software/#gpytorch","text":"This is my defacto library for applying GPs to large scale data. Anything above 10,000 points, and I will resort to this library. It has GPU acceleration and a large suite of different GP algorithms depending upon your problem. I think this is currently the dominant GP library for actually using GPs and I highly recommend it for utility. They have many options available ranging from latent variables to multi-outputs. Recently they've just revamped their entire library and documentation with some I still find it a bit difficult to really customize anything under the hood. But if you can figure out how to mix and match each of the modular parts, then it should work for you. Sample Code In GPyTorch, the library follows the pythonic way of coding that became super popular from deep learning frameworks such as Chainer and subsequently PyTorch. It consists of a 4 step process which is seen in the snippet below. Source - GPyTorch Docs Model class MyGP ( gpytorch . models . ExactGP ): def __init__ ( self , train_x , train_y , likelihood ): super () . __init__ ( train_x , train_y , likelihood ) # Mean Function self . mean_module = gpytorch . means . ZeroMean () # Kernel Function self . covar_module = gpytorch . kernels . ScaleKernel ( gpytorch . kernels . RBFKernel ()) def forward ( self , x ): mean = self . mean_module ( x ) covar = self . covar_module ( x ) return gpytorch . distributions . MultivariateNormal ( mean , covar ) # train_x = ...; train_y = ... likelihood = gpytorch . likelihoods . GaussianLikelihood () model = MyGP ( train_x , train_y , likelihood ) Training # Put model in train mode model . train () likelihood . train () # Define optimizer optimizer = torch . optim . Adam ( model . parameters (), lr = 0.1 ) # Define loss function the marginal log likelihood mll = gpytorch . mlls . ExactMarginalLogLikelihood ( likelihood , model ) # training step for i in range ( training_iter ): # Zero gradients from previous iteration optimizer . zero_grad () # Output from model output = model ( train_x ) # Calc loss loss = - mll ( output , train_y ) # backprop gradients loss . backward () print ( 'Iter %d / %d - Loss: %.3f lengthscale: %.3f noise: %.3f ' % ( i + 1 , training_iter , loss . item (), model . covar_module . base_kernel . lengthscale . item (), model . likelihood . noise . item () )) optimizer . step () Predictions # get the predictive mean class f_preds = model ( test_x ) # can do the same with we want the noise model y_preds = likelihood ( model ( test_x )) # predictive mean f_mean = f_preds . mean # predictive variance f_var = f_preds . variance # predictive covariance f_covar = f_preds . covariance_matrix # sample from posterior distribution f_samples = f_preds . sample ( sample_shape = torch . Size ( 1000 ,)) I am only scratching the surface with this quick snippet. But I wanted to highlight how this fits into Best Resource By far the best you'll see are form the GPyTorch documentation : The Tutorial The Examples . There are a lot. Gaussian Processes (GP) with GPyTorch by DeepBayes.ru A good tutorial from an outside perspective. Similar to the GPy tutorial. Verdict \u2714\ufe0f All working pieces to GP Model customizable \u2714\ufe0f Lots of SOTA models \u2714\ufe0f Super responsive devs \u2714\ufe0f Integration with PyTorch and Pyro \u274c Difficult for absolute beginners \u274c Very difficult to contribute actual code \u274c Boilerplate Code","title":"GPyTorch"},{"location":"software/#gpflow","text":"GPFlow Logo What Pyro is to PyTorch, GPFlow is to TensorFlow. This library is the successor to the GPy library. It is very comprehensive with a lot of SOTA algorithms. I definitely think ifA few of the devs from GPy went to GPFlow so it has a very similar style as GPy. But it is a lot cleaner due to the use of autograd which eliminates all of the code used to track the gradients. Many researchers use this library as a backend for their own research code so I would say it is the second most used library in the research domain. I didn't find it particularly easy to customize in tensorflow =<1.1 because of the session tracking which wasn't clear to me from the beginning. But now with the addition of tensorflow 2.0 and GPFlow adopting that new framework, I am eager to try it out again. They have a new public slack group so their network is going to grow hopefully. Sample Code Model Source : GPFlow Docs # kernel function kernel = gpflow . kernels . Matern52 () # mean function meanf = gpflow . mean_functions . Linear () # define GP model gpr_model = gpflow . models . GPR ( data = ( X , Y ), kernel = kernel , mean_function = meanf ) Training # define optimizer optimizer = gpflow . optimizers . Scipy () # optimize function num_steps = 1_000 opt_logs = opt . minimize ( m . training_loss , m . trainable_variables , options = dict ( maxiter = num_steps ) ) Best Resource By far the best you'll see are form the GPFlow documentation : Tutorials Integration with TensorFlow . There are a lot. Slack Channel StackOverFlow Verdict \u2714\ufe0f Customizable BUT GPy Familiar \u2714\ufe0f Lots of SOTA models \u2714\ufe0f Super responsive devs \u2714\ufe0f Integration with TensorFlow and TensorFlow-probability \u274c Difficult for absolute beginners \u274c Very difficult to contribute actual code \u274c Missing some SOTA","title":"GPFlow"},{"location":"software/#other-libraries","text":"Name Language Comments PyMC3 Python (Theano) Probabilistic programming with exact and sparse implementations and HMC/NUTS inference. Edward2 Python (TensorFlow) Implements drop-in GPs and Sparse GPs as keras layers. MATLAB MATLAB They have their own native implementations (straight from Rasmussen) gpml MATLAB Examples and code used in Rasmussen & Williams GPstuff MATLAB A library with a wide range of inference methods. Including HMC. GaussianProcess.jl Julia GP library utilising Julia's fast JIT compilation Stan R, Python, shell, MATLAB, Julia, Stata Probabilistic programming using MCMC that can be easily be used to model GPs","title":"Other Libraries"},{"location":"software/#gpu-support","text":"Package Backend GPU Support GPy Numpy \u2713 Scikit-Learn Numpy \u2717 PyMC3 Theano \u2713 TensorFlow (Probability) TensorFlow \u2713 Edward TensorFlow \u2713 GPFlow TensorFlow \u2713 Pyro.contrib PyTorch \u2713 GPyTorch PyTorch \u2713 PyMC4 TensorFlow \u2713","title":"GPU Support"},{"location":"software/#algorithms-implemented","text":"Package GPy Scikit-Learn PyMC3 TensorFlow (Probability) GPFlow Pyro GPyTorch Exact \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Moment Matching GP \u2713 \u2717 \u2713 \u2717 S S \u2713 SparseGP - FITC \u2713 \u2717 \u2713 \u2717 \u2713 \u2713 \u2713 SparseGP - PEP \u2713 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 SparseSP - VFE \u2713 \u2717 \u2717 \u2717 \u2713 \u2713 \u2713 Variational GP \u2713 \u2717 \u2717 \u2713 \u2713 \u2713 \u2717 Stochastic Variational GP \u2713 \u2717 \u2717 S \u2713 \u2713 \u2713 Deep GP \u2717 \u2717 \u2717 S S \u2713 D Deep Kernel Learning \u2717 \u2717 \u2717 S S S \u2713 GPLVM \u2713 \u2717 \u2717 \u2717 \u2717 \u2713 \u2713 Bayesian GPLVM \u2713 \u2717 \u2717 \u2717 \u2713 \u2713 \u2713 SKI/KISS \u2717 \u2717 \u2717 \u2717 \u2717 \u2713 LOVE \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2713 Key Symbol Status \u2713 Implemented \u2717 Not Implemented D Development S Supported S(?) Maybe Supported","title":"Algorithms Implemented"},{"location":"softwarev2/","text":"What is Deep Learning? \u00b6 Before we get into the software, I just wanted to quickly define deep learning. A recent debate on twitter got me thinking about an appropriate definition and it helped me think about how this definition relates to the software. It gave me perspective. Definition 1 by Yann LeCun - tweet (paraphrased) Deep Learning is methodology: building a model by assembling parameterized modules into (possibly dynamic) graphs and optimizing it with gradient-based methods. Definition II by Danilo Rezende - tweet (paraphrased) Deep Learning is a collection of tools to build complex modular differentiable functions. These definitions are more or less the same: deep learning is a tool to facilitate gradient-based optimization scheme for models. The data we use, the exact way we construct it, and how we train it aren't really in the definition. Most people might think a DL tool is the ensemble of different neural networks like these . But from henceforth, I refer to DL in the terms of facilitating the development of those neural networks, not the network library itself. So in terms of DL software, we need only a few components: Tensor structures Automatic differentiation (AutoGrad) Model Framework (Layers, etc) Optimizers Loss Functions Anything built on top of that can be special cases where we need special structures to create models for special cases. The simple example is a Multi-Layer Perceptron (MLP) model where we need some weight parameter, a bias parameter and an activation function. A library that allows you to train this model using an optimizer and a loss function, I would consider this autograd software (e.g. JAX). A library that has this functionality built-in (a.k.a. a layer ), I would consider this deep learning software (e.g. TensorFlow, PyTorch). While the only difference is the level of encapsulation, the latter makes it much easier to build ' complex modular ' neural networks whereas the former, not so much. You could still do it with the autograd library but you would have to design your entire model structure from scratch as well. So, there are still a LOT of things we can do with parameters and autograd alone but I wouldn't classify it as DL software. This isn't super important in the grand scheme of things but I think it's important to think about when creating a programming language and/or package and thinking about the target user. Anatomy of good DL software \u00b6 Francios Chollet (the creator of keras ) has been very vocal about the benefits of how TensorFlow caters to a broad audience ranging from applied users and algorithm developers. Both sides of the audience have different needs so building software for both audiences can very, very challenging. Below I have included a really interesting figure which highlights the axis of operations. Photo Credit : Francois Chollet Tweet As shown, there are two axis which define one way to split the DL software styles: the x-axis covers the model construction process and the y-axis covers the training process. I am sure that this is just one way to break apart DL software but I find it a good abstract way to look at it because I find that we can classify most use cases somewhere along this graph. I'll briefly outline a few below: Case 1 : All I care about is using a prebuilt model on some new data that my company has given me. I would probably fall somewhere on the upper right corner of the graph with the Sequential model and the built-in training scheme. Case II : I need a slightly more complex training scheme because I want to learn two models that share hidden nodes but they're not the same size. I also want to do some sort of cycle training, i.e. train one model first and then train the other. Then I would probably fall somewhere near the middle, and slightly to the right with the Functional model and a custom training scheme. Case III : I am a DL researcher and I need to control every single aspect of my model. I belong to the left and on the bottom with the full subclass model and completely custom training scheme. So there are many more special cases but by now you can imagine that most general cases can be found on the graph. I would like to stress that designing software to do all of these cases is not easy as these cases require careful design individually. It needs to be flexible. Maybe I'm old school, but I like the modular way of design. So in essence, I think we should design libraries that focus on one aspect, one audience and do it well. I also like a standard practice and integration so that everything can fit together in the end and we can transfer information or products from one part to another. This is similar to how the Japanese revolutionized building cars by having one machine do one thing at a time and it all fit together via a standard assembly line. So in the end, I want people to be able to mix and match as they see fit. To try to please everyone with \" one DL library that rules them all \" seems a bit silly in my opinion because you're spreading out your resources. But then again, I've never built software from scratch and I'm not a mega coorperation like Google or Facebook, so what do I know? I'm just one user...in a sea of many. With great power, comes great responsibility - Uncle Ben On a side note, when you build popular libraries, you shape how a massive amount of people think about the problem. Just like expressiveness is only as good as your vocabulary and limited by your language, the software you create actively morphs how your users think about framing and solving their problems. Just something to think about. Convergence of the Libraries \u00b6 Originally, there was a lot of differences between the deep learning libraries, e.g. static v.s. dynamic , Sequential v.s. Subclass . But now they are all starting to converge or at least have similar ways of constructing models and training. Below is a quick example of 4 deep learning libraries. If you know your python DL libraries trivia, try and guess which library do you think it is. Click on the details below to find out the answer. Photo Credit : Francois Chollet Tweet Answer Gluon TensorFlow PyTorch Chainer It does begs the question: if all of the libraries are basically the same, why are their multiple libraries? That's a great question and I do not know the answer to that. I think options are good as competition generally stimulates innovation. But at some point, there should be a limit no? But then again, the companies backing each of these languages are quite huge (Google, Microsoft, Uber, Facebook, etc). So I'm sure they have more than enough employees to justify the existence of their own library. But then again, imagine if they all put their efforts into making one great library. It could be an epic success! Or an epic disaster. I guess we will never know. So what to choose? \u00b6 There are many schools of thought. Some people suggest doing things from scratch while some favour software to allow users to jumping right in . Fortunately, whatever the case may be or where you're at in your ML journey, there is a library to suit your needs. And as seen above, most of them are converging so learning one python package will have be transferable to another. In the end, people are going to choose whatever based on personal factors such as \"what is my style\" or environmental factors such as \"what is my research lab using now?\". I have a personal short list below just from observations, trends and reading but it is by no means concrete. Do whatever works for you! Jump Right In - fastai If you're interesting in applying your models to new and interesting datasets and are not necessarily interested in development then I suggest you start with fastai. This is a library that simplifies deep learning usage with all of the SOTA tricks built-in so I think it would save the average user a lot of time. From Scratch - JAX If you like to do things from scratch in a very numpy-like way but also want all of the benefits of autograd on CPU/GPU/TPUs, then this is for you. Deep Learning Researcher - PyTorch If you're doing research, then I suggest you use PyTorch. It is currently the most popular library for doing ML research. If you're looking at many of the SOTA algorithms, you'll find most of them being written in PyTorch these days. The API is similar to TensorFlow so you can easily transfer your skills to TF if needed. Production/Industry - TensorFlow TensorFlow holds the market in production. By far. So if you're looking to go into industry, it's highly likely that you'll be using TensorFlow. There are still a lot of researchers that use TF too. Fortunately, the API is similar to PyTorch if you use the subclass system so the skills are transferable. Warning The machine learning community changes rapidly so any trends you observe are extremely volatile. Just like the machine learning literature, what's popular today can change within 6 months. So don't ever lock yourself in and stay flexible to cope with the changes. But also don't jump on bandwagons either as you'll be jumping every weekend. Keep a good balance and maintain your mental health.","title":"Softwarev2"},{"location":"softwarev2/#what-is-deep-learning","text":"Before we get into the software, I just wanted to quickly define deep learning. A recent debate on twitter got me thinking about an appropriate definition and it helped me think about how this definition relates to the software. It gave me perspective. Definition 1 by Yann LeCun - tweet (paraphrased) Deep Learning is methodology: building a model by assembling parameterized modules into (possibly dynamic) graphs and optimizing it with gradient-based methods. Definition II by Danilo Rezende - tweet (paraphrased) Deep Learning is a collection of tools to build complex modular differentiable functions. These definitions are more or less the same: deep learning is a tool to facilitate gradient-based optimization scheme for models. The data we use, the exact way we construct it, and how we train it aren't really in the definition. Most people might think a DL tool is the ensemble of different neural networks like these . But from henceforth, I refer to DL in the terms of facilitating the development of those neural networks, not the network library itself. So in terms of DL software, we need only a few components: Tensor structures Automatic differentiation (AutoGrad) Model Framework (Layers, etc) Optimizers Loss Functions Anything built on top of that can be special cases where we need special structures to create models for special cases. The simple example is a Multi-Layer Perceptron (MLP) model where we need some weight parameter, a bias parameter and an activation function. A library that allows you to train this model using an optimizer and a loss function, I would consider this autograd software (e.g. JAX). A library that has this functionality built-in (a.k.a. a layer ), I would consider this deep learning software (e.g. TensorFlow, PyTorch). While the only difference is the level of encapsulation, the latter makes it much easier to build ' complex modular ' neural networks whereas the former, not so much. You could still do it with the autograd library but you would have to design your entire model structure from scratch as well. So, there are still a LOT of things we can do with parameters and autograd alone but I wouldn't classify it as DL software. This isn't super important in the grand scheme of things but I think it's important to think about when creating a programming language and/or package and thinking about the target user.","title":"What is Deep Learning?"},{"location":"softwarev2/#anatomy-of-good-dl-software","text":"Francios Chollet (the creator of keras ) has been very vocal about the benefits of how TensorFlow caters to a broad audience ranging from applied users and algorithm developers. Both sides of the audience have different needs so building software for both audiences can very, very challenging. Below I have included a really interesting figure which highlights the axis of operations. Photo Credit : Francois Chollet Tweet As shown, there are two axis which define one way to split the DL software styles: the x-axis covers the model construction process and the y-axis covers the training process. I am sure that this is just one way to break apart DL software but I find it a good abstract way to look at it because I find that we can classify most use cases somewhere along this graph. I'll briefly outline a few below: Case 1 : All I care about is using a prebuilt model on some new data that my company has given me. I would probably fall somewhere on the upper right corner of the graph with the Sequential model and the built-in training scheme. Case II : I need a slightly more complex training scheme because I want to learn two models that share hidden nodes but they're not the same size. I also want to do some sort of cycle training, i.e. train one model first and then train the other. Then I would probably fall somewhere near the middle, and slightly to the right with the Functional model and a custom training scheme. Case III : I am a DL researcher and I need to control every single aspect of my model. I belong to the left and on the bottom with the full subclass model and completely custom training scheme. So there are many more special cases but by now you can imagine that most general cases can be found on the graph. I would like to stress that designing software to do all of these cases is not easy as these cases require careful design individually. It needs to be flexible. Maybe I'm old school, but I like the modular way of design. So in essence, I think we should design libraries that focus on one aspect, one audience and do it well. I also like a standard practice and integration so that everything can fit together in the end and we can transfer information or products from one part to another. This is similar to how the Japanese revolutionized building cars by having one machine do one thing at a time and it all fit together via a standard assembly line. So in the end, I want people to be able to mix and match as they see fit. To try to please everyone with \" one DL library that rules them all \" seems a bit silly in my opinion because you're spreading out your resources. But then again, I've never built software from scratch and I'm not a mega coorperation like Google or Facebook, so what do I know? I'm just one user...in a sea of many. With great power, comes great responsibility - Uncle Ben On a side note, when you build popular libraries, you shape how a massive amount of people think about the problem. Just like expressiveness is only as good as your vocabulary and limited by your language, the software you create actively morphs how your users think about framing and solving their problems. Just something to think about.","title":"Anatomy of good DL software"},{"location":"softwarev2/#convergence-of-the-libraries","text":"Originally, there was a lot of differences between the deep learning libraries, e.g. static v.s. dynamic , Sequential v.s. Subclass . But now they are all starting to converge or at least have similar ways of constructing models and training. Below is a quick example of 4 deep learning libraries. If you know your python DL libraries trivia, try and guess which library do you think it is. Click on the details below to find out the answer. Photo Credit : Francois Chollet Tweet Answer Gluon TensorFlow PyTorch Chainer It does begs the question: if all of the libraries are basically the same, why are their multiple libraries? That's a great question and I do not know the answer to that. I think options are good as competition generally stimulates innovation. But at some point, there should be a limit no? But then again, the companies backing each of these languages are quite huge (Google, Microsoft, Uber, Facebook, etc). So I'm sure they have more than enough employees to justify the existence of their own library. But then again, imagine if they all put their efforts into making one great library. It could be an epic success! Or an epic disaster. I guess we will never know.","title":"Convergence of the Libraries"},{"location":"softwarev2/#so-what-to-choose","text":"There are many schools of thought. Some people suggest doing things from scratch while some favour software to allow users to jumping right in . Fortunately, whatever the case may be or where you're at in your ML journey, there is a library to suit your needs. And as seen above, most of them are converging so learning one python package will have be transferable to another. In the end, people are going to choose whatever based on personal factors such as \"what is my style\" or environmental factors such as \"what is my research lab using now?\". I have a personal short list below just from observations, trends and reading but it is by no means concrete. Do whatever works for you! Jump Right In - fastai If you're interesting in applying your models to new and interesting datasets and are not necessarily interested in development then I suggest you start with fastai. This is a library that simplifies deep learning usage with all of the SOTA tricks built-in so I think it would save the average user a lot of time. From Scratch - JAX If you like to do things from scratch in a very numpy-like way but also want all of the benefits of autograd on CPU/GPU/TPUs, then this is for you. Deep Learning Researcher - PyTorch If you're doing research, then I suggest you use PyTorch. It is currently the most popular library for doing ML research. If you're looking at many of the SOTA algorithms, you'll find most of them being written in PyTorch these days. The API is similar to TensorFlow so you can easily transfer your skills to TF if needed. Production/Industry - TensorFlow TensorFlow holds the market in production. By far. So if you're looking to go into industry, it's highly likely that you'll be using TensorFlow. There are still a lot of researchers that use TF too. Fortunately, the API is similar to PyTorch if you use the subclass system so the skills are transferable. Warning The machine learning community changes rapidly so any trends you observe are extremely volatile. Just like the machine learning literature, what's popular today can change within 6 months. So don't ever lock yourself in and stay flexible to cope with the changes. But also don't jump on bandwagons either as you'll be jumping every weekend. Keep a good balance and maintain your mental health.","title":"So what to choose?"},{"location":"demos/","text":"GP Demos \u00b6 Just some demos of GPs being used across platforms. Most of them will be contained notebooks where I make comments as I go along. My audience would be newcomers to GPs using Python. Library Comparison \u00b6 TLDR I did a few demos where I compare different libraries on toy datasets. 1D Toy Example Small Scale Example A 1D regression example with about 100 data points. I compare scikit-learn, GPFlow and GPyTorch.","title":"Overview"},{"location":"demos/#gp-demos","text":"Just some demos of GPs being used across platforms. Most of them will be contained notebooks where I make comments as I go along. My audience would be newcomers to GPs using Python.","title":"GP Demos"},{"location":"demos/#library-comparison","text":"TLDR I did a few demos where I compare different libraries on toy datasets. 1D Toy Example Small Scale Example A 1D regression example with about 100 data points. I compare scikit-learn, GPFlow and GPyTorch.","title":"Library Comparison"},{"location":"demos/1d_example/","text":"1D Example Walk-through \u00b6 Colab Notebooks Name Colab Notebook Sklearn GPFlow GPyTorch This post we will go over some of the 1D . Data \u00b6 The way we generate the data will be the same for all packages Code # control the seed seed = 123 rng = np . random . RandomState ( seed ) # Generate sample data n_samples = 25 def f ( x ): return np . sin ( 0.5 * X ) X = np . linspace ( - 2 * np . pi , 2 * np . pi , n_samples ) y = f ( X ) + 0.2 * rng . randn ( X . shape [ 0 ]) X , y = X [:, np . newaxis ], y [:, np . newaxis ] plt . figure () plt . scatter ( X , y , label = \"Train Data\" ) plt . legend () plt . show () GP Model \u00b6 Scikit-Learn In scikit-learn we just need the .fit() , .predict() , and .score() . First we need to define the parameters necessary for the gaussian process regression algorithm from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import RBF , WhiteKernel , ConstantKernel # define the kernel kernel = ConstantKernel () * RBF () + WhiteKernel () # define the model gp_model = GaussianProcessRegressor ( kernel = kernel , # Kernel Function alpha = 1e-5 , # Noise Level n_restarts_optimizer = 5 # Good Practice ) GPFlow Mean & Kernel Function So GPflow gives us some extra flexibility. We need to have a all the moving parts to define our standard GP. from gpflow.mean_functions import Linear from gpflow.kernels import RBF # define mean function mean_f = Linear () # define the kernel kernel = RBF () GP Model from gpflow.models import GPR # define GP Model gp_model = GPR ( data = ( X , y ), kernel = kernel , mean_function = mean_f ) # get a nice summary print_summary ( gp_model , ) \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 name \u2502 class \u2502 transform \u2502 prior \u2502 trainable \u2502 shape \u2502 dtype \u2502 value \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 GPR.mean_function.A \u2502 Parameter \u2502 Identity \u2502 \u2502 True \u2502 ( 1 , 1 ) \u2502 float64 \u2502 [[ 1 . ]] \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 GPR.mean_function.b \u2502 Parameter \u2502 Identity \u2502 \u2502 True \u2502 ( 1 , ) \u2502 float64 \u2502 [ 0 . ] \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 GPR.kernel.variance \u2502 Parameter \u2502 Softplus \u2502 \u2502 True \u2502 () \u2502 float64 \u2502 1 .0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 GPR.kernel.lengthscales \u2502 Parameter \u2502 Softplus \u2502 \u2502 True \u2502 () \u2502 float64 \u2502 1 .0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 GPR.likelihood.variance \u2502 Parameter \u2502 Softplus + Shift \u2502 \u2502 True \u2502 () \u2502 float64 \u2502 1 .0 \u2502 Numpy 2 Tensor Notice how I didn't do anything about changing from the np.ndarray to the tf.tensor ? Well that's because GPFlow is awesome and does it for you. Little things like that make the coding experience so much better. GPyTorch Numpy to Tensor Unfortunately, this is one of those things where PyTorch becomes more involved to use. There is no automatic conversion from a np.ndarray to a torch.Tensor . So in your code bases, you need to ensure that you do this. # convert to tensor X_tensor , y_tensor = torch . Tensor ( X . squeeze ()), torch . Tensor ( y . squeeze ()) Define GP Model Again, this is involved. You need to create the GP model exactly how one would think about doing it. You need a mean function, a kernel function, a likelihood and some data. We inherit the gpytorch.models.ExactGP class and we're good to go. # We will use the simplest form of GP model, exact inference class GPModel ( gpytorch . models . ExactGP ): def __init__ ( self , train_x , train_y , likelihood , mean_module , covar_module ): super () . __init__ ( train_x , train_y , likelihood ) # Constant Mean function self . mean_module = mean_module # RBF Kernel Function self . covar_module = covar_module def forward ( self , x ): mean_x = self . mean_module ( x ) covar_x = self . covar_module ( x ) return gpytorch . distributions . MultivariateNormal ( mean_x , covar_x ) Initialize GP model Now that we've defined our GP, we can intialize it with the appropriate parameters - mean function , kernel function and likelihood . # initialize mean function mean_f = gpytorch . means . ConstantMean () # initialize kernel function kernel = gpytorch . kernels . ScaleKernel ( gpytorch . kernels . RBFKernel ()) # intialize the likelihood likelihood = gpytorch . likelihoods . GaussianLikelihood () # initialize the gp model with the likelihood gp_model = GPModel ( X_tensor , y_tensor , likelihood , mean_f , kernel ) And we're done! So now you noticed that you learned a bit more than necessary to actually understand how the GP works because you had to build it from scratch (to some extent). This can be cumbersome but it might be more useful than the sklearn implementation because it gives us a bit more flexibility. Training Step \u00b6 Scikit-Learn It doesn't get any simpler than this... # fit GP model gp_model . fit ( X , y ); It's because everything is under the hood within the .fit method. GPFlow # define optimizer and params opt = gpflow . optimizers . Scipy () method = \"L-BFGS-B\" n_iters = 100 # optimize opt_logs = opt . minimize ( gp_model . training_loss , gp_model . trainable_variables , options = dict ( maxiter = n_iters ), method = method , ) # print a summary of the results print_summary ( gp_model ) \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 name \u2502 class \u2502 transform \u2502 prior \u2502 trainable \u2502 shape \u2502 dtype \u2502 value \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 GPR.mean_function.A \u2502 Parameter \u2502 Identity \u2502 \u2502 True \u2502 ( 1 , 1 ) \u2502 float64 \u2502 [[ -0.053 ]] \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 GPR.mean_function.b \u2502 Parameter \u2502 Identity \u2502 \u2502 True \u2502 ( 1 , ) \u2502 float64 \u2502 [ -0.09 ] \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 GPR.kernel.variance \u2502 Parameter \u2502 Softplus \u2502 \u2502 True \u2502 () \u2502 float64 \u2502 1 .0576019909271173 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 GPR.kernel.lengthscales \u2502 Parameter \u2502 Softplus \u2502 \u2502 True \u2502 () \u2502 float64 \u2502 3 .3182129463115735 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 GPR.likelihood.variance \u2502 Parameter \u2502 Softplus + Shift \u2502 \u2502 True \u2502 () \u2502 float64 \u2502 0 .05877109253391096 \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b GPyTorch So admittedly, this is where we start to enter into boilerplate code because this is stuff that needs to be done almost always. Set Model and Likelihood in Training Mode For this step, we need to let PyTorch know that we want all of the layers inside of our class primed and ready in train mode. Note This is mostly from things like BatchNormalization and DropOut which tend to behave differently between train and test. But we inherited the commands. # set model and likelihood in train mode likelihood . train () gp_model . train () One nice thing is that it will give you a small overview of how the model looks as a tuple of tuples. GPModel ( ( likelihood ) : GaussianLikelihood ( ( noise_covar ) : HomoskedasticNoise ( ( raw_noise_constraint ) : GreaterThan ( 1 .000E-04 ) ) ) ( mean_module ) : ConstantMean () ( covar_module ) : ScaleKernel ( ( base_kernel ) : RBFKernel ( ( raw_lengthscale_constraint ) : Positive () ) ( raw_outputscale_constraint ) : Positive () ) ) Choose Optimizer and Loss Now we define the optimizer as well as the loss function. # optimizer setup lr = 0.1 # Use the adam optimizer optimizer = torch . optim . Adam ( gp_model . parameters (), lr = lr ) # Loss Function - the marginal log likelihood mll = gpytorch . mlls . ExactMarginalLogLikelihood ( likelihood , gp_model ) Training And now we can finally train our model! Now, all of this that comes after is boilerplate code. We have to do it almost every time when using PyTorch. No escaping unless you want to use another package (which you should; I'll demonstrate that in a later tutorial). # choose iterations n_epochs = 100 # initialize progressbar with tqdm . notebook . trange ( n_epochs ) as pbar : for i in pbar : # Zero gradients from previous iteration optimizer . zero_grad () # Output from model output = gp_model ( X_tensor ) # Calc loss loss = - mll ( output , y_tensor ) # backprop gradients loss . backward () # get parameters we want to track postfix = dict ( Loss = loss . item (), scale = gp_model . covar_module . outputscale . item (), length_scale = gp_model . covar_module . base_kernel . lengthscale . item (), noise = gp_model . likelihood . noise . item () ) # step forward in the optimization optimizer . step () # update the progress bar pbar . set_postfix ( postfix ) Predictions \u00b6 Scikit-Learn Predictions are also very simple. You simply call the .predict method and you will get your predictive mean. If you want the predictive variance, you need to put the return_std flag as True . # generate some points for plotting X_plot = np . linspace ( - 1.2 , 1.2 , 100 )[:, np . newaxis ] # predictive mean and standard deviation y_mean , y_std = gp_model . predict ( X_plot , return_std = True ) # confidence intervals y_upper = y_mean . squeeze () + 2 * y_std y_lower = y_mean . squeeze () - 2 * y_std Predictive Standard Deviation There is no way to turn off or no the likelihood noise or not (i.e. the predictive mean y of the predictive mean f ). It is always y so the standard deviations will be a little high. To not use the y , you will need to actually subtract the WhiteKernel and the alpha from your predictive standard deviation. GPFlow # generate some points for plotting X_plot = np . linspace ( - 3 * np . pi , 3 * np . pi , 100 )[:, np . newaxis ] # predictive mean and standard deviation y_mean , y_var = gp_model . predict_y ( X_plot ) # convert to numpy arrays y_mean , y_var = y_mean . numpy (), y_var . numpy () # confidence intervals y_upper = y_mean + 2 * np . sqrt ( y_var ) y_lower = y_mean - 2 * np . sqrt ( y_var ) Tensor 2 Numpy So we do have to convert to numpy arrays from tensors for the predictions. Note, you can plot tensors, but some of the commands might be different. E.g. y_upper = tf . squeeze ( y_mean + 2 * tf . sqrt ( y_var )) GPyTorch We're still now out of it yet. We still need to do a few extra stuff to get predictions. Here we can simply Eval Mode We need to put our model back in .eval mode. This will allow all layers to behave in eval mode. # put the model in eval mode gp_model . eval () likelihood . eval () One nice thing is that the resulting observed_pred is actually a class with a few methods and properties, e.g. mean , variance , confidence_region . Very convenient for predictions. # generate some points for plotting X_plot = np . linspace ( - 3 * np . pi , 3 * np . pi , 100 )[:, np . newaxis ] # convert to tensor X_plot_tensor = torch . Tensor ( X_plot ) # turn off the gradient-tracking with torch . no_grad (): # generate data X_plot_tensor = torch . linspace ( - 3 * np . pi , 3 * np . pi , 100 ) # get predictions observed_pred = likelihood ( gp_model ( X_plot_tensor )) # extract mean prediction y_mean_tensor = observed_pred . mean # extract confidence intervals y_upper_tensor , y_lower_tensor = observed_pred . confidence_region () And yes...we still have to convert our data to np.ndarray . # convert to numpy array X_plot = X_plot_tensor . numpy () y_mean = y_mean_tensor . numpy () y_upper = y_upper_tensor . numpy () y_lower = y_lower_tensor . numpy () Visualization \u00b6 Plot Function It's the same for all libraries as I first convert everything to numpy arrays and then plot the results. fig , ax = plt . subplots () ax . scatter ( X , y , label = \"Training Data\" , color = 'Red' ) ax . plot ( X_plot , y_mean , color = 'black' , lw = 3 , label = 'Predictions' ) plt . fill_between ( X_plot . squeeze (), y_upper . squeeze (), y_lower . squeeze (), color = 'darkorange' , alpha = 0.2 , label = '95% Confidence' ) ax . set_ylim ([ - 3 , 3 ]) ax . set_xlim ([ - 10.2 , 10.2 ]) ax . legend () plt . tight_layout () plt . show () Scikit-Learn GPFlow GPyTorch","title":"1D Example"},{"location":"demos/1d_example/#1d-example-walk-through","text":"Colab Notebooks Name Colab Notebook Sklearn GPFlow GPyTorch This post we will go over some of the 1D .","title":"1D Example Walk-through"},{"location":"demos/1d_example/#data","text":"The way we generate the data will be the same for all packages Code # control the seed seed = 123 rng = np . random . RandomState ( seed ) # Generate sample data n_samples = 25 def f ( x ): return np . sin ( 0.5 * X ) X = np . linspace ( - 2 * np . pi , 2 * np . pi , n_samples ) y = f ( X ) + 0.2 * rng . randn ( X . shape [ 0 ]) X , y = X [:, np . newaxis ], y [:, np . newaxis ] plt . figure () plt . scatter ( X , y , label = \"Train Data\" ) plt . legend () plt . show ()","title":"Data"},{"location":"demos/1d_example/#gp-model","text":"Scikit-Learn In scikit-learn we just need the .fit() , .predict() , and .score() . First we need to define the parameters necessary for the gaussian process regression algorithm from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import RBF , WhiteKernel , ConstantKernel # define the kernel kernel = ConstantKernel () * RBF () + WhiteKernel () # define the model gp_model = GaussianProcessRegressor ( kernel = kernel , # Kernel Function alpha = 1e-5 , # Noise Level n_restarts_optimizer = 5 # Good Practice ) GPFlow Mean & Kernel Function So GPflow gives us some extra flexibility. We need to have a all the moving parts to define our standard GP. from gpflow.mean_functions import Linear from gpflow.kernels import RBF # define mean function mean_f = Linear () # define the kernel kernel = RBF () GP Model from gpflow.models import GPR # define GP Model gp_model = GPR ( data = ( X , y ), kernel = kernel , mean_function = mean_f ) # get a nice summary print_summary ( gp_model , ) \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 name \u2502 class \u2502 transform \u2502 prior \u2502 trainable \u2502 shape \u2502 dtype \u2502 value \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 GPR.mean_function.A \u2502 Parameter \u2502 Identity \u2502 \u2502 True \u2502 ( 1 , 1 ) \u2502 float64 \u2502 [[ 1 . ]] \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 GPR.mean_function.b \u2502 Parameter \u2502 Identity \u2502 \u2502 True \u2502 ( 1 , ) \u2502 float64 \u2502 [ 0 . ] \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 GPR.kernel.variance \u2502 Parameter \u2502 Softplus \u2502 \u2502 True \u2502 () \u2502 float64 \u2502 1 .0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 GPR.kernel.lengthscales \u2502 Parameter \u2502 Softplus \u2502 \u2502 True \u2502 () \u2502 float64 \u2502 1 .0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 GPR.likelihood.variance \u2502 Parameter \u2502 Softplus + Shift \u2502 \u2502 True \u2502 () \u2502 float64 \u2502 1 .0 \u2502 Numpy 2 Tensor Notice how I didn't do anything about changing from the np.ndarray to the tf.tensor ? Well that's because GPFlow is awesome and does it for you. Little things like that make the coding experience so much better. GPyTorch Numpy to Tensor Unfortunately, this is one of those things where PyTorch becomes more involved to use. There is no automatic conversion from a np.ndarray to a torch.Tensor . So in your code bases, you need to ensure that you do this. # convert to tensor X_tensor , y_tensor = torch . Tensor ( X . squeeze ()), torch . Tensor ( y . squeeze ()) Define GP Model Again, this is involved. You need to create the GP model exactly how one would think about doing it. You need a mean function, a kernel function, a likelihood and some data. We inherit the gpytorch.models.ExactGP class and we're good to go. # We will use the simplest form of GP model, exact inference class GPModel ( gpytorch . models . ExactGP ): def __init__ ( self , train_x , train_y , likelihood , mean_module , covar_module ): super () . __init__ ( train_x , train_y , likelihood ) # Constant Mean function self . mean_module = mean_module # RBF Kernel Function self . covar_module = covar_module def forward ( self , x ): mean_x = self . mean_module ( x ) covar_x = self . covar_module ( x ) return gpytorch . distributions . MultivariateNormal ( mean_x , covar_x ) Initialize GP model Now that we've defined our GP, we can intialize it with the appropriate parameters - mean function , kernel function and likelihood . # initialize mean function mean_f = gpytorch . means . ConstantMean () # initialize kernel function kernel = gpytorch . kernels . ScaleKernel ( gpytorch . kernels . RBFKernel ()) # intialize the likelihood likelihood = gpytorch . likelihoods . GaussianLikelihood () # initialize the gp model with the likelihood gp_model = GPModel ( X_tensor , y_tensor , likelihood , mean_f , kernel ) And we're done! So now you noticed that you learned a bit more than necessary to actually understand how the GP works because you had to build it from scratch (to some extent). This can be cumbersome but it might be more useful than the sklearn implementation because it gives us a bit more flexibility.","title":"GP Model"},{"location":"demos/1d_example/#training-step","text":"Scikit-Learn It doesn't get any simpler than this... # fit GP model gp_model . fit ( X , y ); It's because everything is under the hood within the .fit method. GPFlow # define optimizer and params opt = gpflow . optimizers . Scipy () method = \"L-BFGS-B\" n_iters = 100 # optimize opt_logs = opt . minimize ( gp_model . training_loss , gp_model . trainable_variables , options = dict ( maxiter = n_iters ), method = method , ) # print a summary of the results print_summary ( gp_model ) \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 name \u2502 class \u2502 transform \u2502 prior \u2502 trainable \u2502 shape \u2502 dtype \u2502 value \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 GPR.mean_function.A \u2502 Parameter \u2502 Identity \u2502 \u2502 True \u2502 ( 1 , 1 ) \u2502 float64 \u2502 [[ -0.053 ]] \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 GPR.mean_function.b \u2502 Parameter \u2502 Identity \u2502 \u2502 True \u2502 ( 1 , ) \u2502 float64 \u2502 [ -0.09 ] \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 GPR.kernel.variance \u2502 Parameter \u2502 Softplus \u2502 \u2502 True \u2502 () \u2502 float64 \u2502 1 .0576019909271173 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 GPR.kernel.lengthscales \u2502 Parameter \u2502 Softplus \u2502 \u2502 True \u2502 () \u2502 float64 \u2502 3 .3182129463115735 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 GPR.likelihood.variance \u2502 Parameter \u2502 Softplus + Shift \u2502 \u2502 True \u2502 () \u2502 float64 \u2502 0 .05877109253391096 \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b GPyTorch So admittedly, this is where we start to enter into boilerplate code because this is stuff that needs to be done almost always. Set Model and Likelihood in Training Mode For this step, we need to let PyTorch know that we want all of the layers inside of our class primed and ready in train mode. Note This is mostly from things like BatchNormalization and DropOut which tend to behave differently between train and test. But we inherited the commands. # set model and likelihood in train mode likelihood . train () gp_model . train () One nice thing is that it will give you a small overview of how the model looks as a tuple of tuples. GPModel ( ( likelihood ) : GaussianLikelihood ( ( noise_covar ) : HomoskedasticNoise ( ( raw_noise_constraint ) : GreaterThan ( 1 .000E-04 ) ) ) ( mean_module ) : ConstantMean () ( covar_module ) : ScaleKernel ( ( base_kernel ) : RBFKernel ( ( raw_lengthscale_constraint ) : Positive () ) ( raw_outputscale_constraint ) : Positive () ) ) Choose Optimizer and Loss Now we define the optimizer as well as the loss function. # optimizer setup lr = 0.1 # Use the adam optimizer optimizer = torch . optim . Adam ( gp_model . parameters (), lr = lr ) # Loss Function - the marginal log likelihood mll = gpytorch . mlls . ExactMarginalLogLikelihood ( likelihood , gp_model ) Training And now we can finally train our model! Now, all of this that comes after is boilerplate code. We have to do it almost every time when using PyTorch. No escaping unless you want to use another package (which you should; I'll demonstrate that in a later tutorial). # choose iterations n_epochs = 100 # initialize progressbar with tqdm . notebook . trange ( n_epochs ) as pbar : for i in pbar : # Zero gradients from previous iteration optimizer . zero_grad () # Output from model output = gp_model ( X_tensor ) # Calc loss loss = - mll ( output , y_tensor ) # backprop gradients loss . backward () # get parameters we want to track postfix = dict ( Loss = loss . item (), scale = gp_model . covar_module . outputscale . item (), length_scale = gp_model . covar_module . base_kernel . lengthscale . item (), noise = gp_model . likelihood . noise . item () ) # step forward in the optimization optimizer . step () # update the progress bar pbar . set_postfix ( postfix )","title":"Training Step"},{"location":"demos/1d_example/#predictions","text":"Scikit-Learn Predictions are also very simple. You simply call the .predict method and you will get your predictive mean. If you want the predictive variance, you need to put the return_std flag as True . # generate some points for plotting X_plot = np . linspace ( - 1.2 , 1.2 , 100 )[:, np . newaxis ] # predictive mean and standard deviation y_mean , y_std = gp_model . predict ( X_plot , return_std = True ) # confidence intervals y_upper = y_mean . squeeze () + 2 * y_std y_lower = y_mean . squeeze () - 2 * y_std Predictive Standard Deviation There is no way to turn off or no the likelihood noise or not (i.e. the predictive mean y of the predictive mean f ). It is always y so the standard deviations will be a little high. To not use the y , you will need to actually subtract the WhiteKernel and the alpha from your predictive standard deviation. GPFlow # generate some points for plotting X_plot = np . linspace ( - 3 * np . pi , 3 * np . pi , 100 )[:, np . newaxis ] # predictive mean and standard deviation y_mean , y_var = gp_model . predict_y ( X_plot ) # convert to numpy arrays y_mean , y_var = y_mean . numpy (), y_var . numpy () # confidence intervals y_upper = y_mean + 2 * np . sqrt ( y_var ) y_lower = y_mean - 2 * np . sqrt ( y_var ) Tensor 2 Numpy So we do have to convert to numpy arrays from tensors for the predictions. Note, you can plot tensors, but some of the commands might be different. E.g. y_upper = tf . squeeze ( y_mean + 2 * tf . sqrt ( y_var )) GPyTorch We're still now out of it yet. We still need to do a few extra stuff to get predictions. Here we can simply Eval Mode We need to put our model back in .eval mode. This will allow all layers to behave in eval mode. # put the model in eval mode gp_model . eval () likelihood . eval () One nice thing is that the resulting observed_pred is actually a class with a few methods and properties, e.g. mean , variance , confidence_region . Very convenient for predictions. # generate some points for plotting X_plot = np . linspace ( - 3 * np . pi , 3 * np . pi , 100 )[:, np . newaxis ] # convert to tensor X_plot_tensor = torch . Tensor ( X_plot ) # turn off the gradient-tracking with torch . no_grad (): # generate data X_plot_tensor = torch . linspace ( - 3 * np . pi , 3 * np . pi , 100 ) # get predictions observed_pred = likelihood ( gp_model ( X_plot_tensor )) # extract mean prediction y_mean_tensor = observed_pred . mean # extract confidence intervals y_upper_tensor , y_lower_tensor = observed_pred . confidence_region () And yes...we still have to convert our data to np.ndarray . # convert to numpy array X_plot = X_plot_tensor . numpy () y_mean = y_mean_tensor . numpy () y_upper = y_upper_tensor . numpy () y_lower = y_lower_tensor . numpy ()","title":"Predictions"},{"location":"demos/1d_example/#visualization","text":"Plot Function It's the same for all libraries as I first convert everything to numpy arrays and then plot the results. fig , ax = plt . subplots () ax . scatter ( X , y , label = \"Training Data\" , color = 'Red' ) ax . plot ( X_plot , y_mean , color = 'black' , lw = 3 , label = 'Predictions' ) plt . fill_between ( X_plot . squeeze (), y_upper . squeeze (), y_lower . squeeze (), color = 'darkorange' , alpha = 0.2 , label = '95% Confidence' ) ax . set_ylim ([ - 3 , 3 ]) ax . set_xlim ([ - 10.2 , 10.2 ]) ax . legend () plt . tight_layout () plt . show () Scikit-Learn GPFlow GPyTorch","title":"Visualization"},{"location":"demos/1d_example_ls/","text":"Large Scale 1D Example Walk-through \u00b6 Colab Notebooks Name Colab Notebook PyMC3 GPFlow GPyTorch This post we will go over some of the 1D . Data \u00b6 f(x) = \\sin(3\\pi x) + \\frac{1}{3}\\sin(9\\pi x) + \\frac{1}{2} \\sin(7 \\pi x) f(x) = \\sin(3\\pi x) + \\frac{1}{3}\\sin(9\\pi x) + \\frac{1}{2} \\sin(7 \\pi x) n_samples = 10_000 def f ( x ): return np . sin ( x * 3 * 3.14 ) + 0.3 * np . cos ( x * 9 * 3.14 ) + 0.5 * np . sin ( x * 7 * 3.14 ) X = np . linspace ( - 1.1 , 1.1 , n_samples ) X_plot = np . linspace ( - 1.3 , 1.3 , 100 )[:, np . newaxis ] y = f ( X ) + 0.2 * rng . randn ( X . shape [ 0 ]) X , y = X [:, np . newaxis ], y [:, np . newaxis ] Source : GPFlow - Big Data Tutorial GP Model \u00b6 GPFlow Kernel & Mean Function from gpflow.mean_functions import Linear from gpflow.kernels import RBF # define the kernel kernel = RBF () # define mean function mean_f = Linear () Inducing Points from sklearn.cluster import KMeans n_inducing = 50 seed = 123 # KMeans model kmeans_clf = KMeans ( n_clusters = n_inducing ) kmeans_clf . fit ( X ) # get cluster centers as inducing points Z = kmeans_clf . cluster_centers_ GP Model from gpflow.models import SVGP , SGPR # define GP Model sgpr_model = SGPR ( data = ( X , y ), kernel = kernel , inducing_variable = Z , mean_function = mean_f , ) # get a nice summary print_summary ( sgpr_model , ) \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 name \u2502 class \u2502 transform \u2502 prior \u2502 trainable \u2502 shape \u2502 dtype \u2502 value \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 GPR.mean_function.A \u2502 Parameter \u2502 Identity \u2502 \u2502 True \u2502 ( 1 , 1 ) \u2502 float64 \u2502 [[ 1 . ]] \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 GPR.mean_function.b \u2502 Parameter \u2502 Identity \u2502 \u2502 True \u2502 ( 1 , ) \u2502 float64 \u2502 [ 0 . ] \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 GPR.kernel.variance \u2502 Parameter \u2502 Softplus \u2502 \u2502 True \u2502 () \u2502 float64 \u2502 1 .0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 GPR.kernel.lengthscales \u2502 Parameter \u2502 Softplus \u2502 \u2502 True \u2502 () \u2502 float64 \u2502 1 .0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 GPR.likelihood.variance \u2502 Parameter \u2502 Softplus + Shift \u2502 \u2502 True \u2502 () \u2502 float64 \u2502 1 .0 \u2502 Numpy 2 Tensor Notice how I didn't do anything about changing from the np.ndarray to the tf.tensor ? Well that's because GPFlow is awesome and does it for you. Little things like that make the coding experience so much better. GPyTorch Training Step \u00b6 GPFlow # define optimizer and params minibatch_size = 128 # turn of training for inducing points opt = gpflow . optimizers . Scipy () # training loss training_loss = sgpr_model . training_loss_closure () method = \"L-BFGS-B\" n_iters = 1_000 # optimize opt_logs = opt . minimize ( sgpr_model . training_loss , sgpr_model . trainable_variables , options = dict ( maxiter = n_iters ), method = method , ) # print a summary of the results print_summary ( sgpr_model ) \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 name \u2502 class \u2502 transform \u2502 prior \u2502 trainable \u2502 shape \u2502 dtype \u2502 value \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 SGPR.mean_function.A \u2502 Parameter \u2502 Identity \u2502 \u2502 True \u2502 ( 1 , 1 ) \u2502 float64 \u2502 [[ -0.21 ]] \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 SGPR.mean_function.b \u2502 Parameter \u2502 Identity \u2502 \u2502 True \u2502 ( 1 , ) \u2502 float64 \u2502 [ 0 .019 ] \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 SGPR.kernel.variance \u2502 Parameter \u2502 Softplus \u2502 \u2502 True \u2502 () \u2502 float64 \u2502 1 .161352313910884 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 SGPR.kernel.lengthscales \u2502 Parameter \u2502 Softplus \u2502 \u2502 True \u2502 () \u2502 float64 \u2502 0 .09574863612699928 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 SGPR.likelihood.variance \u2502 Parameter \u2502 Softplus + Shift \u2502 \u2502 True \u2502 () \u2502 float64 \u2502 0 .04012048665014923 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 SGPR.inducing_variable.Z \u2502 Parameter \u2502 Identity \u2502 \u2502 True \u2502 ( 50 , 1 ) \u2502 float64 \u2502 [[ -0.786... \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b GPyTorch Predictions \u00b6 GPFlow # generate some points for plotting X_plot = np . linspace ( - 1.2 , 1.2 , 100 )[:, np . newaxis ] # predictive mean and standard deviation y_mean , y_var = sgpr_model . predict_y ( X_plot ) # convert to numpy arrays y_mean , y_var = y_mean . numpy (), y_var . numpy () # confidence intervals y_upper = y_mean + 2 * np . sqrt ( y_var ) y_lower = y_mean - 2 * np . sqrt ( y_var ) # Get learned inducing points Z = sgpr_model . inducing_variable . Z . numpy () Tensor 2 Numpy So we do have to convert to numpy arrays from tensors for the predictions. Note, you can plot tensors, but some of the commands might be different. E.g. y_upper = tf . squeeze ( y_mean + 2 * tf . sqrt ( y_var )) GPyTorch Visualization \u00b6 Plot Function It's the same for all libraries as I first convert everything to numpy arrays and then plot the results. fig , ax = plt . subplots ( figsize = ( 12 , 4 )) ax . scatter ( X , y , label = \"Training Data\" , color = 'Red' , marker = 'x' , alpha = 0.2 ) ax . plot ( X_plot , y_mean , color = 'black' , lw = 3 , label = 'Predictions' ) plt . fill_between ( X_plot . squeeze (), y_upper . squeeze (), y_lower . squeeze (), color = 'lightblue' , alpha = 0.6 , label = '95% Confidence' ) plt . scatter ( Z , np . zeros_like ( Z ), color = \"black\" , marker = \"|\" , label = \"Inducing locations\" ) ax . legend () plt . tight_layout () plt . show () GPFlow GPyTorch","title":"1D Example (Large Scale)"},{"location":"demos/1d_example_ls/#large-scale-1d-example-walk-through","text":"Colab Notebooks Name Colab Notebook PyMC3 GPFlow GPyTorch This post we will go over some of the 1D .","title":"Large Scale 1D Example Walk-through"},{"location":"demos/1d_example_ls/#data","text":"f(x) = \\sin(3\\pi x) + \\frac{1}{3}\\sin(9\\pi x) + \\frac{1}{2} \\sin(7 \\pi x) f(x) = \\sin(3\\pi x) + \\frac{1}{3}\\sin(9\\pi x) + \\frac{1}{2} \\sin(7 \\pi x) n_samples = 10_000 def f ( x ): return np . sin ( x * 3 * 3.14 ) + 0.3 * np . cos ( x * 9 * 3.14 ) + 0.5 * np . sin ( x * 7 * 3.14 ) X = np . linspace ( - 1.1 , 1.1 , n_samples ) X_plot = np . linspace ( - 1.3 , 1.3 , 100 )[:, np . newaxis ] y = f ( X ) + 0.2 * rng . randn ( X . shape [ 0 ]) X , y = X [:, np . newaxis ], y [:, np . newaxis ] Source : GPFlow - Big Data Tutorial","title":"Data"},{"location":"demos/1d_example_ls/#gp-model","text":"GPFlow Kernel & Mean Function from gpflow.mean_functions import Linear from gpflow.kernels import RBF # define the kernel kernel = RBF () # define mean function mean_f = Linear () Inducing Points from sklearn.cluster import KMeans n_inducing = 50 seed = 123 # KMeans model kmeans_clf = KMeans ( n_clusters = n_inducing ) kmeans_clf . fit ( X ) # get cluster centers as inducing points Z = kmeans_clf . cluster_centers_ GP Model from gpflow.models import SVGP , SGPR # define GP Model sgpr_model = SGPR ( data = ( X , y ), kernel = kernel , inducing_variable = Z , mean_function = mean_f , ) # get a nice summary print_summary ( sgpr_model , ) \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 name \u2502 class \u2502 transform \u2502 prior \u2502 trainable \u2502 shape \u2502 dtype \u2502 value \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 GPR.mean_function.A \u2502 Parameter \u2502 Identity \u2502 \u2502 True \u2502 ( 1 , 1 ) \u2502 float64 \u2502 [[ 1 . ]] \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 GPR.mean_function.b \u2502 Parameter \u2502 Identity \u2502 \u2502 True \u2502 ( 1 , ) \u2502 float64 \u2502 [ 0 . ] \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 GPR.kernel.variance \u2502 Parameter \u2502 Softplus \u2502 \u2502 True \u2502 () \u2502 float64 \u2502 1 .0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 GPR.kernel.lengthscales \u2502 Parameter \u2502 Softplus \u2502 \u2502 True \u2502 () \u2502 float64 \u2502 1 .0 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 GPR.likelihood.variance \u2502 Parameter \u2502 Softplus + Shift \u2502 \u2502 True \u2502 () \u2502 float64 \u2502 1 .0 \u2502 Numpy 2 Tensor Notice how I didn't do anything about changing from the np.ndarray to the tf.tensor ? Well that's because GPFlow is awesome and does it for you. Little things like that make the coding experience so much better. GPyTorch","title":"GP Model"},{"location":"demos/1d_example_ls/#training-step","text":"GPFlow # define optimizer and params minibatch_size = 128 # turn of training for inducing points opt = gpflow . optimizers . Scipy () # training loss training_loss = sgpr_model . training_loss_closure () method = \"L-BFGS-B\" n_iters = 1_000 # optimize opt_logs = opt . minimize ( sgpr_model . training_loss , sgpr_model . trainable_variables , options = dict ( maxiter = n_iters ), method = method , ) # print a summary of the results print_summary ( sgpr_model ) \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 name \u2502 class \u2502 transform \u2502 prior \u2502 trainable \u2502 shape \u2502 dtype \u2502 value \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 SGPR.mean_function.A \u2502 Parameter \u2502 Identity \u2502 \u2502 True \u2502 ( 1 , 1 ) \u2502 float64 \u2502 [[ -0.21 ]] \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 SGPR.mean_function.b \u2502 Parameter \u2502 Identity \u2502 \u2502 True \u2502 ( 1 , ) \u2502 float64 \u2502 [ 0 .019 ] \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 SGPR.kernel.variance \u2502 Parameter \u2502 Softplus \u2502 \u2502 True \u2502 () \u2502 float64 \u2502 1 .161352313910884 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 SGPR.kernel.lengthscales \u2502 Parameter \u2502 Softplus \u2502 \u2502 True \u2502 () \u2502 float64 \u2502 0 .09574863612699928 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 SGPR.likelihood.variance \u2502 Parameter \u2502 Softplus + Shift \u2502 \u2502 True \u2502 () \u2502 float64 \u2502 0 .04012048665014923 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 SGPR.inducing_variable.Z \u2502 Parameter \u2502 Identity \u2502 \u2502 True \u2502 ( 50 , 1 ) \u2502 float64 \u2502 [[ -0.786... \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b GPyTorch","title":"Training Step"},{"location":"demos/1d_example_ls/#predictions","text":"GPFlow # generate some points for plotting X_plot = np . linspace ( - 1.2 , 1.2 , 100 )[:, np . newaxis ] # predictive mean and standard deviation y_mean , y_var = sgpr_model . predict_y ( X_plot ) # convert to numpy arrays y_mean , y_var = y_mean . numpy (), y_var . numpy () # confidence intervals y_upper = y_mean + 2 * np . sqrt ( y_var ) y_lower = y_mean - 2 * np . sqrt ( y_var ) # Get learned inducing points Z = sgpr_model . inducing_variable . Z . numpy () Tensor 2 Numpy So we do have to convert to numpy arrays from tensors for the predictions. Note, you can plot tensors, but some of the commands might be different. E.g. y_upper = tf . squeeze ( y_mean + 2 * tf . sqrt ( y_var )) GPyTorch","title":"Predictions"},{"location":"demos/1d_example_ls/#visualization","text":"Plot Function It's the same for all libraries as I first convert everything to numpy arrays and then plot the results. fig , ax = plt . subplots ( figsize = ( 12 , 4 )) ax . scatter ( X , y , label = \"Training Data\" , color = 'Red' , marker = 'x' , alpha = 0.2 ) ax . plot ( X_plot , y_mean , color = 'black' , lw = 3 , label = 'Predictions' ) plt . fill_between ( X_plot . squeeze (), y_upper . squeeze (), y_lower . squeeze (), color = 'lightblue' , alpha = 0.6 , label = '95% Confidence' ) plt . scatter ( Z , np . zeros_like ( Z ), color = \"black\" , marker = \"|\" , label = \"Inducing locations\" ) ax . legend () plt . tight_layout () plt . show () GPFlow GPyTorch","title":"Visualization"},{"location":"demos/2d_example/","text":"","title":"2d example"},{"location":"demos/ts_example/","text":"","title":"Ts example"},{"location":"literature/","text":"Gaussian Process Literature \u00b6 Introduction \u00b6 This folder contains lists of papers and code bases that deal with Gaussian processes and applications. I also try to keep up with the literature and show some state-of-the-art (SOTA) algorithms and code bases. I'm quite fond of GPs because they are mathematically beautiful and they have many nice properties. They also work with one of the nicest distributions to work with: the Guassian distribution. They deal with uncertainty and because they are function based, they work well with small (and large datasets) with sophisticated interpolation schemes. The biggest problem of GPs back in the day was that they did not scale but nowadays that isn't the case as there are dozens of sparse methods that allow us to scale GPs to 100K+ up to even 1 million data points. My Perspective \u00b6 I think that the GP literature is fairly spread over many fields but there is not really much of any compilation of GP resources (not until recently at least). I have been studying GPs for a while now to try to understand the field as well to see how they have been used. My feeling is that there are more methods than actual applications and many of the algorithms seem to have never been applied to many things outside of their \"corner case\" application. If you look at a standard GP algorithm, it seems to go within one of the following modifications: Algorithm original algorithm is invented Special Kernel a special kernel is invented for a specific use case Fourier-ify they do some random fourier features approximation scheme (or some other approximate kernel methods) Heteroscedastic-ify they modify the likelihood such that the noise function varies w.r.t. the inputs X as well. Sparse-ify they scale it using subsampling, model approximations or posterior approximations Latent Variable Representation they consider the case where the variables are latent and not deterministic MultiOutput-ify they apply it to multioutput (or multifidelity cases) Deep-ify they start stacking the GPs on top of each other In reality, almost all of the methods you'll find in the literature come within these subfields or a combination of a 2 or more more or less in the order I've listed. That's not to say that what people do isn't important or impressive, but it would be nice if we had some better structure to how we classify GP algorithms and improvements we make. My Special Interests There are a few questions and issues which I think have not been answered but are steadily improving with time. They are individual components of the GP algorithms that I consider a bit weak and I think they can be improved by taking knowledge from other fields. A good example would be how the treatment of Matrix-Vector-Multiplication (MVM) was used for scaling GP algorithms. Back in the day, the dominant community was various ways to subsample the data. The MVM community was fairly small within the main GP community but now it has risen as the most scalable and customizable method to date with a dominant python package. Just goes to show how other fields can really come in and improve some of the little intricate aspects of the algorithms and make huge improvements. Below is a list of methods which are not as prevalent in the literature that you won't see at major machine learning conferences but that I believe are super important and could possibly greatly improve the methods that we already have. I will try to pay special attention to #1 in particular because it is apart of my research. Input Uncertainty Uncertainty Calibration Kernel Parameters (initializations and priors) Missing Data / Semi-Supervised Learning Training procedures Expressive Kernels Topics \u00b6 Gaussian Processes \u00b6 If you're new to GPs, start here. I'll go over some of the necessary resources to get you up to speed. Sparse Gaussian Processes \u00b6 This contains some of the main GP algorithms that you need to know in the way of scaling. If your dataset is larger than 2K then you should probably start using one of these sparse methods. Latent Variable Models \u00b6 These algorithms are when we assume that the input X X is not determinant but instead an unknown. The applications of this range from dimensionality reduction to uncertain inputs and even applications in missing data and semi-supervised learning. Sparse Spectrum \u00b6 These algorithms make use of the Bochner theorem which states that we can represent kernel functions as an infinite series with weights that stem from a Gaussian distribution. These (what are essentially Fourier transformations) are typically known as Sparse Spectrum GPs in the community. Uncertain Inputs \u00b6 This is directly related to my research so I'll pay special attention to this. I look at the literature spanned from the beginning up until now. This will mostly be about moment-matching algorithms and algorithms that use variational inference. Deep GPs \u00b6 I have made this a special section because I think it's quite an interesting topic. It's still a fairly young aspect of GPs (last 7 years or so) so it won't have such a depth of literature like the other topics. I'll also include stuff related to how Deep GPs are related to neural networks. Neural Networks and GPs \u00b6 This section is made up of the papers that talk about the connections between neural networks and GPs as well as some specific cases where people have used neural networks to extract features as inputs for GPs (i.e. Deep Kernel Learning). Inference \u00b6 A very important part of GPs, how do we obtain the hyperparameters. So this section will be looking at methods by GPs. I'll include the usual suspects such as maximum likelihood, variational inference, expectation propagation and monte carlo methods. Kernels \u00b6 I have a section where you can find stuff on different kernel methods that have specific use cases. Software \u00b6 The fun part. Here is where I look at all the latest software that one can use to run some of the SOTA algorithms. It will python libraries only because it's the language I personally use to code. Applications \u00b6 This consists of papers which have applied GPs to their respective fields. I'll focus more on Earth observation applications but I'll put up any others if I find them of interest. Special \u00b6 This will include all of the GP algorithms that I consider 'corner cases', i.e. GPs modified that apply to a specific application where some modification was necessary to make the GP work their use case.","title":"About"},{"location":"literature/#gaussian-process-literature","text":"","title":"Gaussian Process Literature"},{"location":"literature/#introduction","text":"This folder contains lists of papers and code bases that deal with Gaussian processes and applications. I also try to keep up with the literature and show some state-of-the-art (SOTA) algorithms and code bases. I'm quite fond of GPs because they are mathematically beautiful and they have many nice properties. They also work with one of the nicest distributions to work with: the Guassian distribution. They deal with uncertainty and because they are function based, they work well with small (and large datasets) with sophisticated interpolation schemes. The biggest problem of GPs back in the day was that they did not scale but nowadays that isn't the case as there are dozens of sparse methods that allow us to scale GPs to 100K+ up to even 1 million data points.","title":"Introduction"},{"location":"literature/#my-perspective","text":"I think that the GP literature is fairly spread over many fields but there is not really much of any compilation of GP resources (not until recently at least). I have been studying GPs for a while now to try to understand the field as well to see how they have been used. My feeling is that there are more methods than actual applications and many of the algorithms seem to have never been applied to many things outside of their \"corner case\" application. If you look at a standard GP algorithm, it seems to go within one of the following modifications: Algorithm original algorithm is invented Special Kernel a special kernel is invented for a specific use case Fourier-ify they do some random fourier features approximation scheme (or some other approximate kernel methods) Heteroscedastic-ify they modify the likelihood such that the noise function varies w.r.t. the inputs X as well. Sparse-ify they scale it using subsampling, model approximations or posterior approximations Latent Variable Representation they consider the case where the variables are latent and not deterministic MultiOutput-ify they apply it to multioutput (or multifidelity cases) Deep-ify they start stacking the GPs on top of each other In reality, almost all of the methods you'll find in the literature come within these subfields or a combination of a 2 or more more or less in the order I've listed. That's not to say that what people do isn't important or impressive, but it would be nice if we had some better structure to how we classify GP algorithms and improvements we make. My Special Interests There are a few questions and issues which I think have not been answered but are steadily improving with time. They are individual components of the GP algorithms that I consider a bit weak and I think they can be improved by taking knowledge from other fields. A good example would be how the treatment of Matrix-Vector-Multiplication (MVM) was used for scaling GP algorithms. Back in the day, the dominant community was various ways to subsample the data. The MVM community was fairly small within the main GP community but now it has risen as the most scalable and customizable method to date with a dominant python package. Just goes to show how other fields can really come in and improve some of the little intricate aspects of the algorithms and make huge improvements. Below is a list of methods which are not as prevalent in the literature that you won't see at major machine learning conferences but that I believe are super important and could possibly greatly improve the methods that we already have. I will try to pay special attention to #1 in particular because it is apart of my research. Input Uncertainty Uncertainty Calibration Kernel Parameters (initializations and priors) Missing Data / Semi-Supervised Learning Training procedures Expressive Kernels","title":"My Perspective"},{"location":"literature/#topics","text":"","title":"Topics"},{"location":"literature/#gaussian-processes","text":"If you're new to GPs, start here. I'll go over some of the necessary resources to get you up to speed.","title":"Gaussian Processes"},{"location":"literature/#sparse-gaussian-processes","text":"This contains some of the main GP algorithms that you need to know in the way of scaling. If your dataset is larger than 2K then you should probably start using one of these sparse methods.","title":"Sparse Gaussian Processes"},{"location":"literature/#latent-variable-models","text":"These algorithms are when we assume that the input X X is not determinant but instead an unknown. The applications of this range from dimensionality reduction to uncertain inputs and even applications in missing data and semi-supervised learning.","title":"Latent Variable Models"},{"location":"literature/#sparse-spectrum","text":"These algorithms make use of the Bochner theorem which states that we can represent kernel functions as an infinite series with weights that stem from a Gaussian distribution. These (what are essentially Fourier transformations) are typically known as Sparse Spectrum GPs in the community.","title":"Sparse Spectrum"},{"location":"literature/#uncertain-inputs","text":"This is directly related to my research so I'll pay special attention to this. I look at the literature spanned from the beginning up until now. This will mostly be about moment-matching algorithms and algorithms that use variational inference.","title":"Uncertain Inputs"},{"location":"literature/#deep-gps","text":"I have made this a special section because I think it's quite an interesting topic. It's still a fairly young aspect of GPs (last 7 years or so) so it won't have such a depth of literature like the other topics. I'll also include stuff related to how Deep GPs are related to neural networks.","title":"Deep GPs"},{"location":"literature/#neural-networks-and-gps","text":"This section is made up of the papers that talk about the connections between neural networks and GPs as well as some specific cases where people have used neural networks to extract features as inputs for GPs (i.e. Deep Kernel Learning).","title":"Neural Networks and GPs"},{"location":"literature/#inference","text":"A very important part of GPs, how do we obtain the hyperparameters. So this section will be looking at methods by GPs. I'll include the usual suspects such as maximum likelihood, variational inference, expectation propagation and monte carlo methods.","title":"Inference"},{"location":"literature/#kernels","text":"I have a section where you can find stuff on different kernel methods that have specific use cases.","title":"Kernels"},{"location":"literature/#software","text":"The fun part. Here is where I look at all the latest software that one can use to run some of the SOTA algorithms. It will python libraries only because it's the language I personally use to code.","title":"Software"},{"location":"literature/#applications","text":"This consists of papers which have applied GPs to their respective fields. I'll focus more on Earth observation applications but I'll put up any others if I find them of interest.","title":"Applications"},{"location":"literature/#special","text":"This will include all of the GP algorithms that I consider 'corner cases', i.e. GPs modified that apply to a specific application where some modification was necessary to make the GP work their use case.","title":"Special"},{"location":"literature/applications/","text":"Applications \u00b6 Earth Observation \u00b6 All papers in this section look at applications in Earth observation e.g. parameter retrieval, time-scale modeling, remote sensing, and any data dealing with multi/hyperspectral imagery. A Survey on Gaussian Processes for Earth-Observation Data Analysis - Camps-Valls et. al. (2016) | Paper This survey introduces GPs in a fairly simple way and lists a few basic GP algorithms. It also goes over some applications such as parameter retrieval, emulation and feature ranking. It's a great overview of GPs in the field from standard GPs. The software recommendations are outdated. I suggest you check out the GPy library for all GP algorithms mentioned in the paper. Also the treatment of sparse GPs (especially Matrix-Vector-Multiplications (MVNs)) is very brief and underrated. See this for more information about large scale GPs. Gaussian Processes for Vegetation Parameter Estimation from Hyperspectral Data with Limited Ground Truth - Gewali et. al. (2019) | Paper Physics-Aware \u00b6 Physics-aware Gaussian processes in remote sensing - Camps-Valls et. al. (2018) | Paper Other Arguments \u00b6 GPs for Little Data","title":"Applications"},{"location":"literature/applications/#applications","text":"","title":"Applications"},{"location":"literature/applications/#earth-observation","text":"All papers in this section look at applications in Earth observation e.g. parameter retrieval, time-scale modeling, remote sensing, and any data dealing with multi/hyperspectral imagery. A Survey on Gaussian Processes for Earth-Observation Data Analysis - Camps-Valls et. al. (2016) | Paper This survey introduces GPs in a fairly simple way and lists a few basic GP algorithms. It also goes over some applications such as parameter retrieval, emulation and feature ranking. It's a great overview of GPs in the field from standard GPs. The software recommendations are outdated. I suggest you check out the GPy library for all GP algorithms mentioned in the paper. Also the treatment of sparse GPs (especially Matrix-Vector-Multiplications (MVNs)) is very brief and underrated. See this for more information about large scale GPs. Gaussian Processes for Vegetation Parameter Estimation from Hyperspectral Data with Limited Ground Truth - Gewali et. al. (2019) | Paper","title":"Earth Observation"},{"location":"literature/applications/#physics-aware","text":"Physics-aware Gaussian processes in remote sensing - Camps-Valls et. al. (2018) | Paper","title":"Physics-Aware"},{"location":"literature/applications/#other-arguments","text":"GPs for Little Data","title":"Other Arguments"},{"location":"literature/deep_gps/","text":"Deep Gaussian Processes \u00b6 These are GP models that stack GPs one after the other. As far as understanding, the best would be lectures as I have highlighted below. Resources \u00b6 Neil Lawrence @ MLSS 2019 > I would say this is the best lecture to understand the nature of GPs and why we would might want to use them. Blog | Lecture | Slides Neil Lawrence @ GPSS 2019 Notes | Lecture Maurizio Filippone @ DeepBayes.ru 2018 I would say this is the second best lecture because Maurizio gives a nice overview of the GP methods there already are (at the time). Lecture | Slides | New Slides Algorithms \u00b6 The literature isn\u2019t so big but there a number of different implementations depending on the lab: Variational Inference This is the most popular method and has been pursued the most. It's also the implementation that you will find standard in libraries like GPyTorch, GPFlow and Pyro. Expectation Propagation This group used expectation propagation to train the GP. They haven't really done so much since then and I'm not entirely sure why this line of the DGP has gone a bit dry. It would be nice if they resumed. I suspect it may be because of the software. I haven't seen too much software that focuses on clever expectation propagation schemes; they mainly focus on variational inference and MC sampling schemes. MC sampling One lab has tackled this where you can use some variants of MC sampling to train a Deep GP. You'll find this standard in many GP libraries because it's fairly easy to integrate in almost any scheme. MC sampling is famous for being slow but the community is working on it. I imagine a break-through is bound to happen. Random Feature Expansions This uses RFF to approximate a GP and then stacks these on top. I find this a big elegent and probably the simplest. But I didn't see too much research on the tiny bits of the algorithm like the training or the initialization procedures. I don\u2019t think there is any best one because I\u2019m almost certain noone has done any complete comparison. I can say that the VI one is the most studied because that lab is still working on it. In the meantime, personally I would try to use implementations in standard libraries where the devs have ironed out the bugs and allowed for easy customization and configuration; so basically the doubly stochastic. Variational Inference \u00b6 Deep Gaussian Processes - Damianou & Lawrence (2013) This paper is the original method of Deep GPs. It might not be useful for production but there are still many insights to be had from the originators. Code Nested Variational Compression in Deep Gaussian Processes - Hensman & Lawrence (2014) Doubly Stochastic Variational Inference for Deep Gaussian Processes - Salimbeni & Deisenroth (2017) This paper uses stochastic gradient descent for training the Deep GP. I think this achieves the state-of-the-art results thus far. It also has the most implementations in the standard literature. Code | Pyro | GPyTorch Random Fourier Features \u00b6 Random Feature Expansions for Deep Gaussian Processes - Cutjar et. al. (2017) This implementation uses ideas from random fourier features in conjunction with Deep GPs. Paper II | Video | Code Lecture I | Slides | Lecture (Maurizio) | Slides | Code MC Sampling \u00b6 Learning deep latent Gaussian models with Markov chain Monte Carlo - Hoffman (2017) Inference in Deep Gaussian Processes Using Stochastic Gradient Hamiltonian Monte Carlo - Havasi et. al. (2018) Expectation Propagation \u00b6 Deep Gaussian Processes for Regression using Approximate Expectation Propagation - Bui et. al. (2016) This paper uses an approximate expectation method for the inference in Deep GPs. Paper | Code Hybrids \u00b6 Deep Gaussian Processes with Importance-Weighted Variational Inference - Salimbeni et. al. (2019) This paper uses the idea that our noisy inputs are instead 'latent covariates' instead of additive noise or that our input itself is a latent covariate. They also propose a way to do importance sampling coupled with variational inference to improve single layer and multiple layer GPs and have shown that they can get equivalent or better results than just standard variational inference. The latent variables alone will improve performance for both the IWVI and the VI training procedures. * Paper | Code | Video | Poster | ICML 2019 Slides | Workshop Slides Insights \u00b6 Problems with Deep GPs \u00b6 Deep Gaussian Process Pathologies - Paper This paper shows how some of the kernel compositions give very bad estimates of the functions between layers; similar to how residual NN do much better.","title":"Deep GPs"},{"location":"literature/deep_gps/#deep-gaussian-processes","text":"These are GP models that stack GPs one after the other. As far as understanding, the best would be lectures as I have highlighted below.","title":"Deep Gaussian Processes"},{"location":"literature/deep_gps/#resources","text":"Neil Lawrence @ MLSS 2019 > I would say this is the best lecture to understand the nature of GPs and why we would might want to use them. Blog | Lecture | Slides Neil Lawrence @ GPSS 2019 Notes | Lecture Maurizio Filippone @ DeepBayes.ru 2018 I would say this is the second best lecture because Maurizio gives a nice overview of the GP methods there already are (at the time). Lecture | Slides | New Slides","title":"Resources"},{"location":"literature/deep_gps/#algorithms","text":"The literature isn\u2019t so big but there a number of different implementations depending on the lab: Variational Inference This is the most popular method and has been pursued the most. It's also the implementation that you will find standard in libraries like GPyTorch, GPFlow and Pyro. Expectation Propagation This group used expectation propagation to train the GP. They haven't really done so much since then and I'm not entirely sure why this line of the DGP has gone a bit dry. It would be nice if they resumed. I suspect it may be because of the software. I haven't seen too much software that focuses on clever expectation propagation schemes; they mainly focus on variational inference and MC sampling schemes. MC sampling One lab has tackled this where you can use some variants of MC sampling to train a Deep GP. You'll find this standard in many GP libraries because it's fairly easy to integrate in almost any scheme. MC sampling is famous for being slow but the community is working on it. I imagine a break-through is bound to happen. Random Feature Expansions This uses RFF to approximate a GP and then stacks these on top. I find this a big elegent and probably the simplest. But I didn't see too much research on the tiny bits of the algorithm like the training or the initialization procedures. I don\u2019t think there is any best one because I\u2019m almost certain noone has done any complete comparison. I can say that the VI one is the most studied because that lab is still working on it. In the meantime, personally I would try to use implementations in standard libraries where the devs have ironed out the bugs and allowed for easy customization and configuration; so basically the doubly stochastic.","title":"Algorithms"},{"location":"literature/deep_gps/#variational-inference","text":"Deep Gaussian Processes - Damianou & Lawrence (2013) This paper is the original method of Deep GPs. It might not be useful for production but there are still many insights to be had from the originators. Code Nested Variational Compression in Deep Gaussian Processes - Hensman & Lawrence (2014) Doubly Stochastic Variational Inference for Deep Gaussian Processes - Salimbeni & Deisenroth (2017) This paper uses stochastic gradient descent for training the Deep GP. I think this achieves the state-of-the-art results thus far. It also has the most implementations in the standard literature. Code | Pyro | GPyTorch","title":"Variational Inference"},{"location":"literature/deep_gps/#random-fourier-features","text":"Random Feature Expansions for Deep Gaussian Processes - Cutjar et. al. (2017) This implementation uses ideas from random fourier features in conjunction with Deep GPs. Paper II | Video | Code Lecture I | Slides | Lecture (Maurizio) | Slides | Code","title":"Random Fourier Features"},{"location":"literature/deep_gps/#mc-sampling","text":"Learning deep latent Gaussian models with Markov chain Monte Carlo - Hoffman (2017) Inference in Deep Gaussian Processes Using Stochastic Gradient Hamiltonian Monte Carlo - Havasi et. al. (2018)","title":"MC Sampling"},{"location":"literature/deep_gps/#expectation-propagation","text":"Deep Gaussian Processes for Regression using Approximate Expectation Propagation - Bui et. al. (2016) This paper uses an approximate expectation method for the inference in Deep GPs. Paper | Code","title":"Expectation Propagation"},{"location":"literature/deep_gps/#hybrids","text":"Deep Gaussian Processes with Importance-Weighted Variational Inference - Salimbeni et. al. (2019) This paper uses the idea that our noisy inputs are instead 'latent covariates' instead of additive noise or that our input itself is a latent covariate. They also propose a way to do importance sampling coupled with variational inference to improve single layer and multiple layer GPs and have shown that they can get equivalent or better results than just standard variational inference. The latent variables alone will improve performance for both the IWVI and the VI training procedures. * Paper | Code | Video | Poster | ICML 2019 Slides | Workshop Slides","title":"Hybrids"},{"location":"literature/deep_gps/#insights","text":"","title":"Insights"},{"location":"literature/deep_gps/#problems-with-deep-gps","text":"Deep Gaussian Process Pathologies - Paper This paper shows how some of the kernel compositions give very bad estimates of the functions between layers; similar to how residual NN do much better.","title":"Problems with Deep GPs"},{"location":"literature/fourier/","text":"Sparse Spectrum Gaussian Processes \u00b6 These are essentially the analogue to the random fourier features for Gaussian processes. SSGP \u00b6 Sparse Spectrum Gaussian Process Regression - L\u00e1zaro-Gredilla et. al. (2010) - PDF The original algorithm for SSGP. Prediction under Uncertainty in Sparse Spectrum Gaussian Processes with Applications to Filtering and Control - Pan et. al. (2017) - PDF > This is a moment matching extension to deal with the uncertainty in the inputs at prediction time. Python Implementation Numpy GPFlow Variational SSGPs \u00b6 So according to this paper the SSGP algorithm had a tendency to overfit. So they added some additional parameters to account for the noise in the inputs making the marginal likelihood term intractable. They added variational methods to deal with the 1. Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs - Gal et. al. (2015) \"...proposed variational inference in a sparse spectrum model that is derived from a GP model.\" - Hensman et. al. (2018) 2. Variational Fourier Features for Gaussian Processes - Hensman et al (2018) \"...our work aims to directly approximate the posterior of the true models using a variational representation.\" - Hensman et. al. (2018) Yarin Gal's Stuff - website Code Numpy Theano Uncertain Inputs \u00b6 I've only seen one paper that attempts to extend this method to account for uncertain inputs. Prediction under Uncertainty in Sparse Spectrum Gaussian Processes with Applications to Filtering and Control - Pan et. al. (2017) This is the only paper I've seen that tries to extend this method Latest \u00b6 Know Your Boundaries: Constraining Gaussian Processes by Variational Harmonic Features - Solin & Kok (2019)","title":"Fourier"},{"location":"literature/fourier/#sparse-spectrum-gaussian-processes","text":"These are essentially the analogue to the random fourier features for Gaussian processes.","title":"Sparse Spectrum Gaussian Processes"},{"location":"literature/fourier/#ssgp","text":"Sparse Spectrum Gaussian Process Regression - L\u00e1zaro-Gredilla et. al. (2010) - PDF The original algorithm for SSGP. Prediction under Uncertainty in Sparse Spectrum Gaussian Processes with Applications to Filtering and Control - Pan et. al. (2017) - PDF > This is a moment matching extension to deal with the uncertainty in the inputs at prediction time. Python Implementation Numpy GPFlow","title":"SSGP"},{"location":"literature/fourier/#variational-ssgps","text":"So according to this paper the SSGP algorithm had a tendency to overfit. So they added some additional parameters to account for the noise in the inputs making the marginal likelihood term intractable. They added variational methods to deal with the 1. Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs - Gal et. al. (2015) \"...proposed variational inference in a sparse spectrum model that is derived from a GP model.\" - Hensman et. al. (2018) 2. Variational Fourier Features for Gaussian Processes - Hensman et al (2018) \"...our work aims to directly approximate the posterior of the true models using a variational representation.\" - Hensman et. al. (2018) Yarin Gal's Stuff - website Code Numpy Theano","title":"Variational SSGPs"},{"location":"literature/fourier/#uncertain-inputs","text":"I've only seen one paper that attempts to extend this method to account for uncertain inputs. Prediction under Uncertainty in Sparse Spectrum Gaussian Processes with Applications to Filtering and Control - Pan et. al. (2017) This is the only paper I've seen that tries to extend this method","title":"Uncertain Inputs"},{"location":"literature/fourier/#latest","text":"Know Your Boundaries: Constraining Gaussian Processes by Variational Harmonic Features - Solin & Kok (2019)","title":"Latest"},{"location":"literature/inference/","text":"Inference \u00b6 Variational Inference \u00b6 This section outlines a few interesting papers I found where they are trying to improve how we do variational inference. I try to stick to methods where people have tried and succeeded at applying them to GPs. Below are a few key SOTA objective functions that you may come across in the GP literature. The most common is definitely the Variational ELBO but there are a few unknown objective functions that came out recently and I think they might be useful in the future. We just need to get them implemented and tested. Along the way there have been other modifications. Variational Evidence Lower Bound (ELBO) \u00b6 This is the standard objective function that you will find the literature. Scalable Variational Gaussian Process Classification - Hensman et. al. (2015) Details \\mathcal{L}_{ELBO} = \\sum_{i=1}^{N} \\mathbb{E}_{q(\\mathbf{u})} \\left[ \\mathbb{E}_{f(f|\\mathbf{u})} \\left[ \\log p(y_i | f_i) \\right] \\right] - \\beta D_{KL}\\left[ q(\\mathbf{u} || p(\\mathbf{u})) \\right] \\mathcal{L}_{ELBO} = \\sum_{i=1}^{N} \\mathbb{E}_{q(\\mathbf{u})} \\left[ \\mathbb{E}_{f(f|\\mathbf{u})} \\left[ \\log p(y_i | f_i) \\right] \\right] - \\beta D_{KL}\\left[ q(\\mathbf{u} || p(\\mathbf{u})) \\right] where: N N - number of data points p(\\mathbf{u}) p(\\mathbf{u}) - prior distribution for the inducing function values q(\\mathbf{u}) q(\\mathbf{u}) - variational distribution for the inducing function values \\beta \\beta - free parameter for the D_{KL} D_{KL} regularization penalization Natural Gradients (NGs) \u00b6 Natural Gradients in Practice: Non-Conjugate Variational Inference in Gaussian Process Models - Salimbeni et. al. (2018) | Code This paper argues that training sparse GP algorithms with gradient descent can be quite slow due to the need to optimize the variational parameters q_\\phi(u) q_\\phi(u) as well as the model parameters. So they propose to use the natural gradient for the variational parameters and then the standard gradient methods for the remaining parameters. They show that the SVGP and the DGP methods all converge much faster with this training regime. I imagine this would also be super useful for the BayesianGPLVM where we also have variational parameters for our inputs as well. Noisy Natural Gradient as Variational Inference - Zhang (2018) - Code PyTorch Implementation Importance Weighted Variational Inference (IWVI) \u00b6 Deep Gaussian Processes with Importance-Weighted Variational Inference - Salimbeni et. al. (2019) - Paper | Code | Video | Poster | ICML 2019 Slides | Workshop Slides They propose a way to do importance sampling coupled with variational inference to improve single layer and multi-layer GPs and have shown that they can get equivalent or better results than just standard variational inference. Importance Weighting and Variational Inference - Domke & Sheldon (2018) Predictive Log Likelihood (PLL) \u00b6 Sparse Gaussian Process Regression Beyond Variational Inference - Jankowiak et. al. (2019) Details \\begin{aligned} \\mathcal{L}_{PLL} &= \\mathbb{E}_{p_{data}(\\mathbf{y}, \\mathbf{x})} \\left[ \\log p(\\mathbf{y|x})\\right] - \\beta D_{KL}\\left[ q(\\mathbf{u}|| p(\\mathbf{u})\\right] \\\\ &\\approx \\sum_{i=1}^{N} \\log \\mathbb{E}_{q(\\mathbf{u})} \\left[ \\int p(y_i |f_i) p(f_i | \\mathbf{u})df_i \\right] - \\beta D_{KL}\\left[ q(\\mathbf{u}) || p(\\mathbf{u}) \\right] \\end{aligned} \\begin{aligned} \\mathcal{L}_{PLL} &= \\mathbb{E}_{p_{data}(\\mathbf{y}, \\mathbf{x})} \\left[ \\log p(\\mathbf{y|x})\\right] - \\beta D_{KL}\\left[ q(\\mathbf{u}|| p(\\mathbf{u})\\right] \\\\ &\\approx \\sum_{i=1}^{N} \\log \\mathbb{E}_{q(\\mathbf{u})} \\left[ \\int p(y_i |f_i) p(f_i | \\mathbf{u})df_i \\right] - \\beta D_{KL}\\left[ q(\\mathbf{u}) || p(\\mathbf{u}) \\right] \\end{aligned} where: N N - number of data points p(\\mathbf{u}) p(\\mathbf{u}) - prior distribution for the inducing function values q(\\mathbf{u}) q(\\mathbf{u}) - variational distribution for the inducing function values Generalized Variational Inference (GVI) \u00b6 Generalized Variational Inference - Knoblauch et. al. (2019) A generalized Bayesian inference framework. It goes into a different variational family related to Renyi's family of Information theoretic methods; which isn't very typical because normally we look at the Shannon perspective. They had success applying it to Bayesian Neural Networks and Deep Gaussian Processes. Deep GP paper Gradient Descent Regimes \u00b6 Parallel training of DNNs with Natural Gradient and Parameter Averaging - Povey et. al. (2014) | Code | Blog A seamingly drop-in replacement for stochastic gradient descent with some added benefits of being shown to improve generalization tasks, stability of the training, and can help obtain high quality uncertainty estimates. Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm - Lui & Wang (2016) A tractable approach for learning high dimensional prob dist using Functional Gradient Descent in RKHS. It's from a connection with the derivative of the KL divergence and the Stein's identity. * Stein's Method Webpage * Pyro Implementation Regularization \u00b6 Regularized Sparse Gaussian Processes - Meng & Lee (2019) [ arxiv ] Impose a regularization coefficient on the KL term in the Sparse GP implementation. Addresses issue where the distribution of the inducing inputs fail to capture the distribution of the training inputs.","title":"Inference"},{"location":"literature/inference/#inference","text":"","title":"Inference"},{"location":"literature/inference/#variational-inference","text":"This section outlines a few interesting papers I found where they are trying to improve how we do variational inference. I try to stick to methods where people have tried and succeeded at applying them to GPs. Below are a few key SOTA objective functions that you may come across in the GP literature. The most common is definitely the Variational ELBO but there are a few unknown objective functions that came out recently and I think they might be useful in the future. We just need to get them implemented and tested. Along the way there have been other modifications.","title":"Variational Inference"},{"location":"literature/inference/#variational-evidence-lower-bound-elbo","text":"This is the standard objective function that you will find the literature. Scalable Variational Gaussian Process Classification - Hensman et. al. (2015) Details \\mathcal{L}_{ELBO} = \\sum_{i=1}^{N} \\mathbb{E}_{q(\\mathbf{u})} \\left[ \\mathbb{E}_{f(f|\\mathbf{u})} \\left[ \\log p(y_i | f_i) \\right] \\right] - \\beta D_{KL}\\left[ q(\\mathbf{u} || p(\\mathbf{u})) \\right] \\mathcal{L}_{ELBO} = \\sum_{i=1}^{N} \\mathbb{E}_{q(\\mathbf{u})} \\left[ \\mathbb{E}_{f(f|\\mathbf{u})} \\left[ \\log p(y_i | f_i) \\right] \\right] - \\beta D_{KL}\\left[ q(\\mathbf{u} || p(\\mathbf{u})) \\right] where: N N - number of data points p(\\mathbf{u}) p(\\mathbf{u}) - prior distribution for the inducing function values q(\\mathbf{u}) q(\\mathbf{u}) - variational distribution for the inducing function values \\beta \\beta - free parameter for the D_{KL} D_{KL} regularization penalization","title":"Variational Evidence Lower Bound (ELBO)"},{"location":"literature/inference/#natural-gradients-ngs","text":"Natural Gradients in Practice: Non-Conjugate Variational Inference in Gaussian Process Models - Salimbeni et. al. (2018) | Code This paper argues that training sparse GP algorithms with gradient descent can be quite slow due to the need to optimize the variational parameters q_\\phi(u) q_\\phi(u) as well as the model parameters. So they propose to use the natural gradient for the variational parameters and then the standard gradient methods for the remaining parameters. They show that the SVGP and the DGP methods all converge much faster with this training regime. I imagine this would also be super useful for the BayesianGPLVM where we also have variational parameters for our inputs as well. Noisy Natural Gradient as Variational Inference - Zhang (2018) - Code PyTorch Implementation","title":"Natural Gradients (NGs)"},{"location":"literature/inference/#importance-weighted-variational-inference-iwvi","text":"Deep Gaussian Processes with Importance-Weighted Variational Inference - Salimbeni et. al. (2019) - Paper | Code | Video | Poster | ICML 2019 Slides | Workshop Slides They propose a way to do importance sampling coupled with variational inference to improve single layer and multi-layer GPs and have shown that they can get equivalent or better results than just standard variational inference. Importance Weighting and Variational Inference - Domke & Sheldon (2018)","title":"Importance Weighted Variational Inference (IWVI)"},{"location":"literature/inference/#predictive-log-likelihood-pll","text":"Sparse Gaussian Process Regression Beyond Variational Inference - Jankowiak et. al. (2019) Details \\begin{aligned} \\mathcal{L}_{PLL} &= \\mathbb{E}_{p_{data}(\\mathbf{y}, \\mathbf{x})} \\left[ \\log p(\\mathbf{y|x})\\right] - \\beta D_{KL}\\left[ q(\\mathbf{u}|| p(\\mathbf{u})\\right] \\\\ &\\approx \\sum_{i=1}^{N} \\log \\mathbb{E}_{q(\\mathbf{u})} \\left[ \\int p(y_i |f_i) p(f_i | \\mathbf{u})df_i \\right] - \\beta D_{KL}\\left[ q(\\mathbf{u}) || p(\\mathbf{u}) \\right] \\end{aligned} \\begin{aligned} \\mathcal{L}_{PLL} &= \\mathbb{E}_{p_{data}(\\mathbf{y}, \\mathbf{x})} \\left[ \\log p(\\mathbf{y|x})\\right] - \\beta D_{KL}\\left[ q(\\mathbf{u}|| p(\\mathbf{u})\\right] \\\\ &\\approx \\sum_{i=1}^{N} \\log \\mathbb{E}_{q(\\mathbf{u})} \\left[ \\int p(y_i |f_i) p(f_i | \\mathbf{u})df_i \\right] - \\beta D_{KL}\\left[ q(\\mathbf{u}) || p(\\mathbf{u}) \\right] \\end{aligned} where: N N - number of data points p(\\mathbf{u}) p(\\mathbf{u}) - prior distribution for the inducing function values q(\\mathbf{u}) q(\\mathbf{u}) - variational distribution for the inducing function values","title":"Predictive Log Likelihood (PLL)"},{"location":"literature/inference/#generalized-variational-inference-gvi","text":"Generalized Variational Inference - Knoblauch et. al. (2019) A generalized Bayesian inference framework. It goes into a different variational family related to Renyi's family of Information theoretic methods; which isn't very typical because normally we look at the Shannon perspective. They had success applying it to Bayesian Neural Networks and Deep Gaussian Processes. Deep GP paper","title":"Generalized Variational Inference (GVI)"},{"location":"literature/inference/#gradient-descent-regimes","text":"Parallel training of DNNs with Natural Gradient and Parameter Averaging - Povey et. al. (2014) | Code | Blog A seamingly drop-in replacement for stochastic gradient descent with some added benefits of being shown to improve generalization tasks, stability of the training, and can help obtain high quality uncertainty estimates. Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm - Lui & Wang (2016) A tractable approach for learning high dimensional prob dist using Functional Gradient Descent in RKHS. It's from a connection with the derivative of the KL divergence and the Stein's identity. * Stein's Method Webpage * Pyro Implementation","title":"Gradient Descent Regimes"},{"location":"literature/inference/#regularization","text":"Regularized Sparse Gaussian Processes - Meng & Lee (2019) [ arxiv ] Impose a regularization coefficient on the KL term in the Sparse GP implementation. Addresses issue where the distribution of the inducing inputs fail to capture the distribution of the training inputs.","title":"Regularization"},{"location":"literature/kernels/","text":"Kernel Functions \u00b6 Deep Kernel Learning \u00b6 This is a Probabilistic Neural Network (PNN). It's when we try to learn features through a Neural Network and then on the last layer, we fit a Gaussian Process. It's a great idea and I think that this has a lot of potential. One of the criticisms of people in the GP community ( Bonilla et. al., 2016 ) is that we don't typically use very expressive kernels. That's where the power of GPs come from. So if we can have kernels from Neural Networks (one of the most expressive ML methods available to date), then we can get a potentially great ML algorithm. Even in practice, a developer have stated that we can get state-of-the-art results with some minimum tweaking of the architecture. Comment I've also heard this called \"Deep Feature Extraction\". This is NOT a Deep GP. I've seen one paper that incorrectly called it that. A deep GP is where we stack GPs on top of each other. See the deep GP guide for more details. Literature Deep Kernel Learning - Wilson et. al. (2015) Stochastic Variational Deep Kernel learning - Wilson et. al. (2016) A Representer Theorem for Deep Kernel Learning - Bohn et. al. (2019) Software \u00b6 Multiple Kernel Learning - MKLpy Kernel Methods - kernelmethods pykernels > A huge suite of different python kernels. kernpy Library focused on statistical tests keops Use kernel methods on the GPU with autograd and without memory overflows. Backend of numpy and pytorch. pyGPs This is a GP library but I saw quite a few graph kernels implemented with different Laplacian matrices implemented. megaman A library for large scale manifold learning. I saw quite a few different Laplacian matrices implemented.","title":"Kernels"},{"location":"literature/kernels/#kernel-functions","text":"","title":"Kernel Functions"},{"location":"literature/kernels/#deep-kernel-learning","text":"This is a Probabilistic Neural Network (PNN). It's when we try to learn features through a Neural Network and then on the last layer, we fit a Gaussian Process. It's a great idea and I think that this has a lot of potential. One of the criticisms of people in the GP community ( Bonilla et. al., 2016 ) is that we don't typically use very expressive kernels. That's where the power of GPs come from. So if we can have kernels from Neural Networks (one of the most expressive ML methods available to date), then we can get a potentially great ML algorithm. Even in practice, a developer have stated that we can get state-of-the-art results with some minimum tweaking of the architecture. Comment I've also heard this called \"Deep Feature Extraction\". This is NOT a Deep GP. I've seen one paper that incorrectly called it that. A deep GP is where we stack GPs on top of each other. See the deep GP guide for more details. Literature Deep Kernel Learning - Wilson et. al. (2015) Stochastic Variational Deep Kernel learning - Wilson et. al. (2016) A Representer Theorem for Deep Kernel Learning - Bohn et. al. (2019)","title":"Deep Kernel Learning"},{"location":"literature/kernels/#software","text":"Multiple Kernel Learning - MKLpy Kernel Methods - kernelmethods pykernels > A huge suite of different python kernels. kernpy Library focused on statistical tests keops Use kernel methods on the GPU with autograd and without memory overflows. Backend of numpy and pytorch. pyGPs This is a GP library but I saw quite a few graph kernels implemented with different Laplacian matrices implemented. megaman A library for large scale manifold learning. I saw quite a few different Laplacian matrices implemented.","title":"Software"},{"location":"literature/large_scale/","text":"State-of-the-Art \u00b6 Exact GP on a Million Data Points - Wang et. al. (2019) | Code The authors manage to train a GP with multiple GPUs on a million or so data points. They use the matrix-vector-multiplication strategy. Quite amazing actually... Constant-Time Predictive Distributions for GPs - Pleiss et. al. (2018) | Code Using MVM techniquees, they are able to make constant time predictive mean and variance estimates; addressing the computational bottleneck of predictive distributions for GPs.","title":"Large scale"},{"location":"literature/large_scale/#state-of-the-art","text":"Exact GP on a Million Data Points - Wang et. al. (2019) | Code The authors manage to train a GP with multiple GPUs on a million or so data points. They use the matrix-vector-multiplication strategy. Quite amazing actually... Constant-Time Predictive Distributions for GPs - Pleiss et. al. (2018) | Code Using MVM techniquees, they are able to make constant time predictive mean and variance estimates; addressing the computational bottleneck of predictive distributions for GPs.","title":"State-of-the-Art"},{"location":"literature/latent_variable/","text":"Algorithms \u00b6 Latent Variable Models \u00b6 Figure : (Gal et. al., 2015) Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data - Gal et. al. (2015) - Resources","title":"Latent variable"},{"location":"literature/latent_variable/#algorithms","text":"","title":"Algorithms"},{"location":"literature/latent_variable/#latent-variable-models","text":"Figure : (Gal et. al., 2015) Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data - Gal et. al. (2015) - Resources","title":"Latent Variable Models"},{"location":"literature/multioutput/","text":"Multi-Output Models \u00b6 In regression, this is when we try to learn a function f f or multiple functions to predict multiple output y y . Details Concretly, we are trying to predict y\\in \\mathbb{R}^{N\\times P} y\\in \\mathbb{R}^{N\\times P} where N N is the number of samples and P P is the number of outputs. Terminology \u00b6 Firstly, there is some confusion about the terminology. I've heard the following names: Multi-Task when we have different outputs. BUT possibly with different learning objectives and even different data types. e.g. one output is a regression task, one output is a classification problem, one output is a meta-learning task, etc. (a.k.a. the different tasks). The easiest parallel I can think of is self-driving cars. The objective is to drive, but you need to do many different tasks in order to reach the objective: drive without crashing. Multi-Output typically when we have one task but just more than one output. e.g. multiple outputs for regression or multiple outputs for classification. So a concrete example is if we are predicting temperature and windspeed from some inputs. Multi-Fidelity same as multi-output but catered more to situations where we know one of the outputs is of lower quality. e.g. a regression problem where one of the targets has less resolution or perhaps missing some data. These definitions come from a discussion I had with the GPFlow Community . I have yet to see a paper that is consistent with how these are done. I have broken up each section based off of their name but as seen from the names, there is a lot of overlap. Lectures \u00b6 GPSS 2017 Summer School - Alvarez - Video | Slides Literature \u00b6 Multi-Task \u00b6 Multi-task Gaussian Process prediction - Bonilla et al. 2007 - paper | GPSS 2008 Slides Multi-Output \u00b6 Efficient multioutput Gaussian processes through variational inducing kernels - Alvarez et al. (2011) - paper Remarks on multi-output Gaussian process regression - Liu et. al. (2018) - pdf Heterogeneous Multi-output Gaussian Process Prediction - Moreno-Mu\u00f1ez et. al. (2018) - paper | code Multi-Fidelity \u00b6 Deep Multi-fidelity Gaussian Processes - Raissi & Karniadakis (2016) - paper | blog | Deep Gaussian Processes for Multi-fidelity Modeling - Cutjar et. al. (2019) - paper | notebook | poster","title":"Multi-Output"},{"location":"literature/multioutput/#multi-output-models","text":"In regression, this is when we try to learn a function f f or multiple functions to predict multiple output y y . Details Concretly, we are trying to predict y\\in \\mathbb{R}^{N\\times P} y\\in \\mathbb{R}^{N\\times P} where N N is the number of samples and P P is the number of outputs.","title":"Multi-Output Models"},{"location":"literature/multioutput/#terminology","text":"Firstly, there is some confusion about the terminology. I've heard the following names: Multi-Task when we have different outputs. BUT possibly with different learning objectives and even different data types. e.g. one output is a regression task, one output is a classification problem, one output is a meta-learning task, etc. (a.k.a. the different tasks). The easiest parallel I can think of is self-driving cars. The objective is to drive, but you need to do many different tasks in order to reach the objective: drive without crashing. Multi-Output typically when we have one task but just more than one output. e.g. multiple outputs for regression or multiple outputs for classification. So a concrete example is if we are predicting temperature and windspeed from some inputs. Multi-Fidelity same as multi-output but catered more to situations where we know one of the outputs is of lower quality. e.g. a regression problem where one of the targets has less resolution or perhaps missing some data. These definitions come from a discussion I had with the GPFlow Community . I have yet to see a paper that is consistent with how these are done. I have broken up each section based off of their name but as seen from the names, there is a lot of overlap.","title":"Terminology"},{"location":"literature/multioutput/#lectures","text":"GPSS 2017 Summer School - Alvarez - Video | Slides","title":"Lectures"},{"location":"literature/multioutput/#literature","text":"","title":"Literature"},{"location":"literature/multioutput/#multi-task","text":"Multi-task Gaussian Process prediction - Bonilla et al. 2007 - paper | GPSS 2008 Slides","title":"Multi-Task"},{"location":"literature/multioutput/#multi-output","text":"Efficient multioutput Gaussian processes through variational inducing kernels - Alvarez et al. (2011) - paper Remarks on multi-output Gaussian process regression - Liu et. al. (2018) - pdf Heterogeneous Multi-output Gaussian Process Prediction - Moreno-Mu\u00f1ez et. al. (2018) - paper | code","title":"Multi-Output"},{"location":"literature/multioutput/#multi-fidelity","text":"Deep Multi-fidelity Gaussian Processes - Raissi & Karniadakis (2016) - paper | blog | Deep Gaussian Processes for Multi-fidelity Modeling - Cutjar et. al. (2019) - paper | notebook | poster","title":"Multi-Fidelity"},{"location":"literature/neural_networks/","text":"Neural Networks and Gaussian Processes \u00b6 Neural Networks & Deep Gaussian Processes \u00b6 Building Bayesian Neural Networks with Blocks: On Structure, Interpretability and Uncertainty - Zhou et. al. (2018) Deep Kernel Learning \u00b6 This is a Probabilistic Neural Network (PNN). It's when we try to learn features through a Neural Network and then on the last layer, we fit a Gaussian Process. It's a great idea and I think that this has a lot of potential. One of the criticisms of people in the GP community ( Bonilla et. al., 2016 ) is that we don't typically use very expressive kernels. That's where the power of GPs come from. So if we can have kernels from Neural Networks (one of the most expressive ML methods available to date), then we can get a potentially great ML algorithm. Even in practice, a developer have stated that we can get state-of-the-art results with some minimum tweaking of the architecture. Comments : * I've also heard this called \"Deep Feature Extraction\". * This is NOT a Deep GP. I've seen one paper that incorrectly called it that. A deep GP is where we stack GPs on top of each other. See the deep GP guide for more details. Literature Deep Kernel Learning - Wilson et. al. (2015) Stochastic Variational Deep Kernel learning - Wilson et. al. (2016) A Representer Theorem for Deep Kernel Learning - Bohn et. al. (2019) Code TensorFlow Implementation | GP Dist Example Pyro Implementation GPFlow Implementation GPyTorch Implementation Latest \u00b6 Deep Probabilistic Kernels for Sample-Efficient Learning - Mallick et. al. (2019) [ arxiv ] Propose a deep probabilistic kernel to address 1) traditional GP kernels aren't good at capturing similarities between high dimensional data points and 2) deep neural network kernels are not sample efficient. Has aspects such as Random Fourier Features, semi-supervised learning and utilizes the Stein Variational Gradient Descent algorithm. On the expected behaviour of noise regulariseddeep neural networks as Gaussian processes - Pretorius et al (2019) [ arxiv ] They study the impact of noise regularization via droput on Deep Kernel learning. SOTA \u00b6","title":"Neural Networks and Gaussian Processes"},{"location":"literature/neural_networks/#neural-networks-and-gaussian-processes","text":"","title":"Neural Networks and Gaussian Processes"},{"location":"literature/neural_networks/#neural-networks-deep-gaussian-processes","text":"Building Bayesian Neural Networks with Blocks: On Structure, Interpretability and Uncertainty - Zhou et. al. (2018)","title":"Neural Networks &amp; Deep Gaussian Processes"},{"location":"literature/neural_networks/#deep-kernel-learning","text":"This is a Probabilistic Neural Network (PNN). It's when we try to learn features through a Neural Network and then on the last layer, we fit a Gaussian Process. It's a great idea and I think that this has a lot of potential. One of the criticisms of people in the GP community ( Bonilla et. al., 2016 ) is that we don't typically use very expressive kernels. That's where the power of GPs come from. So if we can have kernels from Neural Networks (one of the most expressive ML methods available to date), then we can get a potentially great ML algorithm. Even in practice, a developer have stated that we can get state-of-the-art results with some minimum tweaking of the architecture. Comments : * I've also heard this called \"Deep Feature Extraction\". * This is NOT a Deep GP. I've seen one paper that incorrectly called it that. A deep GP is where we stack GPs on top of each other. See the deep GP guide for more details. Literature Deep Kernel Learning - Wilson et. al. (2015) Stochastic Variational Deep Kernel learning - Wilson et. al. (2016) A Representer Theorem for Deep Kernel Learning - Bohn et. al. (2019) Code TensorFlow Implementation | GP Dist Example Pyro Implementation GPFlow Implementation GPyTorch Implementation","title":"Deep Kernel Learning"},{"location":"literature/neural_networks/#latest","text":"Deep Probabilistic Kernels for Sample-Efficient Learning - Mallick et. al. (2019) [ arxiv ] Propose a deep probabilistic kernel to address 1) traditional GP kernels aren't good at capturing similarities between high dimensional data points and 2) deep neural network kernels are not sample efficient. Has aspects such as Random Fourier Features, semi-supervised learning and utilizes the Stein Variational Gradient Descent algorithm. On the expected behaviour of noise regulariseddeep neural networks as Gaussian processes - Pretorius et al (2019) [ arxiv ] They study the impact of noise regularization via droput on Deep Kernel learning.","title":"Latest"},{"location":"literature/neural_networks/#sota","text":"","title":"SOTA"},{"location":"literature/sparse_gps/","text":"Resources \u00b6 Papers \u00b6 Nystrom Approximation Using Nystrom to Speed Up Kernel Machines - Williams & Seeger (2001) Fully Independent Training Conditional (FITC) Sparse Gaussian Processes Using Pseudo-Inputs - Snelson and Ghahramani (2006) Flexible and Efficient GP Models for Machine Learning - Snelson (2007) Variational Free Energy (VFE) Variational Learning of Inducing Variables in Sparse GPs - Titsias (2009) On Sparse Variational meethods and the KL Divergence between Stochastic Processes - Matthews et. al. (2015) Stochastic Variational Inference Gaussian Processes for Big Data - Hensman et al. (2013) Sparse Spectrum GPR - Lazaro-Gredilla et al. (2010) SGD, SVI Improving the GP SS Approximation by Representing Uncertainty in Frequency Inputs - Gal et al. (2015) Prediction under Uncertainty in SSGPs w/ Applications to Filtering and Control - Pan et. al. (2017) Variational Fourier Features for GPs - Hensman (2018) Understanding Probabilistic Sparse GP Approx - Bauer et. al. (2016) A good paper which highlights some import differences between the FITC, DTC and VFE. It provides a clear notational differences and also mentions how VFE is a special case of DTC. A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation - Bui (2017) A good summary of all of the methods under one unified framework called the Power Expectation Propagation formula. Latest \u00b6 Deep Structured Mixtures of Gaussian Processes - Trapp et. al. (2019) Going back to the old days of improving the local-expert technique. Sparse Gaussian Process Regression Beyond Variational Inference - Jankowiak et. al. (2019) Thesis Explain \u00b6 Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a few thesis that help to explain some of the finer details. I've arranged them in order starting from the easiest to the most difficult. GPR Techniques - Bijl (2016) Chapter V - Noisy Input GPR Non-Stationary Surrogate Modeling with Deep Gaussian Processes - Dutordoir (2016) Chapter IV - Finding Uncertain Patterns in GPs Nonlinear Modeling and Control using GPs - McHutchon (2014) Chapter II - GP w/ Input Noise (NIGP) Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM Presentations \u00b6 Variational Inference for Gaussian and Determinantal Point Processes - Titsias (2014) Notes \u00b6 On the paper: Variational Learning of Inducing Variables in Sparse Gaussian Processees - Bui and Turner (2014) Blogs \u00b6 Variational Free Energy for Sparse GPs - Gonzalo","title":"Sparse GPs"},{"location":"literature/sparse_gps/#resources","text":"","title":"Resources"},{"location":"literature/sparse_gps/#papers","text":"Nystrom Approximation Using Nystrom to Speed Up Kernel Machines - Williams & Seeger (2001) Fully Independent Training Conditional (FITC) Sparse Gaussian Processes Using Pseudo-Inputs - Snelson and Ghahramani (2006) Flexible and Efficient GP Models for Machine Learning - Snelson (2007) Variational Free Energy (VFE) Variational Learning of Inducing Variables in Sparse GPs - Titsias (2009) On Sparse Variational meethods and the KL Divergence between Stochastic Processes - Matthews et. al. (2015) Stochastic Variational Inference Gaussian Processes for Big Data - Hensman et al. (2013) Sparse Spectrum GPR - Lazaro-Gredilla et al. (2010) SGD, SVI Improving the GP SS Approximation by Representing Uncertainty in Frequency Inputs - Gal et al. (2015) Prediction under Uncertainty in SSGPs w/ Applications to Filtering and Control - Pan et. al. (2017) Variational Fourier Features for GPs - Hensman (2018) Understanding Probabilistic Sparse GP Approx - Bauer et. al. (2016) A good paper which highlights some import differences between the FITC, DTC and VFE. It provides a clear notational differences and also mentions how VFE is a special case of DTC. A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation - Bui (2017) A good summary of all of the methods under one unified framework called the Power Expectation Propagation formula.","title":"Papers"},{"location":"literature/sparse_gps/#latest","text":"Deep Structured Mixtures of Gaussian Processes - Trapp et. al. (2019) Going back to the old days of improving the local-expert technique. Sparse Gaussian Process Regression Beyond Variational Inference - Jankowiak et. al. (2019)","title":"Latest"},{"location":"literature/sparse_gps/#thesis-explain","text":"Often times the papers that people publish in conferences in Journals don't have enough information in them. Sometimes it's really difficult to go through some of the mathematics that people put in their articles especially with cryptic explanations like \"it's easy to show that...\" or \"trivially it can be shown that...\". For most of us it's not easy nor is it trivial. So I've included a few thesis that help to explain some of the finer details. I've arranged them in order starting from the easiest to the most difficult. GPR Techniques - Bijl (2016) Chapter V - Noisy Input GPR Non-Stationary Surrogate Modeling with Deep Gaussian Processes - Dutordoir (2016) Chapter IV - Finding Uncertain Patterns in GPs Nonlinear Modeling and Control using GPs - McHutchon (2014) Chapter II - GP w/ Input Noise (NIGP) Deep GPs and Variational Propagation of Uncertainty - Damianou (2015) Chapter IV - Uncertain Inputs in Variational GPs Chapter II (2.1) - Lit Review Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences - Zwie\u00dfele (2017) Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM","title":"Thesis Explain"},{"location":"literature/sparse_gps/#presentations","text":"Variational Inference for Gaussian and Determinantal Point Processes - Titsias (2014)","title":"Presentations"},{"location":"literature/sparse_gps/#notes","text":"On the paper: Variational Learning of Inducing Variables in Sparse Gaussian Processees - Bui and Turner (2014)","title":"Notes"},{"location":"literature/sparse_gps/#blogs","text":"Variational Free Energy for Sparse GPs - Gonzalo","title":"Blogs"},{"location":"literature/special/","text":"Special Models \u00b6 These are models that I deem 'special' in the sense that they're not exactly what GPs were originally tended for but they have usefulness in other fields. I consider these algorithms in the category of \"jacking around\" (see Neal Lawrences lecture for the origin of the phrase). In my mind, these are algorithms that tackle a special case problem in the application realm. It's a bit difficult to verify how it does outside of the corner case due to no real comprehensive code-bases or benchmarks of GPs in real-world applications. Derivative Constraints \u00b6 Scaling GPR with Derivatives - Eriksson et. al. (2018) | Code They use MVM methods to enable one to solve for functions and their derivatives at the same time at scale.","title":"Special Models"},{"location":"literature/special/#special-models","text":"These are models that I deem 'special' in the sense that they're not exactly what GPs were originally tended for but they have usefulness in other fields. I consider these algorithms in the category of \"jacking around\" (see Neal Lawrences lecture for the origin of the phrase). In my mind, these are algorithms that tackle a special case problem in the application realm. It's a bit difficult to verify how it does outside of the corner case due to no real comprehensive code-bases or benchmarks of GPs in real-world applications.","title":"Special Models"},{"location":"literature/special/#derivative-constraints","text":"Scaling GPR with Derivatives - Eriksson et. al. (2018) | Code They use MVM methods to enable one to solve for functions and their derivatives at the same time at scale.","title":"Derivative Constraints"},{"location":"literature/uncertain_inputs/","text":"Uncertain Inputs in Gaussian Processes \u00b6 Please go to my dedicated repository where I explore all things to do with uncertain Gaussian processes. It can be found here: jejjohnson.github.io/uncertain_gps/","title":"Uncertain Inputs in Gaussian Processes"},{"location":"literature/uncertain_inputs/#uncertain-inputs-in-gaussian-processes","text":"Please go to my dedicated repository where I explore all things to do with uncertain Gaussian processes. It can be found here: jejjohnson.github.io/uncertain_gps/","title":"Uncertain Inputs in Gaussian Processes"},{"location":"other/ideas/","text":"My Ideas \u00b6 Datasets \u00b6 Complex 1D Time Series MultiOutput Ask Jordi (crops?) Ocean Data Heteroscedastic Noise Models FLUX Data (ask Alvaro) Earth System Data Cube (ESDC) A potentially complex Spatial Temporal dataset. We could get some nice pretty maps. ISP Stuff Some tutorials - Courses Light Curves It would be nice to involve the astrophysics community. A first pass example: Exoplanet Example Tutorials \u00b6 It would be nice to recreate some staple tutorials that have either been in schools or have appeared on the documentation of websites. GPs from scratch - PyTorch | JAX | Marc Slides It would be nice to do a GP from scratch using different libraries. And then slowly refactoring until we become one with the library. A must-do for newcomers but then...don't ever do it again. Comparing Fully Bayesian (Hierarchical, Priors on Params) - Paper | Code | Demo A good and simple paper where they compare different GP implementations with hierarchical parameters (so priors on parameters). Shows MCMC and VI. I didn't see any priors on the params within the code. Pyro - GP Tutorial An excellent tutorial showing step-by-step from simple GP, Sparse GP and Sparse Variational GP. It would be great to have the same thing with GPyTorch, TensorFlow and PyMC3. GPSS Labs 2020 These are great labs. But they need to stop using GPy. Outdated software. So I would like to rewrite this in GPFlow (the successor) and perhaps in pymc3 and GPyTorch . Example - GPs w. GPyTorch GPs w. GPyTorch by DeepBayes2019 An excellent tutorial and an exact replica as the GPy tutorial. GP Intro in Python A good tutorial from Andreas Damianou. Graphs \u00b6 A While Algorithm should you choose chart - example I might be able to ask the devs this one, but it would be nice to have a chart about which algorithm would one recommend. Picture of the Relations - Slides A nice chart to show the connection between weights, functions, kernels and bayesian. Picture of the Libraries and their parents - Example for Numpy GP graphical models - Package | ReDo Francois Plot - Train vs Model Algorithms \u00b6 Exact GP (GP) Sparse GP (SGP) Stochastic Variational GP (SVGP) Deep Gaussian Processes (DGP) - GPyTorch | GPFlow2.0 | Pyro Deep Kernel Learning (DKL) Random Fourier Features Inference Schemes \u00b6 Exact Variational Inference Monte Carlo Expectation Propagation Importance Weighted Sampling Packages \u00b6 PyMC3 \u00b6 I think this is definitely worth looking it. It's a staple package in the Bayesian community that deserves some love. Tutorials * Exact GP * Fully Bayesian Exact GP - MCMC sampling methods * Sparse GP - FITC, VFE * Fully Bayesian Sparse GP? * Mauna Loa Example - Part 1 | Part 2 Edward2 \u00b6 An attempt to do \"drop-in\" GP layers that are keras-like. This is just awesome. I anticipate there will be tricks to this, but I feel like this is definitely the future so I'd like to explore this. Bayesian Layers Improvements \u00b6 Reproducibility \u00b6 Colab Notebooks! Binder! Logging \u00b6 tqdm for the loops wandb for the logging TensorBoard for others BoilerPlate Code \u00b6 Demo Wrapper pytorch-lightning for the PyTorch boilerplate code skorch for the PyTorch boilerplate code and GP capabilities Plotting \u00b6 It would be nice to demonstrate some neat plotting features in all of the tutorials. Perhaps with more increasing complexity with every tutorial. Bokeh - pymc3 tutorial with Mauna Loa GPy - All the plotting functions from the GP library Probflow - All of the Bayesian stuff from the Probflow library","title":"My Ideas"},{"location":"other/ideas/#my-ideas","text":"","title":"My Ideas"},{"location":"other/ideas/#datasets","text":"Complex 1D Time Series MultiOutput Ask Jordi (crops?) Ocean Data Heteroscedastic Noise Models FLUX Data (ask Alvaro) Earth System Data Cube (ESDC) A potentially complex Spatial Temporal dataset. We could get some nice pretty maps. ISP Stuff Some tutorials - Courses Light Curves It would be nice to involve the astrophysics community. A first pass example: Exoplanet Example","title":"Datasets"},{"location":"other/ideas/#tutorials","text":"It would be nice to recreate some staple tutorials that have either been in schools or have appeared on the documentation of websites. GPs from scratch - PyTorch | JAX | Marc Slides It would be nice to do a GP from scratch using different libraries. And then slowly refactoring until we become one with the library. A must-do for newcomers but then...don't ever do it again. Comparing Fully Bayesian (Hierarchical, Priors on Params) - Paper | Code | Demo A good and simple paper where they compare different GP implementations with hierarchical parameters (so priors on parameters). Shows MCMC and VI. I didn't see any priors on the params within the code. Pyro - GP Tutorial An excellent tutorial showing step-by-step from simple GP, Sparse GP and Sparse Variational GP. It would be great to have the same thing with GPyTorch, TensorFlow and PyMC3. GPSS Labs 2020 These are great labs. But they need to stop using GPy. Outdated software. So I would like to rewrite this in GPFlow (the successor) and perhaps in pymc3 and GPyTorch . Example - GPs w. GPyTorch GPs w. GPyTorch by DeepBayes2019 An excellent tutorial and an exact replica as the GPy tutorial. GP Intro in Python A good tutorial from Andreas Damianou.","title":"Tutorials"},{"location":"other/ideas/#graphs","text":"A While Algorithm should you choose chart - example I might be able to ask the devs this one, but it would be nice to have a chart about which algorithm would one recommend. Picture of the Relations - Slides A nice chart to show the connection between weights, functions, kernels and bayesian. Picture of the Libraries and their parents - Example for Numpy GP graphical models - Package | ReDo Francois Plot - Train vs Model","title":"Graphs"},{"location":"other/ideas/#algorithms","text":"Exact GP (GP) Sparse GP (SGP) Stochastic Variational GP (SVGP) Deep Gaussian Processes (DGP) - GPyTorch | GPFlow2.0 | Pyro Deep Kernel Learning (DKL) Random Fourier Features","title":"Algorithms"},{"location":"other/ideas/#inference-schemes","text":"Exact Variational Inference Monte Carlo Expectation Propagation Importance Weighted Sampling","title":"Inference Schemes"},{"location":"other/ideas/#packages","text":"","title":"Packages"},{"location":"other/ideas/#pymc3","text":"I think this is definitely worth looking it. It's a staple package in the Bayesian community that deserves some love. Tutorials * Exact GP * Fully Bayesian Exact GP - MCMC sampling methods * Sparse GP - FITC, VFE * Fully Bayesian Sparse GP? * Mauna Loa Example - Part 1 | Part 2","title":"PyMC3"},{"location":"other/ideas/#edward2","text":"An attempt to do \"drop-in\" GP layers that are keras-like. This is just awesome. I anticipate there will be tricks to this, but I feel like this is definitely the future so I'd like to explore this. Bayesian Layers","title":"Edward2"},{"location":"other/ideas/#improvements","text":"","title":"Improvements"},{"location":"other/ideas/#reproducibility","text":"Colab Notebooks! Binder!","title":"Reproducibility"},{"location":"other/ideas/#logging","text":"tqdm for the loops wandb for the logging TensorBoard for others","title":"Logging"},{"location":"other/ideas/#boilerplate-code","text":"Demo Wrapper pytorch-lightning for the PyTorch boilerplate code skorch for the PyTorch boilerplate code and GP capabilities","title":"BoilerPlate Code"},{"location":"other/ideas/#plotting","text":"It would be nice to demonstrate some neat plotting features in all of the tutorials. Perhaps with more increasing complexity with every tutorial. Bokeh - pymc3 tutorial with Mauna Loa GPy - All the plotting functions from the GP library Probflow - All of the Bayesian stuff from the Probflow library","title":"Plotting"},{"location":"prezi/tf2/slides/","text":"TF2.X and PyTorch \u00b6 For not so Dummies J. Emmanuel Johnson Second slide \u00b6 Best quote ever. Note: speaker notes FTW!","title":"TF2.X and PyTorch"},{"location":"prezi/tf2/slides/#tf2x-and-pytorch","text":"For not so Dummies J. Emmanuel Johnson","title":"TF2.X and PyTorch"},{"location":"prezi/tf2/slides/#second-slide","text":"Best quote ever. Note: speaker notes FTW!","title":"Second slide"},{"location":"prezi/tf2/docs/css/theme/","text":"Dependencies \u00b6 Themes are written using Sass to keep things modular and reduce the need for repeated selectors across files. Make sure that you have the reveal.js development environment including the Grunt dependencies installed before proceeding: https://github.com/hakimel/reveal.js#full-setup Creating a Theme \u00b6 To create your own theme, start by duplicating a .scss file in /css/theme/source . It will be automatically compiled by Grunt from Sass to CSS (see the Gruntfile ) when you run npm run build -- css-themes . Each theme file does four things in the following order: Include /css/theme/template/mixins.scss Shared utility functions. Include /css/theme/template/settings.scss Declares a set of custom variables that the template file (step 4) expects. Can be overridden in step 3. Override This is where you override the default theme. Either by specifying variables (see settings.scss for reference) or by adding any selectors and styles you please. Include /css/theme/template/theme.scss The template theme file which will generate final CSS output based on the currently defined variables.","title":"Index"},{"location":"prezi/tf2/docs/css/theme/#dependencies","text":"Themes are written using Sass to keep things modular and reduce the need for repeated selectors across files. Make sure that you have the reveal.js development environment including the Grunt dependencies installed before proceeding: https://github.com/hakimel/reveal.js#full-setup","title":"Dependencies"},{"location":"prezi/tf2/docs/css/theme/#creating-a-theme","text":"To create your own theme, start by duplicating a .scss file in /css/theme/source . It will be automatically compiled by Grunt from Sass to CSS (see the Gruntfile ) when you run npm run build -- css-themes . Each theme file does four things in the following order: Include /css/theme/template/mixins.scss Shared utility functions. Include /css/theme/template/settings.scss Declares a set of custom variables that the template file (step 4) expects. Can be overridden in step 3. Override This is where you override the default theme. Either by specifying variables (see settings.scss for reference) or by adding any selectors and styles you please. Include /css/theme/template/theme.scss The template theme file which will generate final CSS output based on the currently defined variables.","title":"Creating a Theme"},{"location":"prezi/tf2/docs/plugin/markdown/example/","text":"Markdown Demo \u00b6 External 1.1 \u00b6 Content 1.1 Note: This will only appear in the speaker notes window. External 1.2 \u00b6 Content 1.2 External 2 \u00b6 Content 2.1 External 3.1 \u00b6 Content 3.1 External 3.2 \u00b6 Content 3.2 External 3.3 \u00b6","title":"Markdown Demo"},{"location":"prezi/tf2/docs/plugin/markdown/example/#markdown-demo","text":"","title":"Markdown Demo"},{"location":"prezi/tf2/docs/plugin/markdown/example/#external-11","text":"Content 1.1 Note: This will only appear in the speaker notes window.","title":"External 1.1"},{"location":"prezi/tf2/docs/plugin/markdown/example/#external-12","text":"Content 1.2","title":"External 1.2"},{"location":"prezi/tf2/docs/plugin/markdown/example/#external-2","text":"Content 2.1","title":"External 2"},{"location":"prezi/tf2/docs/plugin/markdown/example/#external-31","text":"Content 3.1","title":"External 3.1"},{"location":"prezi/tf2/docs/plugin/markdown/example/#external-32","text":"Content 3.2","title":"External 3.2"},{"location":"prezi/tf2/docs/plugin/markdown/example/#external-33","text":"","title":"External 3.3"}]}