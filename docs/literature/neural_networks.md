---
title: Neural Networks
description: Neural Networks & Their Relation to GPs.
authors:
    - J. Emmanuel Johnson
path: docs/
source: neural_networks.md
---
# Neural Networks and Gaussian Processes


---
## Neural Networks & Deep Gaussian Processes

* [Building Bayesian Neural Networks with Blocks:
On Structure, Interpretability and Uncertainty](https://arxiv.org/pdf/1806.03563.pdf) - Zhou et. al. (2018)
* [Hierarchical Gaussian Process Priors for Bayesian Neural Network Weights](https://arxiv.org/abs/2002.04033)  - Karaletsos & Bui (10-2020)
* [Wide Neural Networks with Bottlenecks are Deep Gaussian Processes](https://jmlr.org/papers/v21/20-017.html) - Agrawal et. al. (2020) | [Code](https://code.ornl.gov/d0a/bottleneck_nngp)


---
## Latest

* [Deep Probabilistic Kernels for Sample-Efficient Learning](https://paperswithcode.com/paper/deep-probabilistic-kernels-for-sample) - Mallick et. al. (2019) [**arxiv**]
  > Propose a deep probabilistic kernel to address 1) traditional GP kernels aren't good at capturing similarities between high dimensional data points and 2) deep neural network kernels are not sample efficient. Has aspects such as Random Fourier Features, semi-supervised learning and utilizes the Stein Variational Gradient Descent algorithm.
* [On the expected behaviour of noise regulariseddeep neural networks as Gaussian processes](https://paperswithcode.com/paper/on-the-expected-behaviour-of-noise) - Pretorius et al (2019) [**arxiv**]
  > They study the impact of noise regularization via droput on Deep Kernel learning.

??? info "Hierarchical Gaussian Process Priors for Bayesian Neural Network Weights - Karaletsos & Bui (2020)"
    -> [Paper](https://arxiv.org/abs/2002.04033)

---
## Neural Processes

??? info "Neural Processes - Yann Dubois"
    -> [Tweet](https://twitter.com/yanndubs/status/1315698933390282752?s=19)

    -> [JupyterBook](https://yanndubs.github.io/Neural-Process-Family/text/Intro.html)